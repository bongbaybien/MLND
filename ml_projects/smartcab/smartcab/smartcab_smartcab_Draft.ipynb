{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "env = Environment()\n",
    "print 'env.primary_agent', env.primary_agent, '\\n'\n",
    "\n",
    "agent = env.create_agent(LearningAgent)\n",
    "print 'agent.primary_agent', agent.primary_agent\n",
    "env.set_primary_agent(agent)\n",
    "print 'env.primary_agent', env.primary_agent\n",
    "print 'agent.primary_agent', agent.primary_agent, '\\n'\n",
    "\n",
    "agent_learning = env.create_agent(LearningAgent)\n",
    "print 'agent_learning.primary_agent', agent_learning.primary_agent\n",
    "env.set_primary_agent(agent_learning)\n",
    "print 'env.primary_agent', env.primary_agent\n",
    "print 'agent_learning.primary_agent', agent_learning.primary_agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cls.a after init 0\n",
      "cls.a after set_a 1\n",
      "random_arg after set_a 0\n"
     ]
    }
   ],
   "source": [
    "class ExampleClass():\n",
    "    def __init__(self, learning=False):\n",
    "        self.a = 0\n",
    "        self.b = {}\n",
    "        self.learning = learning\n",
    "    def set_a(self, arg1):\n",
    "        self.a = 1\n",
    "        arg1 = 1\n",
    "    def set_b(self, b_val):\n",
    "        if self.learning:\n",
    "            self.b = b_val\n",
    "        return\n",
    "    def create_var(self):\n",
    "        self.c = 10\n",
    "    def return_var(self):\n",
    "        return self.c\n",
    "        \n",
    "        \n",
    "cls = ExampleClass()\n",
    "random_arg = 0\n",
    "print 'cls.a after init', cls.a\n",
    "cls.set_a(random_arg)\n",
    "print 'cls.a after set_a', cls.a\n",
    "print 'random_arg after set_a', random_arg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls.create_var()\n",
    "cls.return_var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before set_b {}\n",
      "after set_b {'b': 1}\n"
     ]
    }
   ],
   "source": [
    "cls = ExampleClass(learning=True)\n",
    "print 'before set_b', cls.b\n",
    "cls.set_b({'b':1})\n",
    "print 'after set_b', cls.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class ExampleClass1():\n",
    "    def __init__(self):\n",
    "        self.a = 0\n",
    "    def set_a(self, class_arg1):\n",
    "        self.a = 1\n",
    "        class_arg1.b = 1\n",
    "        \n",
    "class ExampleClass2():\n",
    "    def __init__(self):\n",
    "        self.b = 0\n",
    "\n",
    "cls = ExampleClass1()\n",
    "random_arg = ExampleClass2()\n",
    "print 'cls.a after init', cls.a\n",
    "print 'random_arg.b after init', random_arg.b\n",
    "cls.set_a(random_arg)\n",
    "print 'cls.a after set_a', cls.a\n",
    "print 'random_arg.b after set_a', random_arg.b\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "agent = env.create_agent(LearningAgent, learning=True)\n",
    "agent.learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "t = random.choice([None, 'forward', 'left','right'])\n",
    "print t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatic pdb calling has been turned ON\n"
     ]
    }
   ],
   "source": [
    "%pdb on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> <ipython-input-3-a52ec586a5ef>(243)run()\n",
      "-> sim.run(n_test=10)\n",
      "(Pdb) c\n",
      "\n",
      "/-------------------------\n",
      "| Training trial 1\n",
      "\\-------------------------\n",
      "\n",
      "> c:\\users\\mhntx\\google drive\\to sync\\udacity\\mlnd\\ml_projects\\smartcab\\smartcab\\simulator.py(160)run()\n",
      "-> self.env.reset(testing)\n",
      "(Pdb) c\n",
      "\n",
      " epsilon  1.0\n",
      "Simulating trial. . . \n",
      "epsilon = 1.0000; alpha = 0.5000\n",
      "Simulating trial. . . \n",
      "epsilon = 1.0000; alpha = 0.5000\n",
      "Simulating trial. . . \n",
      "epsilon = 1.0000; alpha = 0.5000\n",
      "Simulating trial. . . \n",
      "epsilon = 1.0000; alpha = 0.5000\n",
      "Simulating trial. . . \n",
      "epsilon = 1.0000; alpha = 0.5000\n",
      "Simulating trial. . . \n",
      "epsilon = 1.0000; alpha = 0.5000\n",
      "Simulating trial. . . \n",
      "epsilon = 1.0000; alpha = 0.5000\n",
      "Simulating trial. . . \n",
      "epsilon = 1.0000; alpha = 0.5000\n",
      "Simulating trial. . . \n",
      "epsilon = 1.0000; alpha = 0.5000\n",
      "Simulating trial. . . \n",
      "epsilon = 1.0000; alpha = 0.5000\n",
      "Simulating trial. . . \n",
      "epsilon = 1.0000; alpha = 0.5000\n",
      "Simulating trial. . . \n",
      "epsilon = 1.0000; alpha = 0.5000\n",
      "Simulating trial. . . \n",
      "epsilon = 1.0000; alpha = 0.5000\n",
      "Simulating trial. . . \n",
      "epsilon = 1.0000; alpha = 0.5000\n",
      "Simulating trial. . . \n",
      "epsilon = 1.0000; alpha = 0.5000\n",
      "Simulating trial. . . \n",
      "epsilon = 1.0000; alpha = 0.5000\n",
      "Simulating trial. . . \n",
      "epsilon = 1.0000; alpha = 0.5000\n",
      "Simulating trial. . . \n",
      "epsilon = 1.0000; alpha = 0.5000\n",
      "Simulating trial. . . \n",
      "epsilon = 1.0000; alpha = 0.5000\n",
      "Simulating trial. . . \n",
      "epsilon = 1.0000; alpha = 0.5000\n",
      "Simulating trial. . . \n",
      "epsilon = 1.0000; alpha = 0.5000\n",
      "Simulating trial. . . \n",
      "epsilon = 1.0000; alpha = 0.5000\n",
      "Simulating trial. . . \n",
      "epsilon = 1.0000; alpha = 0.5000\n",
      "Simulating trial. . . \n",
      "epsilon = 1.0000; alpha = 0.5000\n",
      "Simulating trial. . . \n",
      "epsilon = 1.0000; alpha = 0.5000\n",
      "Simulating trial. . . \n",
      "epsilon = 1.0000; alpha = 0.5000\n",
      "Simulating trial. . . \n",
      "epsilon = 1.0000; alpha = 0.5000\n",
      "Simulating trial. . . \n",
      "epsilon = 1.0000; alpha = 0.5000\n",
      "Simulating trial. . . \n",
      "epsilon = 1.0000; alpha = 0.5000\n",
      "Simulating trial. . . \n",
      "epsilon = 1.0000; alpha = 0.5000\n",
      "Simulating trial. . . \n",
      "epsilon = 1.0000; alpha = 0.5000\n",
      "Simulating trial. . . \n",
      "epsilon = 1.0000; alpha = 0.5000\n",
      "Simulating trial. . . \n",
      "epsilon = 1.0000; alpha = 0.5000\n",
      "Simulating trial. . . \n",
      "epsilon = 1.0000; alpha = 0.5000\n",
      "Simulating trial. . . \n",
      "epsilon = 1.0000; alpha = 0.5000\n",
      "Simulating trial. . . \n",
      "epsilon = 1.0000; alpha = 0.5000\n",
      "Simulating trial. . . \n",
      "epsilon = 1.0000; alpha = 0.5000\n",
      "Simulating trial. . . \n",
      "epsilon = 1.0000; alpha = 0.5000\n",
      "Simulating trial. . . \n",
      "epsilon = 1.0000; alpha = 0.5000\n",
      "Simulating trial. . . \n",
      "epsilon = 1.0000; alpha = 0.5000\n",
      "Simulating trial. . . \n",
      "epsilon = 1.0000; alpha = 0.5000\n",
      "Simulating trial. . . \n",
      "epsilon = 1.0000; alpha = 0.5000\n",
      "Simulating trial. . . \n",
      "epsilon = 1.0000; alpha = 0.5000\n",
      "Simulating trial. . . \n",
      "epsilon = 1.0000; alpha = 0.5000\n",
      "Simulating trial. . . \n",
      "epsilon = 1.0000; alpha = 0.5000\n",
      "Simulating trial. . . \n",
      "epsilon = 1.0000; alpha = 0.5000\n",
      "Simulating trial. . . \n",
      "epsilon = 1.0000; alpha = 0.5000\n",
      "Simulating trial. . . \n",
      "epsilon = 1.0000; alpha = 0.5000\n",
      "Simulating trial. . . \n",
      "epsilon = 1.0000; alpha = 0.5000\n",
      "Simulating trial. . . \n",
      "epsilon = 1.0000; alpha = 0.5000\n",
      "Simulating trial. . . \n",
      "epsilon = 1.0000; alpha = 0.5000\n",
      "Simulating trial. . . \n",
      "epsilon = 1.0000; alpha = 0.5000\n",
      "Simulating trial. . . \n",
      "epsilon = 1.0000; alpha = 0.5000\n",
      "Simulating trial. . . \n",
      "epsilon = 1.0000; alpha = 0.5000\n",
      "Simulating trial. . . \n",
      "epsilon = 1.0000; alpha = 0.5000\n",
      "Simulating trial. . . \n",
      "epsilon = 1.0000; alpha = 0.5000\n",
      "Simulating trial. . . \n",
      "epsilon = 1.0000; alpha = 0.5000\n",
      "Simulating trial. . . \n",
      "epsilon = 1.0000; alpha = 0.5000\n",
      "Simulating trial. . . \n",
      "epsilon = 1.0000; alpha = 0.5000\n",
      "Simulating trial. . . \n",
      "epsilon = 1.0000; alpha = 0.5000\n",
      "Simulating trial. . . \n",
      "epsilon = 1.0000; alpha = 0.5000\n",
      "Simulating trial. . . \n",
      "epsilon = 1.0000; alpha = 0.5000\n",
      "Simulating trial. . . \n",
      "epsilon = 1.0000; alpha = 0.5000\n",
      "Simulating trial. . . \n",
      "epsilon = 1.0000; alpha = 0.5000\n",
      "Simulating trial. . . \n",
      "epsilon = 1.0000; alpha = 0.5000\n",
      "Simulating trial. . . \n",
      "epsilon = 1.0000; alpha = 0.5000\n",
      "Simulating trial. . . \n",
      "epsilon = 1.0000; alpha = 0.5000\n",
      "Simulating trial. . . \n",
      "epsilon = 1.0000; alpha = 0.5000\n",
      "Simulating trial. . . \n",
      "epsilon = 1.0000; alpha = 0.5000\n",
      "Simulating trial. . . \n",
      "epsilon = 1.0000; alpha = 0.5000\n",
      "Simulating trial. . . \n",
      "epsilon = 1.0000; alpha = 0.5000\n",
      "Simulating trial. . . \n",
      "epsilon = 1.0000; alpha = 0.5000\n",
      "Simulating trial. . . \n",
      "epsilon = 1.0000; alpha = 0.5000\n",
      "Simulating trial. . . \n",
      "epsilon = 1.0000; alpha = 0.5000\n",
      "Simulating trial. . . \n",
      "epsilon = 1.0000; alpha = 0.5000\n",
      "Simulating trial. . . \n",
      "epsilon = 1.0000; alpha = 0.5000\n",
      "Simulating trial. . . \n",
      "epsilon = 1.0000; alpha = 0.5000\n",
      "Simulating trial. . . \n",
      "epsilon = 1.0000; alpha = 0.5000\n",
      "Simulating trial. . . \n",
      "epsilon = 1.0000; alpha = 0.5000\n",
      "Simulating trial. . . \n",
      "epsilon = 1.0000; alpha = 0.5000\n",
      "Simulating trial. . . \n",
      "epsilon = 1.0000; alpha = 0.5000\n",
      "Simulating trial. . . \n",
      "epsilon = 1.0000; alpha = 0.5000\n",
      "Simulating trial. . . \n",
      "epsilon = 1.0000; alpha = 0.5000\n",
      "Simulating trial. . . \n",
      "epsilon = 1.0000; alpha = 0.5000\n",
      "Simulating trial. . . \n",
      "epsilon = 1.0000; alpha = 0.5000\n",
      "Simulating trial. . . \n",
      "epsilon = 1.0000; alpha = 0.5000\n",
      "Simulating trial. . . \n",
      "epsilon = 1.0000; alpha = 0.5000\n",
      "Simulating trial. . . \n",
      "epsilon = 1.0000; alpha = 0.5000\n",
      "Simulating trial. . . \n",
      "epsilon = 1.0000; alpha = 0.5000\n",
      "Simulating trial. . . \n",
      "epsilon = 1.0000; alpha = 0.5000\n",
      "Simulating trial. . . \n",
      "epsilon = 1.0000; alpha = 0.5000\n",
      "Simulating trial. . . \n",
      "epsilon = 1.0000; alpha = 0.5000\n",
      "Simulating trial. . . \n",
      "epsilon = 1.0000; alpha = 0.5000\n",
      "Simulating trial. . . \n",
      "epsilon = 1.0000; alpha = 0.5000\n",
      "Simulating trial. . . \n",
      "epsilon = 1.0000; alpha = 0.5000\n",
      "Simulating trial. . . \n",
      "epsilon = 1.0000; alpha = 0.5000\n",
      "Simulating trial. . . \n",
      "epsilon = 1.0000; alpha = 0.5000\n",
      "Simulating trial. . . \n",
      "epsilon = 1.0000; alpha = 0.5000\n",
      "Simulating trial. . . \n",
      "epsilon = 1.0000; alpha = 0.5000\n",
      "Simulating trial. . . \n",
      "epsilon = 1.0000; alpha = 0.5000\n",
      "Simulating trial. . . \n",
      "epsilon = 1.0000; alpha = 0.5000\n",
      "Simulating trial. . . \n",
      "epsilon = 1.0000; alpha = 0.5000\n",
      "Simulating trial. . . \n",
      "epsilon = 1.0000; alpha = 0.5000\n",
      "Simulating trial. . . \n",
      "epsilon = 1.0000; alpha = 0.5000\n",
      "Simulating trial. . . \n",
      "epsilon = 1.0000; alpha = 0.5000\n",
      "Simulating trial. . . \n",
      "epsilon = 1.0000; alpha = 0.5000\n",
      "Simulating trial. . . \n",
      "epsilon = 1.0000; alpha = 0.5000\n",
      "Simulating trial. . . \n",
      "epsilon = 1.0000; alpha = 0.5000\n",
      "Simulating trial. . . \n",
      "epsilon = 1.0000; alpha = 0.5000\n",
      "Simulating trial. . . \n",
      "epsilon = 1.0000; alpha = 0.5000\n",
      "Simulating trial. . . \n",
      "epsilon = 1.0000; alpha = 0.5000\n",
      "Simulating trial. . . \n",
      "epsilon = 1.0000; alpha = 0.5000\n",
      "Simulating trial. . . \n",
      "epsilon = 1.0000; alpha = 0.5000\n",
      "Simulating trial. . . \n",
      "epsilon = 1.0000; alpha = 0.5000\n",
      "> c:\\users\\mhntx\\google drive\\to sync\\udacity\\mlnd\\ml_projects\\smartcab\\smartcab\\simulator.py(186)run()\n",
      "-> self.env.step()\n",
      "(Pdb) c\n",
      "\n",
      "/-------------------\n",
      "| Step 0 Results\n",
      "\\-------------------\n",
      "\n",
      "\n",
      " chosing action\n",
      "random action  forward\n",
      "max action  right\n",
      "epsilon  1.0\n",
      "action chosen  forward\n",
      "\n",
      " learning\n",
      "self.Q[state] before learning  {'forward': 0.0, None: 0.0, 'right': 0.0, 'left': 0.0}\n",
      "self.Q[state] after learning  {'forward': -20.063346748763173, None: 0.0, 'right': 0.0, 'left': 0.0} \n",
      "\n",
      "Agent previous state: ('left', 'red', None, None, 'forward', 25)\n",
      "Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -40.13)\n",
      "96% of time remaining to reach destination.\n",
      "> c:\\users\\mhntx\\google drive\\to sync\\udacity\\mlnd\\ml_projects\\smartcab\\smartcab\\simulator.py(185)run()\n",
      "-> pdb.set_trace()\n",
      "(Pdb) c\n",
      "\n",
      "/-------------------\n",
      "| Step 1 Results\n",
      "\\-------------------\n",
      "\n",
      "\n",
      " chosing action\n",
      "random action  None\n",
      "max action  right\n",
      "epsilon  1.0\n",
      "action chosen  None\n",
      "\n",
      " learning\n",
      "self.Q[state] before learning  {'forward': 0.0, None: 0.0, 'right': 0.0, 'left': 0.0}\n",
      "self.Q[state] after learning  {'forward': 0.0, None: 0.9055172184464583, 'right': 0.0, 'left': 0.0} \n",
      "\n",
      "Agent previous state: ('left', 'red', 'right', None, 'right', 24)\n",
      "Agent properly idled at a red light. (rewarded 1.81)\n",
      "92% of time remaining to reach destination.\n",
      "> c:\\users\\mhntx\\google drive\\to sync\\udacity\\mlnd\\ml_projects\\smartcab\\smartcab\\simulator.py(186)run()\n",
      "-> self.env.step()\n",
      "(Pdb) c\n",
      "\n",
      "/-------------------\n",
      "| Step 2 Results\n",
      "\\-------------------\n",
      "\n",
      "\n",
      " chosing action\n",
      "random action  left\n",
      "max action  right\n",
      "epsilon  1.0\n",
      "action chosen  left\n",
      "\n",
      " learning\n",
      "self.Q[state] before learning  {'forward': 0.0, None: 0.0, 'right': 0.0, 'left': 0.0}\n",
      "self.Q[state] after learning  {'forward': 0.0, None: 0.0, 'right': 0.0, 'left': -4.889608342738952} \n",
      "\n",
      "Agent previous state: ('left', 'red', None, None, None, 23)\n",
      "Agent attempted driving left through a red light. (rewarded -9.78)\n",
      "88% of time remaining to reach destination.\n",
      "> c:\\users\\mhntx\\google drive\\to sync\\udacity\\mlnd\\ml_projects\\smartcab\\smartcab\\simulator.py(185)run()\n",
      "-> pdb.set_trace()\n",
      "(Pdb) c\n",
      "\n",
      "/-------------------\n",
      "| Step 3 Results\n",
      "\\-------------------\n",
      "\n",
      "\n",
      " chosing action\n",
      "random action  left\n",
      "max action  right\n",
      "epsilon  1.0\n",
      "action chosen  left\n",
      "\n",
      " learning\n",
      "self.Q[state] before learning  {'forward': 0.0, None: 0.0, 'right': 0.0, 'left': 0.0}\n",
      "self.Q[state] after learning  {'forward': 0.0, None: 0.0, 'right': 0.0, 'left': -4.730706047598816} \n",
      "\n",
      "Agent previous state: ('left', 'red', None, None, None, 22)\n",
      "Agent attempted driving left through a red light. (rewarded -9.46)\n",
      "84% of time remaining to reach destination.\n",
      "> c:\\users\\mhntx\\google drive\\to sync\\udacity\\mlnd\\ml_projects\\smartcab\\smartcab\\simulator.py(186)run()\n",
      "-> self.env.step()\n",
      "(Pdb) c\n",
      "\n",
      "/-------------------\n",
      "| Step 4 Results\n",
      "\\-------------------\n",
      "\n",
      "\n",
      " chosing action\n",
      "random action  right\n",
      "max action  right\n",
      "epsilon  1.0\n",
      "action chosen  right\n",
      "\n",
      " learning\n",
      "self.Q[state] before learning  {'forward': 0.0, None: 0.0, 'right': 0.0, 'left': 0.0}\n",
      "self.Q[state] after learning  {'forward': 0.0, None: 0.0, 'right': 0.3307531712821216, 'left': 0.0} \n",
      "\n",
      "Agent previous state: ('left', 'red', None, None, 'forward', 21)\n",
      "Agent drove right instead of left. (rewarded 0.66)\n",
      "80% of time remaining to reach destination.\n",
      "> c:\\users\\mhntx\\google drive\\to sync\\udacity\\mlnd\\ml_projects\\smartcab\\smartcab\\simulator.py(185)run()\n",
      "-> pdb.set_trace()\n",
      "(Pdb) c\n",
      "\n",
      "/-------------------\n",
      "| Step 5 Results\n",
      "\\-------------------\n",
      "\n",
      "\n",
      " chosing action\n",
      "random action  right\n",
      "max action  right\n",
      "epsilon  1.0\n",
      "action chosen  right\n",
      "\n",
      " learning\n",
      "self.Q[state] before learning  {'forward': 0.0, None: 0.0, 'right': 0.0, 'left': 0.0}\n",
      "self.Q[state] after learning  {'forward': 0.0, None: 0.0, 'right': 1.138734525920241, 'left': 0.0} \n",
      "\n",
      "Agent previous state: ('right', 'green', None, None, None, 20)\n",
      "Agent followed the waypoint right. (rewarded 2.28)\n",
      "76% of time remaining to reach destination.\n",
      "> c:\\users\\mhntx\\google drive\\to sync\\udacity\\mlnd\\ml_projects\\smartcab\\smartcab\\simulator.py(186)run()\n",
      "-> self.env.step()\n",
      "(Pdb) q\n"
     ]
    },
    {
     "ename": "BdbQuit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mBdbQuit\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-a52ec586a5ef>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    245\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    246\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 247\u001b[1;33m     \u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-3-a52ec586a5ef>\u001b[0m in \u001b[0;36mrun\u001b[1;34m()\u001b[0m\n\u001b[0;32m    241\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    242\u001b[0m \u001b[1;31m##    sim.run()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 243\u001b[1;33m     \u001b[0msim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_test\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    244\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    245\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\mhntx\\Google Drive\\To Sync\\Udacity\\MLND\\ml_projects\\smartcab\\smartcab\\simulator.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, tolerance, n_test)\u001b[0m\n\u001b[0;32m    184\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcurrent_time\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlast_updated\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate_delay\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    185\u001b[0m                         \u001b[0mpdb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 186\u001b[1;33m                         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    187\u001b[0m                         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlast_updated\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcurrent_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    188\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\mhntx\\Google Drive\\To Sync\\Udacity\\MLND\\ml_projects\\smartcab\\smartcab\\simulator.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, tolerance, n_test)\u001b[0m\n\u001b[0;32m    184\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcurrent_time\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlast_updated\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate_delay\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    185\u001b[0m                         \u001b[0mpdb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 186\u001b[1;33m                         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    187\u001b[0m                         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlast_updated\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcurrent_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    188\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Python2\\lib\\bdb.pyc\u001b[0m in \u001b[0;36mtrace_dispatch\u001b[1;34m(self, frame, event, arg)\u001b[0m\n\u001b[0;32m     47\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;31m# None\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'line'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_line\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'call'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Python2\\lib\\bdb.pyc\u001b[0m in \u001b[0;36mdispatch_line\u001b[1;34m(self, frame)\u001b[0m\n\u001b[0;32m     66\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop_here\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbreak_here\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muser_line\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 68\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mquitting\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mraise\u001b[0m \u001b[0mBdbQuit\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     69\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrace_dispatch\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mBdbQuit\u001b[0m: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[1;32mc:\\program files (x86)\\python2\\lib\\bdb.py\u001b[0m(68)\u001b[0;36mdispatch_line\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m     66 \u001b[1;33m        \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop_here\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbreak_here\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m     67 \u001b[1;33m            \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muser_line\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m---> 68 \u001b[1;33m            \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mquitting\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mraise\u001b[0m \u001b[0mBdbQuit\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m     69 \u001b[1;33m        \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrace_dispatch\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m     70 \u001b[1;33m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> q\n"
     ]
    }
   ],
   "source": [
    "# %load agent.py\n",
    "import random\n",
    "import math\n",
    "from environment import Agent, Environment\n",
    "from planner import RoutePlanner\n",
    "from simulator import Simulator\n",
    "\n",
    "import pdb\n",
    "import numpy as np\n",
    "\n",
    "class LearningAgent(Agent):\n",
    "    \"\"\" An agent that learns to drive in the Smartcab world.\n",
    "        This is the object you will be modifying. \"\"\" \n",
    "\n",
    "    def __init__(self, env, learning=False, epsilon=1.0, alpha=0.5):\n",
    "        super(LearningAgent, self).__init__(env)     # Set the agent in the evironment \n",
    "        self.planner = RoutePlanner(self.env, self)  # Create a route planner\n",
    "        self.valid_actions = self.env.valid_actions  # The set of valid actions\n",
    "\n",
    "        # Set parameters of the learning agent\n",
    "        self.learning = learning # Whether the agent is expected to learn\n",
    "        self.Q = dict()          # Create a Q-table which will be a dictionary of tuples\n",
    "        self.epsilon = epsilon   # Random exploration factor\n",
    "        self.alpha = alpha       # Learning factor\n",
    "\n",
    "        ###########\n",
    "        ## TO DO ##\n",
    "        ###########\n",
    "        # Set any additional class parameters as needed\n",
    "        self.trial = 0\n",
    "\n",
    "\n",
    "    def reset(self, destination=None, testing=False):\n",
    "        \"\"\" The reset function is called at the beginning of each trial.\n",
    "            'testing' is set to True if testing trials are being used\n",
    "            once training trials have completed. \"\"\"\n",
    "\n",
    "        # Select the destination as the new location to route to\n",
    "        self.planner.route_to(destination)\n",
    "        \n",
    "        ########### \n",
    "        ## TO DO ##\n",
    "        ###########\n",
    "        self.trial += 1        \n",
    "        # Update epsilon using a decay function of your choice\n",
    "        if self.trial > 1:\n",
    "            self.epsilon = self.epsilon - 0.05        \n",
    "        # Update additional class parameters as needed\n",
    "        # If 'testing' is True, set epsilon and alpha to 0\n",
    "        if testing==True:\n",
    "            self.epsilon = 0.0\n",
    "            self.alpha = 0.0\n",
    "            \n",
    "        print '\\n epsilon ', self.epsilon\n",
    "        \n",
    "        return None\n",
    "\n",
    "    def build_state(self):\n",
    "        \"\"\" The build_state function is called when the agent requests data from the \n",
    "            environment. The next waypoint, the intersection inputs, and the deadline \n",
    "            are all features available to the agent. \"\"\"\n",
    "\n",
    "        # Collect data about the environment\n",
    "        waypoint = self.planner.next_waypoint() # The next waypoint \n",
    "        inputs = self.env.sense(self)           # Visual input - intersection light and traffic\n",
    "        deadline = self.env.get_deadline(self)  # Remaining deadline\n",
    "\n",
    "        ########### \n",
    "        ## TO DO ##\n",
    "        ###########\n",
    "        # Set 'state' as a tuple of relevant data for the agent\n",
    "##        state = None\n",
    "        state = (waypoint, inputs['light'], inputs['oncoming'], inputs['left'], inputs['right'], deadline)\n",
    "\n",
    "        return state\n",
    "\n",
    "\n",
    "    def get_maxQ(self, state):\n",
    "        \"\"\" The get_max_Q function is called when the agent is asked to find the\n",
    "            maximum Q-value of all actions based on the 'state' the smartcab is in. \"\"\"\n",
    "\n",
    "        ########### \n",
    "        ## TO DO ##\n",
    "        ###########\n",
    "        # Calculate the maximum Q-value of all actions for a given state\n",
    "\n",
    "        maxQ = None\n",
    "\n",
    "        return maxQ \n",
    "\n",
    "\n",
    "    def createQ(self, state):\n",
    "        \"\"\" The createQ function is called when a state is generated by the agent. \"\"\"\n",
    "\n",
    "        ########### \n",
    "        ## TO DO ##\n",
    "        ###########\n",
    "        # When learning, check if the 'state' is not in the Q-table\n",
    "        # If it is not, create a new dictionary for that state\n",
    "        #   Then, for each action available, set the initial Q-value to 0.0\n",
    "\n",
    "        # [My] No need to check if the agent is at the boundaries so that certain actions are not allowed.\n",
    "        # The RoutePlanner takes care of this when calculating the next_waypoint\n",
    "        if self.learning:\n",
    "            if state not in self.Q:\n",
    "                self.Q[state] = {None:0.0, 'forward':0.0, 'left':0.0, 'right':0.0}            \n",
    "\n",
    "        return\n",
    "\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        \"\"\" The choose_action function is called when the agent is asked to choose\n",
    "            which action to take, based on the 'state' the smartcab is in. \"\"\"\n",
    "\n",
    "        # Set the agent state and default action\n",
    "        self.state = state\n",
    "        self.next_waypoint = self.planner.next_waypoint()\n",
    "        action = None\n",
    "\n",
    "        ########### \n",
    "        ## TO DO ##\n",
    "        ###########\n",
    "        # When not learning, choose a random action\n",
    "        if not self.learning:\n",
    "            action = random.choice(self.valid_actions)\n",
    "        # When learning, choose a random action with 'epsilon' probability\n",
    "        #   Otherwise, choose an action with the highest Q-value for the current state\n",
    "        elif self.learning:\n",
    "            action_random = random.choice(self.valid_actions)\n",
    "            print 'random action ', action_random\n",
    "            \n",
    "            action_max = max(self.Q[state])\n",
    "            print 'max action ', action_max\n",
    "\n",
    "            print 'epsilon ', self.epsilon\n",
    "            action = np.random.choice([action_random, action_max], p=[self.epsilon, 1-self.epsilon])\n",
    " \n",
    "        return action\n",
    "\n",
    "\n",
    "    def learn(self, state, action, reward):\n",
    "        \"\"\" The learn function is called after the agent completes an action and\n",
    "            receives an award. This function does not consider future rewards \n",
    "            when conducting learning. \"\"\"\n",
    "\n",
    "        ########### \n",
    "        ## TO DO ##\n",
    "        ###########\n",
    "        # When learning, implement the value iteration update rule\n",
    "        #   Use only the learning rate 'alpha' (do not use the discount factor 'gamma')\n",
    "        if self.learning:\n",
    "            next_state = self.build_state()\n",
    "##            print '\\n next state: self.build_state() \\n', next_state\n",
    "            self.createQ(next_state)            \n",
    "            self.Q[state][action] += (reward + max(self.Q[next_state].values()) - self.Q[state][action]) * self.alpha\n",
    "\n",
    "        return\n",
    "\n",
    "\n",
    "    def update(self):\n",
    "        \"\"\" The update function is called when a time step is completed in the \n",
    "            environment for a given trial. This function will build the agent\n",
    "            state, choose an action, receive a reward, and learn if enabled. \"\"\"\n",
    "\n",
    "##        print '\\n getting current state'\n",
    "        state = self.build_state()          # Get current state\n",
    "##        print 'agent state (waypoint, light, oncoming, left, right) \\n', state\n",
    "\n",
    "##        print '\\n creating state in Q-table'\n",
    "        self.createQ(state)                 # Create 'state' in Q-table\n",
    "##        print 'current self.Q \\n', self.Q\n",
    "\n",
    "        print '\\n chosing action'\n",
    "        action = self.choose_action(state)  # Choose an action\n",
    "        print 'action chosen ', action\n",
    "\n",
    "##        print '\\n acting'\n",
    "        reward = self.env.act(self, action) # Receive a reward\n",
    "##        print 'reward ', reward\n",
    "\n",
    "        print '\\n learning'\n",
    "        print 'self.Q[state] before learning ', self.Q[state]\n",
    "        self.learn(state, action, reward)   # Q-learn\n",
    "        print 'self.Q[state] after learning ', self.Q[state], '\\n'\n",
    "\n",
    "        return\n",
    "        \n",
    "\n",
    "def run():\n",
    "    \"\"\" Driving function for running the simulation. \n",
    "        Press ESC to close the simulation, or [SPACE] to pause the simulation. \"\"\"\n",
    "\n",
    "    ##############\n",
    "    # Create the environment\n",
    "    # Flags:\n",
    "    #   verbose     - set to True to display additional output from the simulation\n",
    "    #   num_dummies - discrete number of dummy agents in the environment, default is 100\n",
    "    #   grid_size   - discrete number of intersections (columns, rows), default is (8, 6)\n",
    "    \n",
    "    env = Environment()\n",
    "    \n",
    "    ##############\n",
    "    # Create the driving agent\n",
    "    # Flags:\n",
    "    #   learning   - set to True to force the driving agent to use Q-learning\n",
    "    #    * epsilon - continuous value for the exploration factor, default is 1\n",
    "    #    * alpha   - continuous value for the learning rate, default is 0.5\n",
    "\n",
    "##    agent = env.create_agent(LearningAgent)\n",
    "    agent = env.create_agent(LearningAgent, learning=True)\n",
    "    \n",
    "    ##############\n",
    "    # Follow the driving agent\n",
    "    # Flags:\n",
    "    #   enforce_deadline - set to True to enforce a deadline metric\n",
    "    \n",
    "##    env.set_primary_agent(agent)\n",
    "    env.set_primary_agent(agent, enforce_deadline=True)\n",
    "    \n",
    "\n",
    "    ##############\n",
    "    # Create the simulation\n",
    "    # Flags:\n",
    "    #   update_delay - continuous time (in seconds) between actions, default is 2.0 seconds\n",
    "    #   display      - set to False to disable the GUI if PyGame is enabled\n",
    "    #   log_metrics  - set to True to log trial and simulation results to /logs\n",
    "    #   optimized    - set to True to change the default log file name\n",
    "##    sim = Simulator(env)\n",
    "##    sim = Simulator(env, display=False)\n",
    "##    sim = Simulator(env, update_delay=0.01, log_metrics=True, display=True)\n",
    "    sim = Simulator(env, update_delay=0.01, display=False)\n",
    "\n",
    "    \n",
    "    ##############\n",
    "    # Run the simulator\n",
    "    # Flags:\n",
    "    #   tolerance  - epsilon tolerance before beginning testing, default is 0.05 \n",
    "    #   n_test     - discrete number of testing trials to perform, default is 0\n",
    "\n",
    "    pdb.set_trace()\n",
    "    \n",
    "##    sim.run()\n",
    "    sim.run(n_test=10)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    run()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
