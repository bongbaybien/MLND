{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import helper\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import keras\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers.convolutional import Cropping2D, Conv2D, ZeroPadding2D\n",
    "from keras.layers.pooling import MaxPooling2D, AveragePooling2D\n",
    "from keras.layers.core import Flatten, Dropout, Dense\n",
    "from keras import optimizers\n",
    "import keras.backend as k\n",
    "import pdb\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in data, format, & visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emotion_dict = {0:'angry', 1:'disgust', 2:'fear', 3:'happy', 4:'sad', 5:'surprise', 6:'neutral'}\n",
    "image_shape = (48,48,1)\n",
    "n_classes = len(emotion_dict)\n",
    "n_features = np.product(image_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read in data\n",
    "file_path = './Data/Kaggle-FER/fer2013.csv'\n",
    "data_raw = helper.read_in_data(file_path) # data is a tuple of (x_list, y_list, usage_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert data to numpy array\n",
    "data_formatted = helper.format(data_raw, image_shape, n_classes) # return tuple of (x_array, y_array, usage_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 4945, 1: 547, 2: 5121, 3: 8988, 4: 6076, 5: 4001, 6: 6197}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of example for each class\n",
    "label, count = np.unique(data_formatted[1], return_counts=True)\n",
    "dict(zip(label, count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split into train & test sets. Test set = public, private set is used for final testing, to be consistent with the competition\n",
    "(x_train, y_train), (x_test, y_test), (x_private, y_private) = helper.create_train_test(data_formatted, n_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# take a sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # take a random sample\n",
    "# n_sample = 100\n",
    "\n",
    "# x_train = x_train[0:n_sample]\n",
    "# y_train = y_train[0:n_sample]\n",
    "# x_test = x_test[0:n_sample]\n",
    "# y_test = y_test[0:n_sample]\n",
    "# x_private = x_private[0:n_sample]\n",
    "# y_private = y_private[0:n_sample]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# select certain class\n",
    "selected_classes = [3,4]\n",
    "n_sample = 50\n",
    "\n",
    "mask = np.isin(y_train, selected_classes).reshape(-1)\n",
    "# print('mask shape', mask.shape)\n",
    "x_train = x_train[mask][0:n_sample]\n",
    "y_train = y_train[mask][0:n_sample]\n",
    "\n",
    "mask = np.isin(y_test, selected_classes).reshape(-1)\n",
    "x_test = x_test[mask][0:n_sample]\n",
    "y_test = y_test[mask][0:n_sample]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set {3: 30, 4: 20}\n",
      "validation set {3: 32, 4: 18}\n"
     ]
    }
   ],
   "source": [
    "# distribution for each class\n",
    "label, count = np.unique(y_train, return_counts=True)\n",
    "print('training set', dict(zip(label, count)))\n",
    "\n",
    "# distribution for each class\n",
    "label, count = np.unique(y_test, return_counts=True)\n",
    "print('validation set', dict(zip(label, count)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.savetxt('x_train.csv', x_train[index], delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files (x86)\\Python35-64bit\\lib\\site-packages\\keras\\preprocessing\\image.py:648: UserWarning: Expected input to be images (as Numpy array) following the data format convention \"channels_last\" (channels on axis 3), i.e. expected either 1, 3 or 4 channels on axis 3. However, it was passed an array with shape (50, 48, 48, 1) (1 channels).\n",
      "  ' (' + str(x.shape[self.channel_axis]) + ' channels).')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 48, 48, 1)\n",
      "<class 'numpy.ndarray'>"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-92bf7e8e60f2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrain_datagen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Python35-64bit\\lib\\site-packages\\ipykernel\\iostream.py\u001b[0m in \u001b[0;36mwrite\u001b[1;34m(self, string)\u001b[0m\n\u001b[0;32m    350\u001b[0m             \u001b[0mis_child\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_master_process\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    351\u001b[0m             \u001b[1;31m# only touch the buffer in the IO thread to avoid races\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 352\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpub_thread\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mschedule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    353\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_child\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    354\u001b[0m                 \u001b[1;31m# newlines imply flush in subprocesses\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Python35-64bit\\lib\\site-packages\\ipykernel\\iostream.py\u001b[0m in \u001b[0;36mschedule\u001b[1;34m(self, f)\u001b[0m\n\u001b[0;32m    188\u001b[0m                 \u001b[0mevent_id\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0murandom\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m16\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_events\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mevent_id\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 190\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_event_pipe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevent_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    191\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m             \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mzmq\\backend\\cython\\socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.send (zmq\\backend\\cython\\socket.c:7305)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mzmq\\backend\\cython\\socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.send (zmq\\backend\\cython\\socket.c:7048)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mzmq\\backend\\cython\\socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._send_copy (zmq\\backend\\cython\\socket.c:2920)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Python35-64bit\\lib\\site-packages\\zmq\\backend\\cython\\checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc (zmq\\backend\\cython\\socket.c:9621)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "rescale_factor = 255\n",
    "train_datagen = ImageDataGenerator(\n",
    "                                    featurewise_center=True\n",
    "                                    , featurewise_std_normalization=True\n",
    "#                                    , rotation_range=45., width_shift_range=0.2, height_shift_range=0.2\n",
    "#                                    , \n",
    "#                                     horizontal_flip=True\n",
    "#                                     , rescale=1./rescale_factor\n",
    "                                    )\n",
    "train_datagen.fit(x_train)\n",
    "print(len(x_train))\n",
    "\n",
    "for batch in train_datagen.flow(x_train, batch_size=50):    \n",
    "    print(type(batch))\n",
    "    print(batch.shape)\n",
    "\n",
    "\n",
    "# print('x_train[0]', x_train[index])\n",
    "# for index in range(len(x_train)):\n",
    "#     np.savetxt('x_train_' + str(index) + '.csv', x_train[index], delimiter=',')\n",
    "\n",
    "#     for batch in train_datagen.flow(np.reshape(x_train[index], (1,*image_shape))\n",
    "#                                     , batch_size=1):\n",
    "#         np.savetxt('x_train_preprocessed_' + str(index) + '.csv', np.reshape(batch, image_shape), delimiter=',')\n",
    "#     #     print(type(batch))\n",
    "#     #     print(batch.shape)\n",
    "#         break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files (x86)\\Python35-64bit\\lib\\site-packages\\keras\\preprocessing\\image.py:648: UserWarning: Expected input to be images (as Numpy array) following the data format convention \"channels_last\" (channels on axis 3), i.e. expected either 1, 3 or 4 channels on axis 3. However, it was passed an array with shape (3588, 48, 48, 1) (1 channels).\n",
      "  ' (' + str(x.shape[self.channel_axis]) + ' channels).')\n"
     ]
    }
   ],
   "source": [
    "# preprocess: \n",
    "rescale_factor = 255\n",
    "train_datagen = ImageDataGenerator(\n",
    "#                                     featurewise_center=True, featurewise_std_normalization=True\n",
    "#                                    , rotation_range=45., width_shift_range=0.2, height_shift_range=0.2\n",
    "#                                    , \n",
    "                                    horizontal_flip=True\n",
    "#                                     , rescale=1./rescale_factor\n",
    "                                    )\n",
    "# train_datagen.fit(x_train)\n",
    "\n",
    "test_datagen = ImageDataGenerator(featurewise_center=True, featurewise_std_normalization=True\n",
    "                                  , rescale=1./rescale_factor)\n",
    "test_datagen.fit(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "happy\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD8CAYAAABXXhlaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2113a9fdf28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visual after preprocess\n",
    "index = 8\n",
    "\n",
    "img_array = np.reshape(x_train[index], (48,48))\n",
    "plt.imshow(img_array, cmap='gray')\n",
    "print(emotion_dict[y_train[index,0]])\n",
    "\n",
    "# visual after preprocess\n",
    "i = 0\n",
    "for batch in train_datagen.flow(np.reshape(x_train[index], (1,*image_shape))\n",
    "                                , batch_size=32, save_to_dir='preview', save_format='jpeg'):\n",
    "    i += 1\n",
    "    if i > 20:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build model & train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# one-hot encode\n",
    "y_train_onehot = keras.utils.to_categorical(y_train, n_classes)\n",
    "y_test_onehot = keras.utils.to_categorical(y_test, n_classes)\n",
    "y_private_onehot = keras.utils.to_categorical(y_private, n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 48, 48, 1)\n",
      "(50, 1)\n",
      "(50, 7)\n",
      "(50, 48, 48, 1)\n",
      "(50, 1)\n",
      "(50, 7)\n",
      "(3589, 48, 48, 1)\n",
      "(3589, 1)\n",
      "(3589, 7)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(y_train_onehot.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)\n",
    "print(y_test_onehot.shape)\n",
    "print(x_private.shape)\n",
    "print(y_private.shape)\n",
    "print(y_private_onehot.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "epochs = 10\n",
    "batch_size = 256\n",
    "n_train = len(x_train)\n",
    "n_test = len(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# simple model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files (x86)\\Python35-64bit\\lib\\site-packages\\keras\\preprocessing\\image.py:648: UserWarning: Expected input to be images (as Numpy array) following the data format convention \"channels_last\" (channels on axis 3), i.e. expected either 1, 3 or 4 channels on axis 3. However, it was passed an array with shape (28698, 48, 48, 1) (1 channels).\n",
      "  ' (' + str(x.shape[self.channel_axis]) + ' channels).')\n",
      "C:\\Program Files (x86)\\Python35-64bit\\lib\\site-packages\\keras\\preprocessing\\image.py:648: UserWarning: Expected input to be images (as Numpy array) following the data format convention \"channels_last\" (channels on axis 3), i.e. expected either 1, 3 or 4 channels on axis 3. However, it was passed an array with shape (3588, 48, 48, 1) (1 channels).\n",
      "  ' (' + str(x.shape[self.channel_axis]) + ' channels).')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 48, 48, 20)        340       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 24, 24, 20)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 11520)             0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 11520)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               1152100   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 7)                 707       \n",
      "=================================================================\n",
      "Total params: 1,153,147\n",
      "Trainable params: 1,153,147\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "112/112 [==============================] - 39s - loss: 1.6884 - acc: 0.3300 - val_loss: 1.5708 - val_acc: 0.4046\n",
      "Epoch 2/10\n",
      "112/112 [==============================] - 37s - loss: 1.5396 - acc: 0.4112 - val_loss: 1.5037 - val_acc: 0.4244\n",
      "Epoch 3/10\n",
      "112/112 [==============================] - 37s - loss: 1.4556 - acc: 0.4474 - val_loss: 1.4612 - val_acc: 0.4418\n",
      "Epoch 4/10\n",
      "112/112 [==============================] - 37s - loss: 1.3930 - acc: 0.4701 - val_loss: 1.4255 - val_acc: 0.4514\n",
      "Epoch 5/10\n",
      "112/112 [==============================] - 37s - loss: 1.3411 - acc: 0.4957 - val_loss: 1.3959 - val_acc: 0.4646\n",
      "Epoch 6/10\n",
      "112/112 [==============================] - 38s - loss: 1.2943 - acc: 0.5112 - val_loss: 1.3785 - val_acc: 0.4710\n",
      "Epoch 7/10\n",
      "112/112 [==============================] - 38s - loss: 1.2405 - acc: 0.5321 - val_loss: 1.3624 - val_acc: 0.4796\n",
      "Epoch 8/10\n",
      "112/112 [==============================] - 37s - loss: 1.1976 - acc: 0.5510 - val_loss: 1.3527 - val_acc: 0.4850\n",
      "Epoch 9/10\n",
      "112/112 [==============================] - 38s - loss: 1.1573 - acc: 0.5694 - val_loss: 1.3486 - val_acc: 0.4919\n",
      "Epoch 10/10\n",
      "112/112 [==============================] - 37s - loss: 1.1147 - acc: 0.5888 - val_loss: 1.3914 - val_acc: 0.4871\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x21124d1a978>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preprocess: normalized\n",
    "train_datagen = ImageDataGenerator(featurewise_center=True, featurewise_std_normalization=True)\n",
    "train_datagen.fit(x_train)\n",
    "\n",
    "test_datagen = ImageDataGenerator(featurewise_center=True, featurewise_std_normalization=True)\n",
    "test_datagen.fit(x_test)\n",
    "\n",
    "# build model\n",
    "model_name = 'simple'\n",
    "model = Sequential(name=model_name)\n",
    "model.add(Conv2D(20, (4,4), padding='same', activation='relu', input_shape=image_shape))\n",
    "model.add(MaxPooling2D((2,2), strides=(2,2), padding='valid'))\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(n_classes, activation='softmax'))\n",
    "model.summary()\n",
    "\n",
    "sgd = optimizers.SGD(momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "\n",
    "model.fit_generator(train_datagen.flow(x_train, y_train_onehot, batch_size=batch_size)\n",
    "                    , steps_per_epoch=n_train//batch_size\n",
    "                    , epochs=epochs\n",
    "                    , validation_data=test_datagen.flow(x_test, y_test_onehot, batch_size=batch_size)\n",
    "                    , validation_steps=n_test//batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.376213568009566, 0.49002481551622806]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files (x86)\\Python35-64bit\\lib\\site-packages\\keras\\preprocessing\\image.py:648: UserWarning: Expected input to be images (as Numpy array) following the data format convention \"channels_last\" (channels on axis 3), i.e. expected either 1, 3 or 4 channels on axis 3. However, it was passed an array with shape (3589, 48, 48, 1) (1 channels).\n",
      "  ' (' + str(x.shape[self.channel_axis]) + ' channels).')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.3278350113195807, 0.51388140823276496]\n"
     ]
    }
   ],
   "source": [
    "# save and evaluate model\n",
    "optimizer_name = 'sgd'\n",
    "model.save('./Model/' + model_name + '_' + optimizer_name +'.h5')\n",
    "model.save_weights('./Model/' + model_name + '_' + optimizer_name + '_weights.h5')\n",
    "print(model.evaluate_generator(test_datagen.flow(x_test, y_test_onehot, batch_size=batch_size), steps=batch_size))\n",
    "\n",
    "# test model\n",
    "test_datagen.fit(x_private)\n",
    "print(model.evaluate_generator(test_datagen.flow(x_private, y_private_onehot, batch_size=batch_size), steps=batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "112/112 [==============================] - 39s - loss: 1.2560 - acc: 0.5282 - val_loss: 1.3624 - val_acc: 0.4883\n",
      "Epoch 2/10\n",
      "112/112 [==============================] - 38s - loss: 1.1269 - acc: 0.5813 - val_loss: 1.4057 - val_acc: 0.4847\n",
      "Epoch 3/10\n",
      "112/112 [==============================] - 38s - loss: 1.0410 - acc: 0.6174 - val_loss: 1.3924 - val_acc: 0.4922\n",
      "Epoch 4/10\n",
      "112/112 [==============================] - 38s - loss: 0.9551 - acc: 0.6508 - val_loss: 1.4071 - val_acc: 0.4940\n",
      "Epoch 5/10\n",
      "112/112 [==============================] - 38s - loss: 0.8872 - acc: 0.6719 - val_loss: 1.4798 - val_acc: 0.4922\n",
      "Epoch 6/10\n",
      "112/112 [==============================] - 39s - loss: 0.8119 - acc: 0.7057 - val_loss: 1.5014 - val_acc: 0.4958\n",
      "Epoch 7/10\n",
      "112/112 [==============================] - 38s - loss: 0.7439 - acc: 0.7322 - val_loss: 1.5120 - val_acc: 0.5048\n",
      "Epoch 8/10\n",
      "112/112 [==============================] - 38s - loss: 0.6827 - acc: 0.7584 - val_loss: 1.5322 - val_acc: 0.5027\n",
      "Epoch 9/10\n",
      "112/112 [==============================] - 38s - loss: 0.6297 - acc: 0.7743 - val_loss: 1.6599 - val_acc: 0.5003\n",
      "Epoch 10/10\n",
      "112/112 [==============================] - 40s - loss: 0.5788 - acc: 0.7910 - val_loss: 1.6597 - val_acc: 0.5027\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x21136ec1ef0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adam = optimizers.Adam()\n",
    "model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "\n",
    "model.fit_generator(train_datagen.flow(x_train, y_train_onehot, batch_size=batch_size)\n",
    "                    , steps_per_epoch=n_train//batch_size\n",
    "                    , epochs=epochs\n",
    "                    , validation_data=test_datagen.flow(x_test, y_test_onehot, batch_size=batch_size)\n",
    "                    , validation_steps=n_test//batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.6662029315433067, 0.50280807157317309]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files (x86)\\Python35-64bit\\lib\\site-packages\\keras\\preprocessing\\image.py:648: UserWarning: Expected input to be images (as Numpy array) following the data format convention \"channels_last\" (channels on axis 3), i.e. expected either 1, 3 or 4 channels on axis 3. However, it was passed an array with shape (3589, 48, 48, 1) (1 channels).\n",
      "  ' (' + str(x.shape[self.channel_axis]) + ' channels).')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.57499254521549, 0.51982242244900656]\n"
     ]
    }
   ],
   "source": [
    "# save and evaluate model\n",
    "optimizer_name = 'adam'\n",
    "model.save('./Model/' + model_name + '_' + optimizer_name +'.h5')\n",
    "model.save_weights('./Model/' + model_name + '_' + optimizer_name + '_weights.h5')\n",
    "print(model.evaluate_generator(test_datagen.flow(x_test, y_test_onehot, batch_size=batch_size), steps=batch_size))\n",
    "\n",
    "# test model\n",
    "test_datagen.fit(x_private)\n",
    "print(model.evaluate_generator(test_datagen.flow(x_private, y_private_onehot, batch_size=batch_size), steps=batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1st place custom model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files (x86)\\Python35-64bit\\lib\\site-packages\\keras\\preprocessing\\image.py:648: UserWarning: Expected input to be images (as Numpy array) following the data format convention \"channels_last\" (channels on axis 3), i.e. expected either 1, 3 or 4 channels on axis 3. However, it was passed an array with shape (28698, 48, 48, 1) (1 channels).\n",
      "  ' (' + str(x.shape[self.channel_axis]) + ' channels).')\n",
      "C:\\Program Files (x86)\\Python35-64bit\\lib\\site-packages\\keras\\preprocessing\\image.py:648: UserWarning: Expected input to be images (as Numpy array) following the data format convention \"channels_last\" (channels on axis 3), i.e. expected either 1, 3 or 4 channels on axis 3. However, it was passed an array with shape (3588, 48, 48, 1) (1 channels).\n",
      "  ' (' + str(x.shape[self.channel_axis]) + ' channels).')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "cropping2d_2 (Cropping2D)    (None, 42, 42, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 42, 42, 32)        832       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 21, 21, 32)        0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_2 (ZeroPaddin (None, 23, 23, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 20, 20, 32)        16416     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 10, 10, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 10, 10, 64)        51264     \n",
      "_________________________________________________________________\n",
      "average_pooling2d_3 (Average (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 3072)              4918272   \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 7)                 21511     \n",
      "=================================================================\n",
      "Total params: 5,008,295\n",
      "Trainable params: 5,008,295\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "112/112 [==============================] - 144s - loss: 1.8182 - acc: 0.2493 - val_loss: 1.8134 - val_acc: 0.2497\n",
      "Epoch 2/10\n",
      "112/112 [==============================] - 144s - loss: 1.8120 - acc: 0.2535 - val_loss: 1.8277 - val_acc: 0.2395\n",
      "Epoch 3/10\n",
      "112/112 [==============================] - 142s - loss: 1.8112 - acc: 0.2523 - val_loss: 1.8124 - val_acc: 0.2506\n",
      "Epoch 4/10\n",
      "112/112 [==============================] - 144s - loss: 1.8118 - acc: 0.2521 - val_loss: 1.8172 - val_acc: 0.2506\n",
      "Epoch 5/10\n",
      "112/112 [==============================] - 142s - loss: 1.8119 - acc: 0.2498 - val_loss: 1.8098 - val_acc: 0.2455\n",
      "Epoch 6/10\n",
      "112/112 [==============================] - 141s - loss: 1.8106 - acc: 0.2532 - val_loss: 1.8112 - val_acc: 0.2494\n",
      "Epoch 7/10\n",
      "112/112 [==============================] - 140s - loss: 1.8140 - acc: 0.2498 - val_loss: 1.8177 - val_acc: 0.2452\n",
      "Epoch 8/10\n",
      "112/112 [==============================] - 141s - loss: 1.8110 - acc: 0.2520 - val_loss: 1.8167 - val_acc: 0.2476\n",
      "Epoch 9/10\n",
      "112/112 [==============================] - 140s - loss: 1.8118 - acc: 0.2488 - val_loss: 1.8128 - val_acc: 0.2485\n",
      "Epoch 10/10\n",
      "112/112 [==============================] - 141s - loss: 1.8105 - acc: 0.2519 - val_loss: 1.8130 - val_acc: 0.2509\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2112563c748>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preprocess\n",
    "rescale_factor = 255\n",
    "train_datagen = ImageDataGenerator(featurewise_center=True, featurewise_std_normalization=True\n",
    "                                   , rotation_range=45., width_shift_range=0.2, height_shift_range=0.2\n",
    "                                   , vertical_flip=True, rescale=1./rescale_factor)\n",
    "train_datagen.fit(x_train)\n",
    "\n",
    "test_datagen = ImageDataGenerator(featurewise_center=True, featurewise_std_normalization=True\n",
    "                                  , rescale=1./rescale_factor)\n",
    "test_datagen.fit(x_test)\n",
    "\n",
    "# build model\n",
    "model_name = '1st_place_custom'\n",
    "model = Sequential(name=model_name)\n",
    "model.add(Cropping2D(cropping=3, input_shape=image_shape))\n",
    "model.add(Conv2D(32, (5,5), padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D((2,2), strides=(2,2), padding='valid'))\n",
    "\n",
    "model.add(ZeroPadding2D(padding=1))\n",
    "model.add(Conv2D(32, (4,4), padding='valid', activation='relu'))\n",
    "model.add(MaxPooling2D((2,2), strides=(2,2), padding='valid'))\n",
    "\n",
    "model.add(Conv2D(64, (5,5), padding='same', activation='relu'))\n",
    "model.add(AveragePooling2D((2,2), strides=(2,2), padding='valid'))\n",
    "\n",
    "model.add(Flatten())\n",
    "# TODO is this where dropout should be?\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(3072, activation='relu'))\n",
    "# TODO svm activation function\n",
    "model.add(Dense(n_classes, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "sgd = optimizers.SGD(momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "model.fit_generator(train_datagen.flow(x_train, y_train_onehot, batch_size=batch_size)\n",
    "                    , steps_per_epoch=n_train//batch_size\n",
    "                    , epochs=epochs\n",
    "                    , validation_data=test_datagen.flow(x_test, y_test_onehot, batch_size=batch_size)\n",
    "                    , validation_steps=n_test//batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.8128060213863861, 0.24957552406452035]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files (x86)\\Python35-64bit\\lib\\site-packages\\keras\\preprocessing\\image.py:648: UserWarning: Expected input to be images (as Numpy array) following the data format convention \"channels_last\" (channels on axis 3), i.e. expected either 1, 3 or 4 channels on axis 3. However, it was passed an array with shape (3589, 48, 48, 1) (1 channels).\n",
      "  ' (' + str(x.shape[self.channel_axis]) + ' channels).')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.8162171843664057, 0.2449689076104952]\n"
     ]
    }
   ],
   "source": [
    "# save and evaluate model\n",
    "optimizer_name = 'sgd'\n",
    "model.save('./Model/' + model_name + '_' + optimizer_name +'.h5')\n",
    "model.save_weights('./Model/' + model_name + '_' + optimizer_name + '_weights.h5')\n",
    "print(model.evaluate_generator(test_datagen.flow(x_test, y_test_onehot, batch_size=batch_size), steps=batch_size))\n",
    "\n",
    "# test model\n",
    "test_datagen.fit(x_private)\n",
    "print(model.evaluate_generator(test_datagen.flow(x_private, y_private_onehot, batch_size=batch_size), steps=batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "112/112 [==============================] - 143s - loss: 1.8102 - acc: 0.2514 - val_loss: 1.8123 - val_acc: 0.2486\n",
      "Epoch 2/30\n",
      "112/112 [==============================] - 140s - loss: 1.8101 - acc: 0.2514 - val_loss: 1.8211 - val_acc: 0.2377\n",
      "Epoch 3/30\n",
      "112/112 [==============================] - 140s - loss: 1.8092 - acc: 0.2520 - val_loss: 1.8121 - val_acc: 0.2527\n",
      "Epoch 4/30\n",
      "112/112 [==============================] - 139s - loss: 1.8088 - acc: 0.2529 - val_loss: 1.8067 - val_acc: 0.2491\n",
      "Epoch 5/30\n",
      "112/112 [==============================] - 140s - loss: 1.8112 - acc: 0.2501 - val_loss: 1.8106 - val_acc: 0.2563\n",
      "Epoch 6/30\n",
      "112/112 [==============================] - 140s - loss: 1.8084 - acc: 0.2522 - val_loss: 1.8111 - val_acc: 0.2492\n",
      "Epoch 7/30\n",
      "112/112 [==============================] - 140s - loss: 1.8104 - acc: 0.2516 - val_loss: 1.8113 - val_acc: 0.2374\n",
      "Epoch 8/30\n",
      "112/112 [==============================] - 140s - loss: 1.8097 - acc: 0.2511 - val_loss: 1.8195 - val_acc: 0.2446\n",
      "Epoch 9/30\n",
      "112/112 [==============================] - 141s - loss: 1.8084 - acc: 0.2533 - val_loss: 1.8125 - val_acc: 0.2455\n",
      "Epoch 10/30\n",
      "112/112 [==============================] - 140s - loss: 1.8096 - acc: 0.2512 - val_loss: 1.8042 - val_acc: 0.2551\n",
      "Epoch 11/30\n",
      "112/112 [==============================] - 141s - loss: 1.8126 - acc: 0.2505 - val_loss: 1.8115 - val_acc: 0.2492\n",
      "Epoch 12/30\n",
      "112/112 [==============================] - 141s - loss: 1.8076 - acc: 0.2523 - val_loss: 1.8160 - val_acc: 0.2458\n",
      "Epoch 13/30\n",
      "112/112 [==============================] - 140s - loss: 1.8106 - acc: 0.2508 - val_loss: 1.8130 - val_acc: 0.2497\n",
      "Epoch 14/30\n",
      "112/112 [==============================] - 141s - loss: 1.8107 - acc: 0.2498 - val_loss: 1.8119 - val_acc: 0.2461\n",
      "Epoch 15/30\n",
      "112/112 [==============================] - 140s - loss: 1.8116 - acc: 0.2504 - val_loss: 1.8026 - val_acc: 0.2626\n",
      "Epoch 16/30\n",
      "112/112 [==============================] - 141s - loss: 1.8078 - acc: 0.2528 - val_loss: 1.8118 - val_acc: 0.2494\n",
      "Epoch 17/30\n",
      "112/112 [==============================] - 140s - loss: 1.8104 - acc: 0.2515 - val_loss: 1.8042 - val_acc: 0.2455\n",
      "Epoch 18/30\n",
      "112/112 [==============================] - 142s - loss: 1.8102 - acc: 0.2522 - val_loss: 1.8126 - val_acc: 0.2530\n",
      "Epoch 19/30\n",
      "112/112 [==============================] - 141s - loss: 1.8094 - acc: 0.2516 - val_loss: 1.8179 - val_acc: 0.2413\n",
      "Epoch 20/30\n",
      "112/112 [==============================] - 141s - loss: 1.8113 - acc: 0.2496 - val_loss: 1.8240 - val_acc: 0.2437\n",
      "Epoch 21/30\n",
      "112/112 [==============================] - 141s - loss: 1.8101 - acc: 0.2524 - val_loss: 1.8113 - val_acc: 0.2494\n",
      "Epoch 22/30\n",
      "112/112 [==============================] - 141s - loss: 1.8083 - acc: 0.2515 - val_loss: 1.8203 - val_acc: 0.2395\n",
      "Epoch 23/30\n",
      "112/112 [==============================] - 142s - loss: 1.8105 - acc: 0.2530 - val_loss: 1.8071 - val_acc: 0.2533\n",
      "Epoch 24/30\n",
      "112/112 [==============================] - 140s - loss: 1.8136 - acc: 0.2476 - val_loss: 1.8104 - val_acc: 0.2518\n",
      "Epoch 25/30\n",
      "112/112 [==============================] - 140s - loss: 1.8114 - acc: 0.2485 - val_loss: 1.8111 - val_acc: 0.2491\n",
      "Epoch 26/30\n",
      "112/112 [==============================] - 142s - loss: 1.8090 - acc: 0.2518 - val_loss: 1.8114 - val_acc: 0.2492\n",
      "Epoch 27/30\n",
      "112/112 [==============================] - 140s - loss: 1.8100 - acc: 0.2526 - val_loss: 1.8156 - val_acc: 0.2443\n",
      "Epoch 28/30\n",
      "112/112 [==============================] - 141s - loss: 1.8090 - acc: 0.2516 - val_loss: 1.8199 - val_acc: 0.2455\n",
      "Epoch 29/30\n",
      "112/112 [==============================] - 140s - loss: 1.8094 - acc: 0.2488 - val_loss: 1.8155 - val_acc: 0.2377\n",
      "Epoch 30/30\n",
      "112/112 [==============================] - 141s - loss: 1.8111 - acc: 0.2534 - val_loss: 1.8194 - val_acc: 0.2338\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2112b96ee10>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 30\n",
    "adam = optimizers.Adam()\n",
    "model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "\n",
    "model.fit_generator(train_datagen.flow(x_train, y_train_onehot, batch_size=batch_size)\n",
    "                    , steps_per_epoch=n_train//batch_size\n",
    "                    , epochs=epochs\n",
    "                    , validation_data=test_datagen.flow(x_test, y_test_onehot, batch_size=batch_size)\n",
    "                    , validation_steps=n_test//batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.8110208575343942, 0.2494938940769281]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files (x86)\\Python35-64bit\\lib\\site-packages\\keras\\preprocessing\\image.py:648: UserWarning: Expected input to be images (as Numpy array) following the data format convention \"channels_last\" (channels on axis 3), i.e. expected either 1, 3 or 4 channels on axis 3. However, it was passed an array with shape (3589, 48, 48, 1) (1 channels).\n",
      "  ' (' + str(x.shape[self.channel_axis]) + ' channels).')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.8155259787418596, 0.24475672853110919]\n"
     ]
    }
   ],
   "source": [
    "# save and evaluate model\n",
    "optimizer_name = 'adam'\n",
    "model.save('./Model/' + model_name + '_' + optimizer_name +'.h5')\n",
    "model.save_weights('./Model/' + model_name + '_' + optimizer_name + '_weights.h5')\n",
    "print(model.evaluate_generator(test_datagen.flow(x_test, y_test_onehot, batch_size=batch_size), steps=batch_size))\n",
    "\n",
    "# test model\n",
    "test_datagen.fit(x_private)\n",
    "print(model.evaluate_generator(test_datagen.flow(x_private, y_private_onehot, batch_size=batch_size), steps=batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1st place model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## all preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# preprocess\n",
    "rescale_factor = 255\n",
    "train_datagen = ImageDataGenerator(featurewise_center=True, featurewise_std_normalization=True\n",
    "                                   , rotation_range=45., width_shift_range=0.2, height_shift_range=0.2\n",
    "                                   , vertical_flip=True, rescale=1./rescale_factor)\n",
    "train_datagen.fit(x_train)\n",
    "\n",
    "test_datagen = ImageDataGenerator(featurewise_center=True, featurewise_std_normalization=True\n",
    "                                  , rescale=1./rescale_factor)\n",
    "test_datagen.fit(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# build model\n",
    "model_name = '1st_place'\n",
    "model = Sequential(name=model_name)\n",
    "model.add(Cropping2D(cropping=3, input_shape=image_shape))\n",
    "model.add(Conv2D(32, (5,5), padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D((2,2), strides=(2,2), padding='valid'))\n",
    "\n",
    "model.add(ZeroPadding2D(padding=1))\n",
    "model.add(Conv2D(32, (4,4), padding='valid', activation='relu'))\n",
    "model.add(AveragePooling2D((2,2), strides=(2,2), padding='valid'))\n",
    "\n",
    "model.add(Conv2D(64, (5,5), padding='same', activation='relu'))\n",
    "model.add(AveragePooling2D((2,2), strides=(2,2), padding='valid'))\n",
    "\n",
    "model.add(Flatten())\n",
    "# TODO is this where dropout should be?\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(3072, activation='relu'))\n",
    "# TODO svm activation function\n",
    "model.add(Dense(n_classes, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files (x86)\\Python35-64bit\\lib\\site-packages\\keras\\preprocessing\\image.py:648: UserWarning: Expected input to be images (as Numpy array) following the data format convention \"channels_last\" (channels on axis 3), i.e. expected either 1, 3 or 4 channels on axis 3. However, it was passed an array with shape (28698, 48, 48, 1) (1 channels).\n",
      "  ' (' + str(x.shape[self.channel_axis]) + ' channels).')\n",
      "C:\\Program Files (x86)\\Python35-64bit\\lib\\site-packages\\keras\\preprocessing\\image.py:648: UserWarning: Expected input to be images (as Numpy array) following the data format convention \"channels_last\" (channels on axis 3), i.e. expected either 1, 3 or 4 channels on axis 3. However, it was passed an array with shape (3588, 48, 48, 1) (1 channels).\n",
      "  ' (' + str(x.shape[self.channel_axis]) + ' channels).')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "cropping2d_1 (Cropping2D)    (None, 42, 42, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 42, 42, 32)        832       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 21, 21, 32)        0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_1 (ZeroPaddin (None, 23, 23, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 20, 20, 32)        16416     \n",
      "_________________________________________________________________\n",
      "average_pooling2d_1 (Average (None, 10, 10, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 10, 10, 64)        51264     \n",
      "_________________________________________________________________\n",
      "average_pooling2d_2 (Average (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 3072)              4918272   \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 7)                 21511     \n",
      "=================================================================\n",
      "Total params: 5,008,295\n",
      "Trainable params: 5,008,295\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "112/112 [==============================] - 148s - loss: 1.8179 - acc: 0.2503 - val_loss: 1.8116 - val_acc: 0.2494\n",
      "Epoch 2/10\n",
      "112/112 [==============================] - 145s - loss: 1.8123 - acc: 0.2512 - val_loss: 1.8155 - val_acc: 0.2551\n",
      "Epoch 3/10\n",
      "112/112 [==============================] - 146s - loss: 1.8130 - acc: 0.2495 - val_loss: 1.8106 - val_acc: 0.2536\n",
      "Epoch 4/10\n",
      "112/112 [==============================] - 146s - loss: 1.8115 - acc: 0.2517 - val_loss: 1.8131 - val_acc: 0.2470\n",
      "Epoch 5/10\n",
      "112/112 [==============================] - 145s - loss: 1.8103 - acc: 0.2531 - val_loss: 1.8097 - val_acc: 0.2440\n",
      "Epoch 6/10\n",
      "112/112 [==============================] - 147s - loss: 1.8120 - acc: 0.2490 - val_loss: 1.8150 - val_acc: 0.2492\n",
      "Epoch 7/10\n",
      "112/112 [==============================] - 146s - loss: 1.8115 - acc: 0.2504 - val_loss: 1.8154 - val_acc: 0.2443\n",
      "Epoch 8/10\n",
      "112/112 [==============================] - 146s - loss: 1.8133 - acc: 0.2512 - val_loss: 1.8156 - val_acc: 0.2467\n",
      "Epoch 9/10\n",
      "112/112 [==============================] - 146s - loss: 1.8099 - acc: 0.2532 - val_loss: 1.8127 - val_acc: 0.2506\n",
      "Epoch 10/10\n",
      "112/112 [==============================] - 147s - loss: 1.8115 - acc: 0.2499 - val_loss: 1.8090 - val_acc: 0.2578\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x21134dad630>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compile model\n",
    "sgd = optimizers.SGD(momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "# fit model\n",
    "model.fit_generator(train_datagen.flow(x_train, y_train_onehot, batch_size=batch_size)\n",
    "                    , steps_per_epoch=n_train//batch_size\n",
    "                    , epochs=epochs\n",
    "                    , validation_data=test_datagen.flow(x_test, y_test_onehot, batch_size=batch_size)\n",
    "                    , validation_steps=n_test//batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### all preprocessing + sgd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.8115212035928041, 0.24959185006203879]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files (x86)\\Python35-64bit\\lib\\site-packages\\keras\\preprocessing\\image.py:648: UserWarning: Expected input to be images (as Numpy array) following the data format convention \"channels_last\" (channels on axis 3), i.e. expected either 1, 3 or 4 channels on axis 3. However, it was passed an array with shape (3589, 48, 48, 1) (1 channels).\n",
      "  ' (' + str(x.shape[self.channel_axis]) + ' channels).')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.8155006809269953, 0.24491994320737287]\n"
     ]
    }
   ],
   "source": [
    "# save and evaluate model\n",
    "optimizer_name = 'sgd'\n",
    "model.save('./Model/' + model_name + '_' + optimizer_name +'.h5')\n",
    "model.save_weights('./Model/' + model_name + '_' + optimizer_name + '_weights.h5')\n",
    "print(model.evaluate_generator(test_datagen.flow(x_test, y_test_onehot, batch_size=batch_size), steps=batch_size))\n",
    "\n",
    "# test model\n",
    "test_datagen.fit(x_private)\n",
    "print(model.evaluate_generator(test_datagen.flow(x_private, y_private_onehot, batch_size=batch_size), steps=batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### all preprocessing + adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "112/112 [==============================] - 146s - loss: 1.8104 - acc: 0.2514 - val_loss: 1.8120 - val_acc: 0.2497\n",
      "Epoch 2/30\n",
      "112/112 [==============================] - 143s - loss: 1.8101 - acc: 0.2510 - val_loss: 1.8164 - val_acc: 0.2467\n",
      "Epoch 3/30\n",
      "112/112 [==============================] - 143s - loss: 1.8087 - acc: 0.2529 - val_loss: 1.8126 - val_acc: 0.2476\n",
      "Epoch 4/30\n",
      "112/112 [==============================] - 143s - loss: 1.8104 - acc: 0.2505 - val_loss: 1.8121 - val_acc: 0.2515\n",
      "Epoch 5/30\n",
      "112/112 [==============================] - 143s - loss: 1.8106 - acc: 0.2508 - val_loss: 1.8144 - val_acc: 0.2410\n",
      "Epoch 6/30\n",
      "112/112 [==============================] - 143s - loss: 1.8110 - acc: 0.2492 - val_loss: 1.8128 - val_acc: 0.2494\n",
      "Epoch 7/30\n",
      "112/112 [==============================] - 144s - loss: 1.8095 - acc: 0.2528 - val_loss: 1.8076 - val_acc: 0.2482\n",
      "Epoch 8/30\n",
      "112/112 [==============================] - 143s - loss: 1.8106 - acc: 0.2516 - val_loss: 1.8055 - val_acc: 0.2548\n",
      "Epoch 9/30\n",
      "112/112 [==============================] - 143s - loss: 1.8107 - acc: 0.2500 - val_loss: 1.8152 - val_acc: 0.2470\n",
      "Epoch 10/30\n",
      "112/112 [==============================] - 142s - loss: 1.8100 - acc: 0.2511 - val_loss: 1.8051 - val_acc: 0.2536\n",
      "Epoch 11/30\n",
      "112/112 [==============================] - 143s - loss: 1.8115 - acc: 0.2500 - val_loss: 1.8109 - val_acc: 0.2494\n",
      "Epoch 12/30\n",
      "112/112 [==============================] - 144s - loss: 1.8065 - acc: 0.2532 - val_loss: 1.8088 - val_acc: 0.2461\n",
      "Epoch 13/30\n",
      "112/112 [==============================] - 142s - loss: 1.8132 - acc: 0.2511 - val_loss: 1.8219 - val_acc: 0.2386\n",
      "Epoch 14/30\n",
      "112/112 [==============================] - 139s - loss: 1.8123 - acc: 0.2498 - val_loss: 1.8178 - val_acc: 0.2449\n",
      "Epoch 15/30\n",
      "112/112 [==============================] - 140s - loss: 1.8094 - acc: 0.2516 - val_loss: 1.8125 - val_acc: 0.2533\n",
      "Epoch 16/30\n",
      "112/112 [==============================] - 140s - loss: 1.8101 - acc: 0.2532 - val_loss: 1.8116 - val_acc: 0.2497\n",
      "Epoch 17/30\n",
      "112/112 [==============================] - 140s - loss: 1.8090 - acc: 0.2506 - val_loss: 1.8081 - val_acc: 0.2551\n",
      "Epoch 18/30\n",
      "112/112 [==============================] - 140s - loss: 1.8101 - acc: 0.2522 - val_loss: 1.8088 - val_acc: 0.2491\n",
      "Epoch 19/30\n",
      "112/112 [==============================] - 140s - loss: 1.8108 - acc: 0.2492 - val_loss: 1.8196 - val_acc: 0.2467\n",
      "Epoch 20/30\n",
      "112/112 [==============================] - 140s - loss: 1.8096 - acc: 0.2523 - val_loss: 1.8073 - val_acc: 0.2554\n",
      "Epoch 21/30\n",
      "112/112 [==============================] - 140s - loss: 1.8080 - acc: 0.2510 - val_loss: 1.8105 - val_acc: 0.2497\n",
      "Epoch 22/30\n",
      "112/112 [==============================] - 140s - loss: 1.8111 - acc: 0.2514 - val_loss: 1.8082 - val_acc: 0.2575\n",
      "Epoch 23/30\n",
      "112/112 [==============================] - 139s - loss: 1.8096 - acc: 0.2518 - val_loss: 1.8139 - val_acc: 0.2521\n",
      "Epoch 24/30\n",
      "112/112 [==============================] - 140s - loss: 1.8099 - acc: 0.2497 - val_loss: 1.8108 - val_acc: 0.2491\n",
      "Epoch 25/30\n",
      "112/112 [==============================] - 140s - loss: 1.8086 - acc: 0.2508 - val_loss: 1.8123 - val_acc: 0.2542\n",
      "Epoch 26/30\n",
      "112/112 [==============================] - 140s - loss: 1.8104 - acc: 0.2541 - val_loss: 1.8112 - val_acc: 0.2497\n",
      "Epoch 27/30\n",
      "112/112 [==============================] - 139s - loss: 1.8118 - acc: 0.2489 - val_loss: 1.8098 - val_acc: 0.2551\n",
      "Epoch 28/30\n",
      "112/112 [==============================] - 140s - loss: 1.8099 - acc: 0.2518 - val_loss: 1.8128 - val_acc: 0.2461\n",
      "Epoch 29/30\n",
      "112/112 [==============================] - 140s - loss: 1.8090 - acc: 0.2536 - val_loss: 1.8088 - val_acc: 0.2539\n",
      "Epoch 30/30\n",
      "112/112 [==============================] - 140s - loss: 1.8103 - acc: 0.2491 - val_loss: 1.8162 - val_acc: 0.2518\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2112b537eb8>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compile model\n",
    "epochs = 30\n",
    "adam = optimizers.Adam()\n",
    "model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "\n",
    "# fit model\n",
    "model.fit_generator(train_datagen.flow(x_train, y_train_onehot, batch_size=batch_size)\n",
    "                    , steps_per_epoch=n_train//batch_size\n",
    "                    , epochs=epochs\n",
    "                    , validation_data=test_datagen.flow(x_test, y_test_onehot, batch_size=batch_size)\n",
    "                    , validation_steps=n_test//batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.8113366597810281, 0.24960817605955724]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files (x86)\\Python35-64bit\\lib\\site-packages\\keras\\preprocessing\\image.py:648: UserWarning: Expected input to be images (as Numpy array) following the data format convention \"channels_last\" (channels on axis 3), i.e. expected either 1, 3 or 4 channels on axis 3. However, it was passed an array with shape (3589, 48, 48, 1) (1 channels).\n",
      "  ' (' + str(x.shape[self.channel_axis]) + ' channels).')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.8151999946871706, 0.2448220144052628]\n"
     ]
    }
   ],
   "source": [
    "# save and evaluate model\n",
    "optimizer_name = 'adam'\n",
    "model.save('./Model/' + model_name + '_' + optimizer_name +'.h5')\n",
    "model.save_weights('./Model/' + model_name + '_' + optimizer_name + '_weights.h5')\n",
    "print(model.evaluate_generator(test_datagen.flow(x_test, y_test_onehot, batch_size=batch_size), steps=batch_size))\n",
    "\n",
    "# test model\n",
    "test_datagen.fit(x_private)\n",
    "print(model.evaluate_generator(test_datagen.flow(x_private, y_private_onehot, batch_size=batch_size), steps=batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocess: normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files (x86)\\Python35-64bit\\lib\\site-packages\\keras\\preprocessing\\image.py:648: UserWarning: Expected input to be images (as Numpy array) following the data format convention \"channels_last\" (channels on axis 3), i.e. expected either 1, 3 or 4 channels on axis 3. However, it was passed an array with shape (28698, 48, 48, 1) (1 channels).\n",
      "  ' (' + str(x.shape[self.channel_axis]) + ' channels).')\n",
      "C:\\Program Files (x86)\\Python35-64bit\\lib\\site-packages\\keras\\preprocessing\\image.py:648: UserWarning: Expected input to be images (as Numpy array) following the data format convention \"channels_last\" (channels on axis 3), i.e. expected either 1, 3 or 4 channels on axis 3. However, it was passed an array with shape (3588, 48, 48, 1) (1 channels).\n",
      "  ' (' + str(x.shape[self.channel_axis]) + ' channels).')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "cropping2d_3 (Cropping2D)    (None, 42, 42, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 42, 42, 32)        832       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 21, 21, 32)        0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_3 (ZeroPaddin (None, 23, 23, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 20, 20, 32)        16416     \n",
      "_________________________________________________________________\n",
      "average_pooling2d_4 (Average (None, 10, 10, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 10, 10, 64)        51264     \n",
      "_________________________________________________________________\n",
      "average_pooling2d_5 (Average (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 3072)              4918272   \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 7)                 21511     \n",
      "=================================================================\n",
      "Total params: 5,008,295\n",
      "Trainable params: 5,008,295\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# preprocess\n",
    "rescale_factor = 255\n",
    "train_datagen = ImageDataGenerator(featurewise_center=True, featurewise_std_normalization=True)\n",
    "train_datagen.fit(x_train)\n",
    "\n",
    "test_datagen = ImageDataGenerator(featurewise_center=True, featurewise_std_normalization=True)\n",
    "test_datagen.fit(x_test)\n",
    "\n",
    "# build model\n",
    "model_name = '1st_place'\n",
    "model = Sequential(name=model_name)\n",
    "model.add(Cropping2D(cropping=3, input_shape=image_shape))\n",
    "model.add(Conv2D(32, (5,5), padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D((2,2), strides=(2,2), padding='valid'))\n",
    "\n",
    "model.add(ZeroPadding2D(padding=1))\n",
    "model.add(Conv2D(32, (4,4), padding='valid', activation='relu'))\n",
    "model.add(AveragePooling2D((2,2), strides=(2,2), padding='valid'))\n",
    "\n",
    "model.add(Conv2D(64, (5,5), padding='same', activation='relu'))\n",
    "model.add(AveragePooling2D((2,2), strides=(2,2), padding='valid'))\n",
    "\n",
    "model.add(Flatten())\n",
    "# TODO is this where dropout should be?\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(3072, activation='relu'))\n",
    "# TODO svm activation function\n",
    "model.add(Dense(n_classes, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### preprocess: normalization + sgd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "112/112 [==============================] - 145s - loss: 1.7749 - acc: 0.2723 - val_loss: 1.6969 - val_acc: 0.3304\n",
      "Epoch 2/30\n",
      "112/112 [==============================] - 141s - loss: 1.6630 - acc: 0.3487 - val_loss: 1.6325 - val_acc: 0.3685\n",
      "Epoch 3/30\n",
      "112/112 [==============================] - 142s - loss: 1.6034 - acc: 0.3823 - val_loss: 1.5691 - val_acc: 0.4043\n",
      "Epoch 4/30\n",
      " 27/112 [======>.......................] - ETA: 101s - loss: 1.5629 - acc: 0.4002"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-55423d8372c2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m                     \u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m                     \u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtest_datagen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test_onehot\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m                     , validation_steps=n_test//batch_size)\n\u001b[0m",
      "\u001b[1;32mC:\\Program Files (x86)\\Python35-64bit\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     86\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[0;32m     87\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 88\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_support_signature\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minspect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetargspec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Python35-64bit\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_q_size, workers, pickle_safe, initial_epoch)\u001b[0m\n\u001b[0;32m   1122\u001b[0m                                         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1123\u001b[0m                                         \u001b[0mpickle_safe\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpickle_safe\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1124\u001b[1;33m                                         initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   1125\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1126\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Python35-64bit\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     86\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[0;32m     87\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 88\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_support_signature\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minspect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetargspec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Python35-64bit\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_q_size, workers, pickle_safe, initial_epoch)\u001b[0m\n\u001b[0;32m   1900\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[0;32m   1901\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1902\u001b[1;33m                                                class_weight=class_weight)\n\u001b[0m\u001b[0;32m   1903\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1904\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Python35-64bit\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[0;32m   1640\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1641\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1642\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1643\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1644\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Python35-64bit\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2267\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[0;32m   2268\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2269\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2270\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2271\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Python35-64bit\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    776\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 778\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    779\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Python35-64bit\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    980\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m--> 982\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    983\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Python35-64bit\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1030\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m-> 1032\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m   1033\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Python35-64bit\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1037\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1040\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Python35-64bit\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[0;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1021\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m   1022\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1023\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# compile model\n",
    "sgd = optimizers.SGD(momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "# fit model\n",
    "model.fit_generator(train_datagen.flow(x_train, y_train_onehot, batch_size=batch_size)\n",
    "                    , steps_per_epoch=n_train//batch_size\n",
    "                    , epochs=epochs\n",
    "                    , validation_data=test_datagen.flow(x_test, y_test_onehot, batch_size=batch_size)\n",
    "                    , validation_steps=n_test//batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocess: normalization + rescale + sgd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files (x86)\\Python35-64bit\\lib\\site-packages\\keras\\preprocessing\\image.py:648: UserWarning: Expected input to be images (as Numpy array) following the data format convention \"channels_last\" (channels on axis 3), i.e. expected either 1, 3 or 4 channels on axis 3. However, it was passed an array with shape (28698, 48, 48, 1) (1 channels).\n",
      "  ' (' + str(x.shape[self.channel_axis]) + ' channels).')\n",
      "C:\\Program Files (x86)\\Python35-64bit\\lib\\site-packages\\keras\\preprocessing\\image.py:648: UserWarning: Expected input to be images (as Numpy array) following the data format convention \"channels_last\" (channels on axis 3), i.e. expected either 1, 3 or 4 channels on axis 3. However, it was passed an array with shape (3588, 48, 48, 1) (1 channels).\n",
      "  ' (' + str(x.shape[self.channel_axis]) + ' channels).')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "cropping2d_5 (Cropping2D)    (None, 42, 42, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 42, 42, 32)        832       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 21, 21, 32)        0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_5 (ZeroPaddin (None, 23, 23, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 20, 20, 32)        16416     \n",
      "_________________________________________________________________\n",
      "average_pooling2d_8 (Average (None, 10, 10, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_16 (Conv2D)           (None, 10, 10, 64)        51264     \n",
      "_________________________________________________________________\n",
      "average_pooling2d_9 (Average (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 3072)              4918272   \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 7)                 21511     \n",
      "=================================================================\n",
      "Total params: 5,008,295\n",
      "Trainable params: 5,008,295\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "112/112 [==============================] - 146s - loss: 1.8191 - acc: 0.2492 - val_loss: 1.8126 - val_acc: 0.2494\n",
      "Epoch 2/10\n",
      "112/112 [==============================] - 137s - loss: 1.8120 - acc: 0.2535 - val_loss: 1.8100 - val_acc: 0.2449\n",
      "Epoch 3/10\n",
      "112/112 [==============================] - 137s - loss: 1.8120 - acc: 0.2511 - val_loss: 1.8192 - val_acc: 0.2485\n",
      "Epoch 4/10\n",
      "112/112 [==============================] - 139s - loss: 1.8111 - acc: 0.2526 - val_loss: 1.8093 - val_acc: 0.2479\n",
      "Epoch 5/10\n",
      "112/112 [==============================] - 139s - loss: 1.8128 - acc: 0.2493 - val_loss: 1.8159 - val_acc: 0.2560\n",
      "Epoch 6/10\n",
      "112/112 [==============================] - 140s - loss: 1.8105 - acc: 0.2514 - val_loss: 1.8106 - val_acc: 0.2494\n",
      "Epoch 7/10\n",
      "112/112 [==============================] - 139s - loss: 1.8123 - acc: 0.2507 - val_loss: 1.8149 - val_acc: 0.2494\n",
      "Epoch 8/10\n",
      "112/112 [==============================] - 139s - loss: 1.8116 - acc: 0.2519 - val_loss: 1.8140 - val_acc: 0.2473\n",
      "Epoch 9/10\n",
      "112/112 [==============================] - 139s - loss: 1.8112 - acc: 0.2505 - val_loss: 1.8188 - val_acc: 0.2488\n",
      "Epoch 10/10\n",
      "112/112 [==============================] - 139s - loss: 1.8081 - acc: 0.2538 - val_loss: 1.8154 - val_acc: 0.2461\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x211351b0b70>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preprocess\n",
    "rescale_factor = 255\n",
    "train_datagen = ImageDataGenerator(featurewise_center=True, featurewise_std_normalization=True, rescale=1./rescale_factor)\n",
    "train_datagen.fit(x_train)\n",
    "\n",
    "test_datagen = ImageDataGenerator(featurewise_center=True, featurewise_std_normalization=True, rescale=1./rescale_factor)\n",
    "test_datagen.fit(x_test)\n",
    "\n",
    "# build model\n",
    "model_name = '1st_place'\n",
    "model = Sequential(name=model_name)\n",
    "model.add(Cropping2D(cropping=3, input_shape=image_shape))\n",
    "model.add(Conv2D(32, (5,5), padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D((2,2), strides=(2,2), padding='valid'))\n",
    "\n",
    "model.add(ZeroPadding2D(padding=1))\n",
    "model.add(Conv2D(32, (4,4), padding='valid', activation='relu'))\n",
    "model.add(AveragePooling2D((2,2), strides=(2,2), padding='valid'))\n",
    "\n",
    "model.add(Conv2D(64, (5,5), padding='same', activation='relu'))\n",
    "model.add(AveragePooling2D((2,2), strides=(2,2), padding='valid'))\n",
    "\n",
    "model.add(Flatten())\n",
    "# TODO is this where dropout should be?\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(3072, activation='relu'))\n",
    "# TODO svm activation function\n",
    "model.add(Dense(n_classes, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# compile model\n",
    "sgd = optimizers.SGD(momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "# fit model\n",
    "epochs = 10\n",
    "batch_size = 256\n",
    "model.fit_generator(train_datagen.flow(x_train, y_train_onehot, batch_size=batch_size)\n",
    "                    , steps_per_epoch=n_train//batch_size\n",
    "                    , epochs=epochs\n",
    "                    , validation_data=test_datagen.flow(x_test, y_test_onehot, batch_size=batch_size)\n",
    "                    , validation_steps=n_test//batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocess: normalization + others + sgd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files (x86)\\Python35-64bit\\lib\\site-packages\\keras\\preprocessing\\image.py:648: UserWarning: Expected input to be images (as Numpy array) following the data format convention \"channels_last\" (channels on axis 3), i.e. expected either 1, 3 or 4 channels on axis 3. However, it was passed an array with shape (28698, 48, 48, 1) (1 channels).\n",
      "  ' (' + str(x.shape[self.channel_axis]) + ' channels).')\n",
      "C:\\Program Files (x86)\\Python35-64bit\\lib\\site-packages\\keras\\preprocessing\\image.py:648: UserWarning: Expected input to be images (as Numpy array) following the data format convention \"channels_last\" (channels on axis 3), i.e. expected either 1, 3 or 4 channels on axis 3. However, it was passed an array with shape (3588, 48, 48, 1) (1 channels).\n",
      "  ' (' + str(x.shape[self.channel_axis]) + ' channels).')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "cropping2d_6 (Cropping2D)    (None, 42, 42, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 42, 42, 32)        832       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 21, 21, 32)        0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_6 (ZeroPaddin (None, 23, 23, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_18 (Conv2D)           (None, 20, 20, 32)        16416     \n",
      "_________________________________________________________________\n",
      "average_pooling2d_10 (Averag (None, 10, 10, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_19 (Conv2D)           (None, 10, 10, 64)        51264     \n",
      "_________________________________________________________________\n",
      "average_pooling2d_11 (Averag (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 3072)              4918272   \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 7)                 21511     \n",
      "=================================================================\n",
      "Total params: 5,008,295\n",
      "Trainable params: 5,008,295\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "112/112 [==============================] - 145s - loss: 1.8095 - acc: 0.2466 - val_loss: 1.7778 - val_acc: 0.2634\n",
      "Epoch 2/10\n",
      "112/112 [==============================] - 141s - loss: 1.7856 - acc: 0.2539 - val_loss: 1.7777 - val_acc: 0.2554\n",
      "Epoch 3/10\n",
      "112/112 [==============================] - 146s - loss: 1.7826 - acc: 0.2539 - val_loss: 1.7667 - val_acc: 0.2713\n",
      "Epoch 4/10\n",
      "112/112 [==============================] - 146s - loss: 1.7767 - acc: 0.2592 - val_loss: 1.7607 - val_acc: 0.2572\n",
      "Epoch 5/10\n",
      "112/112 [==============================] - 159s - loss: 1.7766 - acc: 0.2594 - val_loss: 1.7578 - val_acc: 0.2716\n",
      "Epoch 6/10\n",
      "112/112 [==============================] - 168s - loss: 1.7705 - acc: 0.2623 - val_loss: 1.7493 - val_acc: 0.2740\n",
      "Epoch 7/10\n",
      " 16/112 [===>..........................] - ETA: 124s - loss: 1.7656 - acc: 0.2642"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-7279f9ad43d9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     43\u001b[0m                     \u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m                     \u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtest_datagen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test_onehot\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m                     , validation_steps=n_test//batch_size)\n\u001b[0m",
      "\u001b[1;32mC:\\Program Files (x86)\\Python35-64bit\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     86\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[0;32m     87\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 88\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_support_signature\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minspect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetargspec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Python35-64bit\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_q_size, workers, pickle_safe, initial_epoch)\u001b[0m\n\u001b[0;32m   1122\u001b[0m                                         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1123\u001b[0m                                         \u001b[0mpickle_safe\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpickle_safe\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1124\u001b[1;33m                                         initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   1125\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1126\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Python35-64bit\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     86\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[0;32m     87\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 88\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_support_signature\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minspect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetargspec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Python35-64bit\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_q_size, workers, pickle_safe, initial_epoch)\u001b[0m\n\u001b[0;32m   1900\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[0;32m   1901\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1902\u001b[1;33m                                                class_weight=class_weight)\n\u001b[0m\u001b[0;32m   1903\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1904\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Python35-64bit\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[0;32m   1640\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1641\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1642\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1643\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1644\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Python35-64bit\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2267\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[0;32m   2268\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2269\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2270\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2271\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Python35-64bit\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    776\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 778\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    779\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Python35-64bit\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    980\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m--> 982\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    983\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Python35-64bit\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1030\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m-> 1032\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m   1033\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Python35-64bit\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1037\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1040\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Python35-64bit\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[0;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1021\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m   1022\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1023\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# preprocess\n",
    "rescale_factor = 255\n",
    "train_datagen = ImageDataGenerator(featurewise_center=True, featurewise_std_normalization=True\n",
    "                                   , rotation_range=45., width_shift_range=0.2, height_shift_range=0.2\n",
    "                                   , vertical_flip=True)\n",
    "train_datagen.fit(x_train)\n",
    "\n",
    "test_datagen = ImageDataGenerator(featurewise_center=True, featurewise_std_normalization=True)\n",
    "test_datagen.fit(x_test)\n",
    "\n",
    "# build model\n",
    "model_name = '1st_place'\n",
    "model = Sequential(name=model_name)\n",
    "model.add(Cropping2D(cropping=3, input_shape=image_shape))\n",
    "model.add(Conv2D(32, (5,5), padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D((2,2), strides=(2,2), padding='valid'))\n",
    "\n",
    "model.add(ZeroPadding2D(padding=1))\n",
    "model.add(Conv2D(32, (4,4), padding='valid', activation='relu'))\n",
    "model.add(AveragePooling2D((2,2), strides=(2,2), padding='valid'))\n",
    "\n",
    "model.add(Conv2D(64, (5,5), padding='same', activation='relu'))\n",
    "model.add(AveragePooling2D((2,2), strides=(2,2), padding='valid'))\n",
    "\n",
    "model.add(Flatten())\n",
    "# TODO is this where dropout should be?\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(3072, activation='relu'))\n",
    "# TODO svm activation function\n",
    "model.add(Dense(n_classes, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# compile model\n",
    "sgd = optimizers.SGD(momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "# fit model\n",
    "epochs = 10\n",
    "batch_size = 256\n",
    "model.fit_generator(train_datagen.flow(x_train, y_train_onehot, batch_size=batch_size)\n",
    "                    , steps_per_epoch=n_train//batch_size\n",
    "                    , epochs=epochs\n",
    "                    , validation_data=test_datagen.flow(x_test, y_test_onehot, batch_size=batch_size)\n",
    "                    , validation_steps=n_test//batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocess: rescale + sgd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files (x86)\\Python35-64bit\\lib\\site-packages\\keras\\preprocessing\\image.py:648: UserWarning: Expected input to be images (as Numpy array) following the data format convention \"channels_last\" (channels on axis 3), i.e. expected either 1, 3 or 4 channels on axis 3. However, it was passed an array with shape (28698, 48, 48, 1) (1 channels).\n",
      "  ' (' + str(x.shape[self.channel_axis]) + ' channels).')\n",
      "C:\\Program Files (x86)\\Python35-64bit\\lib\\site-packages\\keras\\preprocessing\\image.py:648: UserWarning: Expected input to be images (as Numpy array) following the data format convention \"channels_last\" (channels on axis 3), i.e. expected either 1, 3 or 4 channels on axis 3. However, it was passed an array with shape (3588, 48, 48, 1) (1 channels).\n",
      "  ' (' + str(x.shape[self.channel_axis]) + ' channels).')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "cropping2d_7 (Cropping2D)    (None, 42, 42, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_20 (Conv2D)           (None, 42, 42, 32)        832       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 21, 21, 32)        0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_7 (ZeroPaddin (None, 23, 23, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_21 (Conv2D)           (None, 20, 20, 32)        16416     \n",
      "_________________________________________________________________\n",
      "average_pooling2d_12 (Averag (None, 10, 10, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_22 (Conv2D)           (None, 10, 10, 64)        51264     \n",
      "_________________________________________________________________\n",
      "average_pooling2d_13 (Averag (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 3072)              4918272   \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 7)                 21511     \n",
      "=================================================================\n",
      "Total params: 5,008,295\n",
      "Trainable params: 5,008,295\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "112/112 [==============================] - 148s - loss: 1.8218 - acc: 0.2498 - val_loss: 1.8062 - val_acc: 0.2489\n",
      "Epoch 2/10\n",
      "112/112 [==============================] - 145s - loss: 1.7938 - acc: 0.2537 - val_loss: 1.7560 - val_acc: 0.2767\n",
      "Epoch 3/10\n",
      "112/112 [==============================] - 145s - loss: 1.7429 - acc: 0.2917 - val_loss: 1.6870 - val_acc: 0.3379\n",
      "Epoch 4/10\n",
      "112/112 [==============================] - 143s - loss: 1.6862 - acc: 0.3343 - val_loss: 1.6405 - val_acc: 0.3574\n",
      "Epoch 5/10\n",
      " 11/112 [=>............................] - ETA: 117s - loss: 1.6812 - acc: 0.3590"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-34-c965e258ab67>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     41\u001b[0m                     \u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m                     \u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtest_datagen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test_onehot\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m                     , validation_steps=n_test//batch_size)\n\u001b[0m",
      "\u001b[1;32mC:\\Program Files (x86)\\Python35-64bit\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     86\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[0;32m     87\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 88\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_support_signature\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minspect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetargspec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Python35-64bit\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_q_size, workers, pickle_safe, initial_epoch)\u001b[0m\n\u001b[0;32m   1122\u001b[0m                                         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1123\u001b[0m                                         \u001b[0mpickle_safe\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpickle_safe\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1124\u001b[1;33m                                         initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   1125\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1126\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Python35-64bit\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     86\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[0;32m     87\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 88\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_support_signature\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minspect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetargspec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Python35-64bit\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_q_size, workers, pickle_safe, initial_epoch)\u001b[0m\n\u001b[0;32m   1900\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[0;32m   1901\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1902\u001b[1;33m                                                class_weight=class_weight)\n\u001b[0m\u001b[0;32m   1903\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1904\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Python35-64bit\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[0;32m   1640\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1641\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1642\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1643\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1644\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Python35-64bit\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2267\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[0;32m   2268\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2269\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2270\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2271\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Python35-64bit\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    776\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 778\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    779\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Python35-64bit\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    980\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m--> 982\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    983\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Python35-64bit\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1030\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m-> 1032\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m   1033\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Python35-64bit\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1037\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1040\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Python35-64bit\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[0;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1021\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m   1022\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1023\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# preprocess\n",
    "rescale_factor = 255\n",
    "train_datagen = ImageDataGenerator(rescale=1./rescale_factor)\n",
    "train_datagen.fit(x_train)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./rescale_factor)\n",
    "test_datagen.fit(x_test)\n",
    "\n",
    "# build model\n",
    "model_name = '1st_place'\n",
    "model = Sequential(name=model_name)\n",
    "model.add(Cropping2D(cropping=3, input_shape=image_shape))\n",
    "model.add(Conv2D(32, (5,5), padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D((2,2), strides=(2,2), padding='valid'))\n",
    "\n",
    "model.add(ZeroPadding2D(padding=1))\n",
    "model.add(Conv2D(32, (4,4), padding='valid', activation='relu'))\n",
    "model.add(AveragePooling2D((2,2), strides=(2,2), padding='valid'))\n",
    "\n",
    "model.add(Conv2D(64, (5,5), padding='same', activation='relu'))\n",
    "model.add(AveragePooling2D((2,2), strides=(2,2), padding='valid'))\n",
    "\n",
    "model.add(Flatten())\n",
    "# TODO is this where dropout should be?\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(3072, activation='relu'))\n",
    "# TODO svm activation function\n",
    "model.add(Dense(n_classes, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# compile model\n",
    "sgd = optimizers.SGD(momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "# fit model\n",
    "epochs = 10\n",
    "batch_size = 256\n",
    "model.fit_generator(train_datagen.flow(x_train, y_train_onehot, batch_size=batch_size)\n",
    "                    , steps_per_epoch=n_train//batch_size\n",
    "                    , epochs=epochs\n",
    "                    , validation_data=test_datagen.flow(x_test, y_test_onehot, batch_size=batch_size)\n",
    "                    , validation_steps=n_test//batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocess: rescale + others + sgd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files (x86)\\Python35-64bit\\lib\\site-packages\\keras\\preprocessing\\image.py:648: UserWarning: Expected input to be images (as Numpy array) following the data format convention \"channels_last\" (channels on axis 3), i.e. expected either 1, 3 or 4 channels on axis 3. However, it was passed an array with shape (28698, 48, 48, 1) (1 channels).\n",
      "  ' (' + str(x.shape[self.channel_axis]) + ' channels).')\n",
      "C:\\Program Files (x86)\\Python35-64bit\\lib\\site-packages\\keras\\preprocessing\\image.py:648: UserWarning: Expected input to be images (as Numpy array) following the data format convention \"channels_last\" (channels on axis 3), i.e. expected either 1, 3 or 4 channels on axis 3. However, it was passed an array with shape (3588, 48, 48, 1) (1 channels).\n",
      "  ' (' + str(x.shape[self.channel_axis]) + ' channels).')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "cropping2d_8 (Cropping2D)    (None, 42, 42, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_23 (Conv2D)           (None, 42, 42, 32)        832       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling (None, 21, 21, 32)        0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_8 (ZeroPaddin (None, 23, 23, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_24 (Conv2D)           (None, 20, 20, 32)        16416     \n",
      "_________________________________________________________________\n",
      "average_pooling2d_14 (Averag (None, 10, 10, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_25 (Conv2D)           (None, 10, 10, 64)        51264     \n",
      "_________________________________________________________________\n",
      "average_pooling2d_15 (Averag (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 3072)              4918272   \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 7)                 21511     \n",
      "=================================================================\n",
      "Total params: 5,008,295\n",
      "Trainable params: 5,008,295\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "112/112 [==============================] - 151s - loss: 1.8243 - acc: 0.2490 - val_loss: 1.8089 - val_acc: 0.2497\n",
      "Epoch 2/10\n",
      "112/112 [==============================] - 145s - loss: 1.8077 - acc: 0.2518 - val_loss: 1.7951 - val_acc: 0.2506\n",
      "Epoch 3/10\n",
      "112/112 [==============================] - 145s - loss: 1.7927 - acc: 0.2529 - val_loss: 1.7864 - val_acc: 0.2500\n",
      "Epoch 4/10\n",
      "112/112 [==============================] - 149s - loss: 1.7851 - acc: 0.2557 - val_loss: 1.7942 - val_acc: 0.2515\n",
      "Epoch 5/10\n",
      "112/112 [==============================] - 143s - loss: 1.7837 - acc: 0.2540 - val_loss: 1.7669 - val_acc: 0.2695\n",
      "Epoch 6/10\n",
      "111/112 [============================>.] - ETA: 1s - loss: 1.7826 - acc: 0.2562"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-35-0f05165639d2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     43\u001b[0m                     \u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m                     \u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtest_datagen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test_onehot\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m                     , validation_steps=n_test//batch_size)\n\u001b[0m",
      "\u001b[1;32mC:\\Program Files (x86)\\Python35-64bit\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     86\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[0;32m     87\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 88\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_support_signature\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minspect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetargspec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Python35-64bit\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_q_size, workers, pickle_safe, initial_epoch)\u001b[0m\n\u001b[0;32m   1122\u001b[0m                                         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1123\u001b[0m                                         \u001b[0mpickle_safe\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpickle_safe\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1124\u001b[1;33m                                         initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   1125\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1126\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Python35-64bit\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     86\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[0;32m     87\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 88\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_support_signature\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minspect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetargspec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Python35-64bit\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_q_size, workers, pickle_safe, initial_epoch)\u001b[0m\n\u001b[0;32m   1922\u001b[0m                                 \u001b[0mmax_q_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_q_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1923\u001b[0m                                 \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1924\u001b[1;33m                                 pickle_safe=pickle_safe)\n\u001b[0m\u001b[0;32m   1925\u001b[0m                         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1926\u001b[0m                             \u001b[1;31m# No need for try/except because\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Python35-64bit\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     86\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[0;32m     87\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 88\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_support_signature\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minspect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetargspec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Python35-64bit\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mevaluate_generator\u001b[1;34m(self, generator, steps, max_q_size, workers, pickle_safe)\u001b[0m\n\u001b[0;32m   2019\u001b[0m                                      \u001b[1;34m'or (x, y). Found: '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2020\u001b[0m                                      str(generator_output))\n\u001b[1;32m-> 2021\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest_on_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2022\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2023\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Python35-64bit\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtest_on_batch\u001b[1;34m(self, x, y, sample_weight)\u001b[0m\n\u001b[0;32m   1682\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1683\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_test_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1684\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1685\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1686\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Python35-64bit\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2267\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[0;32m   2268\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2269\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2270\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2271\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Python35-64bit\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    776\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 778\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    779\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Python35-64bit\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    980\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m--> 982\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    983\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Python35-64bit\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1030\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m-> 1032\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m   1033\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Python35-64bit\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1037\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1040\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Python35-64bit\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[0;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1021\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m   1022\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1023\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# preprocess\n",
    "rescale_factor = 255\n",
    "train_datagen = ImageDataGenerator(rescale=1./rescale_factor\n",
    "                                   , rotation_range=45., width_shift_range=0.2, height_shift_range=0.2\n",
    "                                   , vertical_flip=True)\n",
    "train_datagen.fit(x_train)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./rescale_factor)\n",
    "test_datagen.fit(x_test)\n",
    "\n",
    "# build model\n",
    "model_name = '1st_place'\n",
    "model = Sequential(name=model_name)\n",
    "model.add(Cropping2D(cropping=3, input_shape=image_shape))\n",
    "model.add(Conv2D(32, (5,5), padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D((2,2), strides=(2,2), padding='valid'))\n",
    "\n",
    "model.add(ZeroPadding2D(padding=1))\n",
    "model.add(Conv2D(32, (4,4), padding='valid', activation='relu'))\n",
    "model.add(AveragePooling2D((2,2), strides=(2,2), padding='valid'))\n",
    "\n",
    "model.add(Conv2D(64, (5,5), padding='same', activation='relu'))\n",
    "model.add(AveragePooling2D((2,2), strides=(2,2), padding='valid'))\n",
    "\n",
    "model.add(Flatten())\n",
    "# TODO is this where dropout should be?\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(3072, activation='relu'))\n",
    "# TODO svm activation function\n",
    "model.add(Dense(n_classes, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# compile model\n",
    "sgd = optimizers.SGD(momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "# fit model\n",
    "epochs = 10\n",
    "batch_size = 256\n",
    "model.fit_generator(train_datagen.flow(x_train, y_train_onehot, batch_size=batch_size)\n",
    "                    , steps_per_epoch=n_train//batch_size\n",
    "                    , epochs=epochs\n",
    "                    , validation_data=test_datagen.flow(x_test, y_test_onehot, batch_size=batch_size)\n",
    "                    , validation_steps=n_test//batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocess: all + truncated_normal initialization + sgd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files (x86)\\Python35-64bit\\lib\\site-packages\\keras\\preprocessing\\image.py:648: UserWarning: Expected input to be images (as Numpy array) following the data format convention \"channels_last\" (channels on axis 3), i.e. expected either 1, 3 or 4 channels on axis 3. However, it was passed an array with shape (28698, 48, 48, 1) (1 channels).\n",
      "  ' (' + str(x.shape[self.channel_axis]) + ' channels).')\n",
      "C:\\Program Files (x86)\\Python35-64bit\\lib\\site-packages\\keras\\preprocessing\\image.py:648: UserWarning: Expected input to be images (as Numpy array) following the data format convention \"channels_last\" (channels on axis 3), i.e. expected either 1, 3 or 4 channels on axis 3. However, it was passed an array with shape (3588, 48, 48, 1) (1 channels).\n",
      "  ' (' + str(x.shape[self.channel_axis]) + ' channels).')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "cropping2d_9 (Cropping2D)    (None, 42, 42, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_26 (Conv2D)           (None, 42, 42, 32)        832       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling (None, 21, 21, 32)        0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_9 (ZeroPaddin (None, 23, 23, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_27 (Conv2D)           (None, 20, 20, 32)        16416     \n",
      "_________________________________________________________________\n",
      "average_pooling2d_16 (Averag (None, 10, 10, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_28 (Conv2D)           (None, 10, 10, 64)        51264     \n",
      "_________________________________________________________________\n",
      "average_pooling2d_17 (Averag (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 3072)              4918272   \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 7)                 21511     \n",
      "=================================================================\n",
      "Total params: 5,008,295\n",
      "Trainable params: 5,008,295\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "112/112 [==============================] - 149s - loss: 1.8203 - acc: 0.2485 - val_loss: 1.8110 - val_acc: 0.2497\n",
      "Epoch 2/10\n",
      "112/112 [==============================] - 146s - loss: 1.8143 - acc: 0.2496 - val_loss: 1.8056 - val_acc: 0.2521\n",
      "Epoch 3/10\n",
      "112/112 [==============================] - 143s - loss: 1.8128 - acc: 0.2510 - val_loss: 1.8159 - val_acc: 0.2509\n",
      "Epoch 4/10\n",
      " 83/112 [=====================>........] - ETA: 36s - loss: 1.8151 - acc: 0.2467"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-36-47929c71fe67>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     43\u001b[0m                     \u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m                     \u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtest_datagen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test_onehot\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m                     , validation_steps=n_test//batch_size)\n\u001b[0m",
      "\u001b[1;32mC:\\Program Files (x86)\\Python35-64bit\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     86\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[0;32m     87\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 88\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_support_signature\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minspect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetargspec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Python35-64bit\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_q_size, workers, pickle_safe, initial_epoch)\u001b[0m\n\u001b[0;32m   1122\u001b[0m                                         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1123\u001b[0m                                         \u001b[0mpickle_safe\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpickle_safe\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1124\u001b[1;33m                                         initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   1125\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1126\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Python35-64bit\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     86\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[0;32m     87\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 88\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_support_signature\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minspect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetargspec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Python35-64bit\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_q_size, workers, pickle_safe, initial_epoch)\u001b[0m\n\u001b[0;32m   1900\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[0;32m   1901\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1902\u001b[1;33m                                                class_weight=class_weight)\n\u001b[0m\u001b[0;32m   1903\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1904\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Python35-64bit\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[0;32m   1640\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1641\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1642\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1643\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1644\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Python35-64bit\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2267\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[0;32m   2268\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2269\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2270\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2271\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Python35-64bit\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    776\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 778\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    779\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Python35-64bit\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    980\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m--> 982\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    983\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Python35-64bit\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1030\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m-> 1032\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m   1033\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Python35-64bit\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1037\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1040\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Python35-64bit\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[0;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1021\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m   1022\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1023\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# preprocess\n",
    "rescale_factor = 255\n",
    "train_datagen = ImageDataGenerator(featurewise_center=True, featurewise_std_normalization=True\n",
    "                                   , rotation_range=45., width_shift_range=0.2, height_shift_range=0.2\n",
    "                                   , vertical_flip=True, rescale=1./rescale_factor)\n",
    "train_datagen.fit(x_train)\n",
    "\n",
    "test_datagen = ImageDataGenerator(featurewise_center=True, featurewise_std_normalization=True, rescale=1./rescale_factor)\n",
    "test_datagen.fit(x_test)\n",
    "\n",
    "# build model\n",
    "model_name = '1st_place'\n",
    "model = Sequential(name=model_name)\n",
    "model.add(Cropping2D(cropping=3, input_shape=image_shape))\n",
    "model.add(Conv2D(32, (5,5), padding='same', activation='relu', kernel_initializer='TruncatedNormal'))\n",
    "model.add(MaxPooling2D((2,2), strides=(2,2), padding='valid'))\n",
    "\n",
    "model.add(ZeroPadding2D(padding=1))\n",
    "model.add(Conv2D(32, (4,4), padding='valid', activation='relu', kernel_initializer='TruncatedNormal'))\n",
    "model.add(AveragePooling2D((2,2), strides=(2,2), padding='valid'))\n",
    "\n",
    "model.add(Conv2D(64, (5,5), padding='same', activation='relu', kernel_initializer='TruncatedNormal'))\n",
    "model.add(AveragePooling2D((2,2), strides=(2,2), padding='valid'))\n",
    "\n",
    "model.add(Flatten())\n",
    "# TODO is this where dropout should be?\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(3072, activation='relu', kernel_initializer='TruncatedNormal'))\n",
    "# TODO svm activation function\n",
    "model.add(Dense(n_classes, activation='softmax', kernel_initializer='TruncatedNormal'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# compile model\n",
    "sgd = optimizers.SGD(momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "# fit model\n",
    "epochs = 10\n",
    "batch_size = 256\n",
    "model.fit_generator(train_datagen.flow(x_train, y_train_onehot, batch_size=batch_size)\n",
    "                    , steps_per_epoch=n_train//batch_size\n",
    "                    , epochs=epochs\n",
    "                    , validation_data=test_datagen.flow(x_test, y_test_onehot, batch_size=batch_size)\n",
    "                    , validation_steps=n_test//batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocess: samplewise, horizontal_flip + truncated_normal initialization + sgd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files (x86)\\Python35-64bit\\lib\\site-packages\\keras\\preprocessing\\image.py:648: UserWarning: Expected input to be images (as Numpy array) following the data format convention \"channels_last\" (channels on axis 3), i.e. expected either 1, 3 or 4 channels on axis 3. However, it was passed an array with shape (28698, 48, 48, 1) (1 channels).\n",
      "  ' (' + str(x.shape[self.channel_axis]) + ' channels).')\n",
      "C:\\Program Files (x86)\\Python35-64bit\\lib\\site-packages\\keras\\preprocessing\\image.py:648: UserWarning: Expected input to be images (as Numpy array) following the data format convention \"channels_last\" (channels on axis 3), i.e. expected either 1, 3 or 4 channels on axis 3. However, it was passed an array with shape (3588, 48, 48, 1) (1 channels).\n",
      "  ' (' + str(x.shape[self.channel_axis]) + ' channels).')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "cropping2d_10 (Cropping2D)   (None, 42, 42, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_29 (Conv2D)           (None, 42, 42, 32)        832       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_12 (MaxPooling (None, 21, 21, 32)        0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_10 (ZeroPaddi (None, 23, 23, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_30 (Conv2D)           (None, 20, 20, 32)        16416     \n",
      "_________________________________________________________________\n",
      "average_pooling2d_18 (Averag (None, 10, 10, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_31 (Conv2D)           (None, 10, 10, 64)        51264     \n",
      "_________________________________________________________________\n",
      "average_pooling2d_19 (Averag (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 3072)              4918272   \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 7)                 21511     \n",
      "=================================================================\n",
      "Total params: 5,008,295\n",
      "Trainable params: 5,008,295\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "112/112 [==============================] - 146s - loss: 1.8220 - acc: 0.2473 - val_loss: 1.8116 - val_acc: 0.2497\n",
      "Epoch 2/30\n",
      "112/112 [==============================] - 142s - loss: 1.8136 - acc: 0.2513 - val_loss: 1.8118 - val_acc: 0.2452\n",
      "Epoch 3/30\n",
      "112/112 [==============================] - 143s - loss: 1.8123 - acc: 0.2528 - val_loss: 1.8039 - val_acc: 0.2578\n",
      "Epoch 4/30\n",
      "112/112 [==============================] - 143s - loss: 1.8119 - acc: 0.2498 - val_loss: 1.8062 - val_acc: 0.2497\n",
      "Epoch 5/30\n",
      "112/112 [==============================] - 142s - loss: 1.8117 - acc: 0.2532 - val_loss: 1.8104 - val_acc: 0.2467\n",
      "Epoch 6/30\n",
      "112/112 [==============================] - 144s - loss: 1.8112 - acc: 0.2507 - val_loss: 1.8118 - val_acc: 0.2497\n",
      "Epoch 7/30\n",
      "112/112 [==============================] - 144s - loss: 1.8098 - acc: 0.2517 - val_loss: 1.8145 - val_acc: 0.2581\n",
      "Epoch 8/30\n",
      "112/112 [==============================] - 145s - loss: 1.8137 - acc: 0.2496 - val_loss: 1.8142 - val_acc: 0.2458\n",
      "Epoch 9/30\n",
      "112/112 [==============================] - 144s - loss: 1.8086 - acc: 0.2536 - val_loss: 1.8117 - val_acc: 0.2491\n",
      "Epoch 10/30\n",
      "112/112 [==============================] - 145s - loss: 1.8113 - acc: 0.2507 - val_loss: 1.8060 - val_acc: 0.2536\n",
      "Epoch 11/30\n",
      "112/112 [==============================] - 145s - loss: 1.8119 - acc: 0.2495 - val_loss: 1.8112 - val_acc: 0.2494\n",
      "Epoch 12/30\n",
      "112/112 [==============================] - 144s - loss: 1.8118 - acc: 0.2517 - val_loss: 1.8073 - val_acc: 0.2485\n",
      "Epoch 13/30\n",
      "112/112 [==============================] - 143s - loss: 1.8091 - acc: 0.2523 - val_loss: 1.8102 - val_acc: 0.2473\n",
      "Epoch 14/30\n",
      "112/112 [==============================] - 141s - loss: 1.8137 - acc: 0.2505 - val_loss: 1.8134 - val_acc: 0.2416\n",
      "Epoch 15/30\n",
      "112/112 [==============================] - 141s - loss: 1.8124 - acc: 0.2502 - val_loss: 1.8045 - val_acc: 0.2587\n",
      "Epoch 16/30\n",
      "112/112 [==============================] - 141s - loss: 1.8086 - acc: 0.2518 - val_loss: 1.8123 - val_acc: 0.2494\n",
      "Epoch 17/30\n",
      "112/112 [==============================] - 139s - loss: 1.8119 - acc: 0.2498 - val_loss: 1.8050 - val_acc: 0.2479\n",
      "Epoch 18/30\n",
      "112/112 [==============================] - 142s - loss: 1.8113 - acc: 0.2512 - val_loss: 1.8181 - val_acc: 0.2536\n",
      "Epoch 19/30\n",
      "112/112 [==============================] - 139s - loss: 1.8123 - acc: 0.2483 - val_loss: 1.8091 - val_acc: 0.2470\n",
      "Epoch 20/30\n",
      "112/112 [==============================] - 139s - loss: 1.8082 - acc: 0.2529 - val_loss: 1.8084 - val_acc: 0.2491\n",
      "Epoch 21/30\n",
      " 44/112 [==========>...................] - ETA: 85s - loss: 1.8083 - acc: 0.2551"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-41-828e978cb93e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     44\u001b[0m                     \u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m                     \u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtest_datagen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test_onehot\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m                     , validation_steps=n_test//batch_size)\n\u001b[0m",
      "\u001b[1;32mC:\\Program Files (x86)\\Python35-64bit\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     86\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[0;32m     87\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 88\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_support_signature\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minspect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetargspec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Python35-64bit\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_q_size, workers, pickle_safe, initial_epoch)\u001b[0m\n\u001b[0;32m   1122\u001b[0m                                         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1123\u001b[0m                                         \u001b[0mpickle_safe\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpickle_safe\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1124\u001b[1;33m                                         initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   1125\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1126\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Python35-64bit\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     86\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[0;32m     87\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 88\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_support_signature\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minspect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetargspec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Python35-64bit\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_q_size, workers, pickle_safe, initial_epoch)\u001b[0m\n\u001b[0;32m   1900\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[0;32m   1901\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1902\u001b[1;33m                                                class_weight=class_weight)\n\u001b[0m\u001b[0;32m   1903\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1904\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Python35-64bit\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[0;32m   1640\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1641\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1642\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1643\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1644\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Python35-64bit\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2267\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[0;32m   2268\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2269\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2270\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2271\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Python35-64bit\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    776\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 778\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    779\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Python35-64bit\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    980\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m--> 982\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    983\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Python35-64bit\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1030\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m-> 1032\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m   1033\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Python35-64bit\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1037\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1040\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Python35-64bit\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[0;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1021\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m   1022\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1023\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# preprocess\n",
    "rescale_factor = 255\n",
    "train_datagen = ImageDataGenerator(featurewise_center=True, featurewise_std_normalization=True\n",
    "                                   , rotation_range=45., width_shift_range=0.2, height_shift_range=0.2\n",
    "                                   , horizontal_flip=True, rescale=1./rescale_factor, samplewise_center=True)\n",
    "train_datagen.fit(x_train)\n",
    "\n",
    "test_datagen = ImageDataGenerator(featurewise_center=True, featurewise_std_normalization=True\n",
    "                                  , rescale=1./rescale_factor, samplewise_center=True)\n",
    "test_datagen.fit(x_test)\n",
    "\n",
    "# build model\n",
    "model_name = '1st_place'\n",
    "model = Sequential(name=model_name)\n",
    "model.add(Cropping2D(cropping=3, input_shape=image_shape))\n",
    "model.add(Conv2D(32, (5,5), padding='same', activation='relu', kernel_initializer='TruncatedNormal'))\n",
    "model.add(MaxPooling2D((2,2), strides=(2,2), padding='valid'))\n",
    "\n",
    "model.add(ZeroPadding2D(padding=1))\n",
    "model.add(Conv2D(32, (4,4), padding='valid', activation='relu', kernel_initializer='TruncatedNormal'))\n",
    "model.add(AveragePooling2D((2,2), strides=(2,2), padding='valid'))\n",
    "\n",
    "model.add(Conv2D(64, (5,5), padding='same', activation='relu', kernel_initializer='TruncatedNormal'))\n",
    "model.add(AveragePooling2D((2,2), strides=(2,2), padding='valid'))\n",
    "\n",
    "model.add(Flatten())\n",
    "# TODO is this where dropout should be?\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(3072, activation='relu', kernel_initializer='TruncatedNormal'))\n",
    "# TODO svm activation function\n",
    "model.add(Dense(n_classes, activation='softmax', kernel_initializer='TruncatedNormal'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# compile model\n",
    "sgd = optimizers.SGD(momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "# fit model\n",
    "epochs = 30\n",
    "batch_size = 256\n",
    "model.fit_generator(train_datagen.flow(x_train, y_train_onehot, batch_size=batch_size)\n",
    "                    , steps_per_epoch=n_train//batch_size\n",
    "                    , epochs=epochs\n",
    "                    , validation_data=test_datagen.flow(x_test, y_test_onehot, batch_size=batch_size)\n",
    "                    , validation_steps=n_test//batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocess: samplewise, horizontal_flip + truncated_normal initialization + adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "112/112 [==============================] - 148s - loss: 1.8139 - acc: 0.2514 - val_loss: 1.8138 - val_acc: 0.2494\n",
      "Epoch 2/30\n",
      "112/112 [==============================] - 147s - loss: 1.8110 - acc: 0.2509 - val_loss: 1.8121 - val_acc: 0.2578\n",
      "Epoch 3/30\n",
      "112/112 [==============================] - 145s - loss: 1.8107 - acc: 0.2529 - val_loss: 1.8113 - val_acc: 0.2464\n",
      "Epoch 4/30\n",
      "112/112 [==============================] - 147s - loss: 1.8121 - acc: 0.2501 - val_loss: 1.8139 - val_acc: 0.2434\n",
      "Epoch 5/30\n",
      "112/112 [==============================] - 146s - loss: 1.8107 - acc: 0.2504 - val_loss: 1.8072 - val_acc: 0.2560\n",
      "Epoch 6/30\n",
      "112/112 [==============================] - 147s - loss: 1.8099 - acc: 0.2523 - val_loss: 1.8105 - val_acc: 0.2497\n",
      "Epoch 7/30\n",
      "112/112 [==============================] - 146s - loss: 1.8113 - acc: 0.2501 - val_loss: 1.8121 - val_acc: 0.2494\n",
      "Epoch 8/30\n",
      "105/112 [===========================>..] - ETA: 8s - loss: 1.8100 - acc: 0.2516 "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-42-1ce96add8ba3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m                     \u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m                     \u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtest_datagen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test_onehot\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m                     , validation_steps=n_test//batch_size)\n\u001b[0m",
      "\u001b[1;32mC:\\Program Files (x86)\\Python35-64bit\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     86\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[0;32m     87\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 88\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_support_signature\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minspect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetargspec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Python35-64bit\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_q_size, workers, pickle_safe, initial_epoch)\u001b[0m\n\u001b[0;32m   1122\u001b[0m                                         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1123\u001b[0m                                         \u001b[0mpickle_safe\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpickle_safe\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1124\u001b[1;33m                                         initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   1125\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1126\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Python35-64bit\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     86\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[0;32m     87\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 88\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_support_signature\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minspect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetargspec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Python35-64bit\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_q_size, workers, pickle_safe, initial_epoch)\u001b[0m\n\u001b[0;32m   1900\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[0;32m   1901\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1902\u001b[1;33m                                                class_weight=class_weight)\n\u001b[0m\u001b[0;32m   1903\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1904\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Python35-64bit\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[0;32m   1640\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1641\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1642\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1643\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1644\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Python35-64bit\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2267\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[0;32m   2268\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2269\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2270\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2271\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Python35-64bit\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    776\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 778\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    779\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Python35-64bit\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    980\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m--> 982\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    983\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Python35-64bit\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1030\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m-> 1032\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m   1033\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Python35-64bit\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1037\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1040\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Python35-64bit\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[0;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1021\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m   1022\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1023\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# compile model\n",
    "adam = optimizers.Adam()\n",
    "model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "\n",
    "# fit model\n",
    "epochs = 30\n",
    "batch_size = 256\n",
    "model.fit_generator(train_datagen.flow(x_train, y_train_onehot, batch_size=batch_size)\n",
    "                    , steps_per_epoch=n_train//batch_size\n",
    "                    , epochs=epochs\n",
    "                    , validation_data=test_datagen.flow(x_test, y_test_onehot, batch_size=batch_size)\n",
    "                    , validation_steps=n_test//batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocess: samplewise, horizontal_flip + truncated_normal initialization + sgd_decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files (x86)\\Python35-64bit\\lib\\site-packages\\keras\\preprocessing\\image.py:648: UserWarning: Expected input to be images (as Numpy array) following the data format convention \"channels_last\" (channels on axis 3), i.e. expected either 1, 3 or 4 channels on axis 3. However, it was passed an array with shape (28698, 48, 48, 1) (1 channels).\n",
      "  ' (' + str(x.shape[self.channel_axis]) + ' channels).')\n",
      "C:\\Program Files (x86)\\Python35-64bit\\lib\\site-packages\\keras\\preprocessing\\image.py:648: UserWarning: Expected input to be images (as Numpy array) following the data format convention \"channels_last\" (channels on axis 3), i.e. expected either 1, 3 or 4 channels on axis 3. However, it was passed an array with shape (3588, 48, 48, 1) (1 channels).\n",
      "  ' (' + str(x.shape[self.channel_axis]) + ' channels).')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "cropping2d_12 (Cropping2D)   (None, 42, 42, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_35 (Conv2D)           (None, 42, 42, 32)        832       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_14 (MaxPooling (None, 21, 21, 32)        0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_12 (ZeroPaddi (None, 23, 23, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_36 (Conv2D)           (None, 20, 20, 32)        16416     \n",
      "_________________________________________________________________\n",
      "average_pooling2d_22 (Averag (None, 10, 10, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_37 (Conv2D)           (None, 10, 10, 64)        51264     \n",
      "_________________________________________________________________\n",
      "average_pooling2d_23 (Averag (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_13 (Flatten)         (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 3072)              4918272   \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 7)                 21511     \n",
      "=================================================================\n",
      "Total params: 5,008,295\n",
      "Trainable params: 5,008,295\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "epoch set: 0 lr value: 0.01\n",
      "Epoch 1/5\n",
      "112/112 [==============================] - 147s - loss: 1.8198 - acc: 0.2493 - val_loss: 1.8139 - val_acc: 0.2494\n",
      "Epoch 2/5\n",
      "112/112 [==============================] - 147s - loss: 1.8129 - acc: 0.2520 - val_loss: 1.8031 - val_acc: 0.2632\n",
      "Epoch 3/5\n",
      "112/112 [==============================] - 144s - loss: 1.8114 - acc: 0.2518 - val_loss: 1.8158 - val_acc: 0.2470\n",
      "Epoch 4/5\n",
      "112/112 [==============================] - 145s - loss: 1.8130 - acc: 0.2516 - val_loss: 1.8107 - val_acc: 0.2503\n",
      "Epoch 5/5\n",
      "112/112 [==============================] - 146s - loss: 1.8120 - acc: 0.2514 - val_loss: 1.8068 - val_acc: 0.2518\n",
      "epoch set: 1 lr value: 0.005\n",
      "Epoch 1/5\n",
      "112/112 [==============================] - 147s - loss: 1.8109 - acc: 0.2512 - val_loss: 1.8120 - val_acc: 0.2492\n",
      "Epoch 2/5\n",
      "112/112 [==============================] - 143s - loss: 1.8097 - acc: 0.2532 - val_loss: 1.8037 - val_acc: 0.2620\n",
      "Epoch 3/5\n",
      "112/112 [==============================] - 145s - loss: 1.8109 - acc: 0.2502 - val_loss: 1.8096 - val_acc: 0.2506\n",
      "Epoch 4/5\n",
      "112/112 [==============================] - 144s - loss: 1.8107 - acc: 0.2508 - val_loss: 1.8117 - val_acc: 0.2524\n",
      "Epoch 5/5\n",
      "112/112 [==============================] - 145s - loss: 1.8100 - acc: 0.2522 - val_loss: 1.8084 - val_acc: 0.2572\n",
      "epoch set: 2 lr value: 0.0025\n",
      "Epoch 1/5\n",
      "112/112 [==============================] - 147s - loss: 1.8102 - acc: 0.2513 - val_loss: 1.8115 - val_acc: 0.2492\n",
      "Epoch 2/5\n",
      "112/112 [==============================] - 148s - loss: 1.8121 - acc: 0.2520 - val_loss: 1.8032 - val_acc: 0.2587\n",
      "Epoch 3/5\n",
      "112/112 [==============================] - 144s - loss: 1.8104 - acc: 0.2508 - val_loss: 1.8163 - val_acc: 0.2467\n",
      "Epoch 4/5\n",
      "112/112 [==============================] - 145s - loss: 1.8104 - acc: 0.2515 - val_loss: 1.8131 - val_acc: 0.2485\n",
      "Epoch 5/5\n",
      "112/112 [==============================] - 144s - loss: 1.8109 - acc: 0.2508 - val_loss: 1.8135 - val_acc: 0.2482\n",
      "epoch set: 3 lr value: 0.00125\n",
      "Epoch 1/5\n",
      "112/112 [==============================] - 147s - loss: 1.8103 - acc: 0.2514 - val_loss: 1.8111 - val_acc: 0.2497\n",
      "Epoch 2/5\n",
      "112/112 [==============================] - 145s - loss: 1.8099 - acc: 0.2515 - val_loss: 1.8096 - val_acc: 0.2485\n",
      "Epoch 3/5\n",
      "112/112 [==============================] - 141s - loss: 1.8103 - acc: 0.2508 - val_loss: 1.8071 - val_acc: 0.2470\n",
      "Epoch 4/5\n",
      "112/112 [==============================] - 140s - loss: 1.8126 - acc: 0.2496 - val_loss: 1.8151 - val_acc: 0.2536\n",
      "Epoch 5/5\n",
      "112/112 [==============================] - 140s - loss: 1.8099 - acc: 0.2508 - val_loss: 1.8198 - val_acc: 0.2419\n",
      "epoch set: 4 lr value: 0.000625\n",
      "Epoch 1/5\n",
      "112/112 [==============================] - 142s - loss: 1.8102 - acc: 0.2513 - val_loss: 1.8113 - val_acc: 0.2497\n",
      "Epoch 2/5\n",
      "112/112 [==============================] - 140s - loss: 1.8100 - acc: 0.2515 - val_loss: 1.8031 - val_acc: 0.2524\n",
      "Epoch 3/5\n",
      "112/112 [==============================] - 140s - loss: 1.8092 - acc: 0.2530 - val_loss: 1.8074 - val_acc: 0.2557\n",
      "Epoch 4/5\n",
      "112/112 [==============================] - 140s - loss: 1.8089 - acc: 0.2521 - val_loss: 1.8039 - val_acc: 0.2512\n",
      "Epoch 5/5\n",
      "112/112 [==============================] - 139s - loss: 1.8101 - acc: 0.2506 - val_loss: 1.8150 - val_acc: 0.2506\n",
      "epoch set: 5 lr value: 0.0003125\n",
      "Epoch 1/5\n",
      "112/112 [==============================] - 140s - loss: 1.8100 - acc: 0.2513 - val_loss: 1.8113 - val_acc: 0.2497\n",
      "Epoch 2/5\n",
      "112/112 [==============================] - 138s - loss: 1.8096 - acc: 0.2523 - val_loss: 1.8205 - val_acc: 0.2476\n",
      "Epoch 3/5\n",
      "112/112 [==============================] - 137s - loss: 1.8099 - acc: 0.2525 - val_loss: 1.8121 - val_acc: 0.2419\n",
      "Epoch 4/5\n",
      "112/112 [==============================] - 138s - loss: 1.8093 - acc: 0.2510 - val_loss: 1.8128 - val_acc: 0.2464\n",
      "Epoch 5/5\n",
      "112/112 [==============================] - 138s - loss: 1.8100 - acc: 0.2516 - val_loss: 1.8103 - val_acc: 0.2551\n",
      "epoch set: 6 lr value: 0.00015625\n",
      "Epoch 1/5\n",
      "112/112 [==============================] - 139s - loss: 1.8101 - acc: 0.2514 - val_loss: 1.8113 - val_acc: 0.2494\n",
      "Epoch 2/5\n",
      "112/112 [==============================] - 137s - loss: 1.8095 - acc: 0.2514 - val_loss: 1.8142 - val_acc: 0.2479\n",
      "Epoch 3/5\n",
      "112/112 [==============================] - 137s - loss: 1.8109 - acc: 0.2505 - val_loss: 1.8092 - val_acc: 0.2572\n",
      "Epoch 4/5\n",
      "112/112 [==============================] - 137s - loss: 1.8080 - acc: 0.2539 - val_loss: 1.8076 - val_acc: 0.2557\n",
      "Epoch 5/5\n",
      "112/112 [==============================] - 138s - loss: 1.8101 - acc: 0.2506 - val_loss: 1.8010 - val_acc: 0.2587\n",
      "epoch set: 7 lr value: 7.8125e-05\n",
      "Epoch 1/5\n",
      "112/112 [==============================] - 138s - loss: 1.8100 - acc: 0.2514 - val_loss: 1.8113 - val_acc: 0.2494\n",
      "Epoch 2/5\n",
      "112/112 [==============================] - 136s - loss: 1.8106 - acc: 0.2507 - val_loss: 1.8090 - val_acc: 0.2587\n",
      "Epoch 3/5\n",
      "112/112 [==============================] - 139s - loss: 1.8088 - acc: 0.2531 - val_loss: 1.8089 - val_acc: 0.2494\n",
      "Epoch 4/5\n",
      "112/112 [==============================] - 141s - loss: 1.8102 - acc: 0.2519 - val_loss: 1.8119 - val_acc: 0.2548\n",
      "Epoch 5/5\n",
      "112/112 [==============================] - 137s - loss: 1.8092 - acc: 0.2525 - val_loss: 1.8046 - val_acc: 0.2617\n",
      "epoch set: 8 lr value: 3.90625e-05\n",
      "Epoch 1/5\n",
      "112/112 [==============================] - 139s - loss: 1.8098 - acc: 0.2514 - val_loss: 1.8111 - val_acc: 0.2497\n",
      "Epoch 2/5\n",
      "112/112 [==============================] - 138s - loss: 1.8103 - acc: 0.2507 - val_loss: 1.8183 - val_acc: 0.2452\n",
      "Epoch 3/5\n",
      "112/112 [==============================] - 138s - loss: 1.8090 - acc: 0.2527 - val_loss: 1.8156 - val_acc: 0.2464\n",
      "Epoch 4/5\n",
      "112/112 [==============================] - 139s - loss: 1.8096 - acc: 0.2521 - val_loss: 1.8139 - val_acc: 0.2521\n",
      "Epoch 5/5\n",
      "112/112 [==============================] - 137s - loss: 1.8098 - acc: 0.2516 - val_loss: 1.8150 - val_acc: 0.2488\n",
      "epoch set: 9 lr value: 1.95312e-05\n",
      "Epoch 1/5\n",
      "112/112 [==============================] - 139s - loss: 1.8100 - acc: 0.2513 - val_loss: 1.8111 - val_acc: 0.2497\n",
      "Epoch 2/5\n",
      "112/112 [==============================] - 137s - loss: 1.8094 - acc: 0.2522 - val_loss: 1.8072 - val_acc: 0.2587\n",
      "Epoch 3/5\n",
      "112/112 [==============================] - 138s - loss: 1.8095 - acc: 0.2510 - val_loss: 1.8121 - val_acc: 0.2506\n",
      "Epoch 4/5\n",
      "112/112 [==============================] - 138s - loss: 1.8096 - acc: 0.2514 - val_loss: 1.8060 - val_acc: 0.2458\n",
      "Epoch 5/5\n",
      "112/112 [==============================] - 138s - loss: 1.8096 - acc: 0.2526 - val_loss: 1.8102 - val_acc: 0.2482\n"
     ]
    }
   ],
   "source": [
    "# preprocess\n",
    "rescale_factor = 255\n",
    "train_datagen = ImageDataGenerator(featurewise_center=True, featurewise_std_normalization=True\n",
    "                                   , rotation_range=45., width_shift_range=0.2, height_shift_range=0.2\n",
    "                                   , horizontal_flip=True, rescale=1./rescale_factor, samplewise_center=True)\n",
    "train_datagen.fit(x_train)\n",
    "\n",
    "test_datagen = ImageDataGenerator(featurewise_center=True, featurewise_std_normalization=True\n",
    "                                  , rescale=1./rescale_factor, samplewise_center=True)\n",
    "test_datagen.fit(x_test)\n",
    "\n",
    "# build model\n",
    "model_name = '1st_place'\n",
    "model = Sequential(name=model_name)\n",
    "model.add(Cropping2D(cropping=3, input_shape=image_shape))\n",
    "model.add(Conv2D(32, (5,5), padding='same', activation='relu', kernel_initializer='TruncatedNormal'))\n",
    "model.add(MaxPooling2D((2,2), strides=(2,2), padding='valid'))\n",
    "\n",
    "model.add(ZeroPadding2D(padding=1))\n",
    "model.add(Conv2D(32, (4,4), padding='valid', activation='relu', kernel_initializer='TruncatedNormal'))\n",
    "model.add(AveragePooling2D((2,2), strides=(2,2), padding='valid'))\n",
    "\n",
    "model.add(Conv2D(64, (5,5), padding='same', activation='relu', kernel_initializer='TruncatedNormal'))\n",
    "model.add(AveragePooling2D((2,2), strides=(2,2), padding='valid'))\n",
    "\n",
    "model.add(Flatten())\n",
    "# TODO is this where dropout should be?\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(3072, activation='relu', kernel_initializer='TruncatedNormal'))\n",
    "# TODO svm activation function\n",
    "model.add(Dense(n_classes, activation='softmax', kernel_initializer='TruncatedNormal'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# compile model\n",
    "sgd = optimizers.SGD(momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "# fit model\n",
    "epoch_sets = 10\n",
    "epochs = 5\n",
    "batch_size = 256\n",
    "for es in range(epoch_sets):\n",
    "    # get current lr\n",
    "    lr = k.get_value(sgd.lr)\n",
    "    print('epoch set:', es, 'lr value:', lr)\n",
    "    \n",
    "    # fit model\n",
    "    model.fit_generator(train_datagen.flow(x_train, y_train_onehot, batch_size=batch_size)\n",
    "                        , steps_per_epoch=n_train//batch_size\n",
    "                        , epochs=epochs\n",
    "                        , validation_data=test_datagen.flow(x_test, y_test_onehot, batch_size=batch_size)\n",
    "                        , validation_steps=n_test//batch_size)\n",
    "    # decay lr by half\n",
    "    lr = lr / 2\n",
    "    k.set_value(sgd.lr, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save and evaluate model\n",
    "optimizer_name = 'sgd_decay'\n",
    "model.save('./Model/' + model_name + '_' + optimizer_name +'.h5')\n",
    "model.save_weights('./Model/' + model_name + '_' + optimizer_name + '_weights.h5')\n",
    "# print(model.evaluate_generator(test_datagen.flow(x_test, y_test_onehot, batch_size=batch_size), steps=batch_size))\n",
    "\n",
    "# test model\n",
    "# test_datagen.fit(x_private)\n",
    "# print(model.evaluate_generator(test_datagen.flow(x_private, y_private_onehot, batch_size=batch_size), steps=batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch set: 0 lr value: 9.76562e-06\n",
      "Epoch 1/5\n",
      "112/112 [==============================] - 147s - loss: 1.8098 - acc: 0.2513 - val_loss: 1.8114 - val_acc: 0.2492\n",
      "Epoch 2/5\n",
      "112/112 [==============================] - 144s - loss: 1.8099 - acc: 0.2516 - val_loss: 1.8097 - val_acc: 0.2557\n",
      "Epoch 3/5\n",
      "112/112 [==============================] - 149s - loss: 1.8093 - acc: 0.2518 - val_loss: 1.8085 - val_acc: 0.2491\n",
      "Epoch 4/5\n",
      "112/112 [==============================] - 143s - loss: 1.8099 - acc: 0.2511 - val_loss: 1.8163 - val_acc: 0.2455\n",
      "Epoch 5/5\n",
      "112/112 [==============================] - 143s - loss: 1.8108 - acc: 0.2513 - val_loss: 1.8143 - val_acc: 0.2449\n",
      "epoch set: 1 lr value: 4.88281e-06\n",
      "Epoch 1/5\n",
      "112/112 [==============================] - 150s - loss: 1.8099 - acc: 0.2514 - val_loss: 1.8112 - val_acc: 0.2497\n",
      "Epoch 2/5\n",
      "112/112 [==============================] - 144s - loss: 1.8098 - acc: 0.2510 - val_loss: 1.8166 - val_acc: 0.2503\n",
      "Epoch 3/5\n",
      "112/112 [==============================] - 141s - loss: 1.8091 - acc: 0.2518 - val_loss: 1.8147 - val_acc: 0.2446\n",
      "Epoch 4/5\n",
      "112/112 [==============================] - 141s - loss: 1.8092 - acc: 0.2523 - val_loss: 1.8125 - val_acc: 0.2512\n",
      "Epoch 5/5\n",
      "112/112 [==============================] - 142s - loss: 1.8105 - acc: 0.2508 - val_loss: 1.8054 - val_acc: 0.2521\n",
      "epoch set: 2 lr value: 2.44141e-06\n",
      "Epoch 1/5\n",
      "112/112 [==============================] - 146s - loss: 1.8098 - acc: 0.2514 - val_loss: 1.8116 - val_acc: 0.2492\n",
      "Epoch 2/5\n",
      "112/112 [==============================] - 142s - loss: 1.8105 - acc: 0.2513 - val_loss: 1.8058 - val_acc: 0.2524\n",
      "Epoch 3/5\n",
      "112/112 [==============================] - 142s - loss: 1.8102 - acc: 0.2523 - val_loss: 1.8048 - val_acc: 0.2548\n",
      "Epoch 4/5\n",
      "112/112 [==============================] - 141s - loss: 1.8101 - acc: 0.2513 - val_loss: 1.8126 - val_acc: 0.2464\n",
      "Epoch 5/5\n",
      "112/112 [==============================] - 142s - loss: 1.8107 - acc: 0.2500 - val_loss: 1.8083 - val_acc: 0.2551\n",
      "epoch set: 3 lr value: 1.2207e-06\n",
      "Epoch 1/5\n",
      "112/112 [==============================] - 145s - loss: 1.8100 - acc: 0.2513 - val_loss: 1.8112 - val_acc: 0.2497\n",
      "Epoch 2/5\n",
      "112/112 [==============================] - 142s - loss: 1.8091 - acc: 0.2521 - val_loss: 1.8037 - val_acc: 0.2506\n",
      "Epoch 3/5\n",
      "112/112 [==============================] - 141s - loss: 1.8095 - acc: 0.2509 - val_loss: 1.8092 - val_acc: 0.2491\n",
      "Epoch 4/5\n",
      "112/112 [==============================] - 142s - loss: 1.8115 - acc: 0.2513 - val_loss: 1.8084 - val_acc: 0.2497\n",
      "Epoch 5/5\n",
      "112/112 [==============================] - 141s - loss: 1.8087 - acc: 0.2527 - val_loss: 1.8109 - val_acc: 0.2482\n",
      "epoch set: 4 lr value: 6.10352e-07\n",
      "Epoch 1/5\n",
      "112/112 [==============================] - 147s - loss: 1.8098 - acc: 0.2513 - val_loss: 1.8108 - val_acc: 0.2492\n",
      "Epoch 2/5\n",
      "112/112 [==============================] - 145s - loss: 1.8102 - acc: 0.2515 - val_loss: 1.8041 - val_acc: 0.2578\n",
      "Epoch 3/5\n",
      "112/112 [==============================] - 146s - loss: 1.8099 - acc: 0.2515 - val_loss: 1.8083 - val_acc: 0.2560\n",
      "Epoch 4/5\n",
      "112/112 [==============================] - 143s - loss: 1.8093 - acc: 0.2520 - val_loss: 1.8066 - val_acc: 0.2539\n",
      "Epoch 5/5\n",
      "112/112 [==============================] - 144s - loss: 1.8092 - acc: 0.2512 - val_loss: 1.8157 - val_acc: 0.2458\n",
      "epoch set: 5 lr value: 3.05176e-07\n",
      "Epoch 1/5\n",
      "112/112 [==============================] - 147s - loss: 1.8098 - acc: 0.2513 - val_loss: 1.8112 - val_acc: 0.2497\n",
      "Epoch 2/5\n",
      "112/112 [==============================] - 145s - loss: 1.8102 - acc: 0.2519 - val_loss: 1.8202 - val_acc: 0.2431\n",
      "Epoch 3/5\n",
      "112/112 [==============================] - 146s - loss: 1.8094 - acc: 0.2513 - val_loss: 1.8067 - val_acc: 0.2569\n",
      "Epoch 4/5\n",
      "112/112 [==============================] - 146s - loss: 1.8116 - acc: 0.2493 - val_loss: 1.8120 - val_acc: 0.2500\n",
      "Epoch 5/5\n",
      "112/112 [==============================] - 142s - loss: 1.8083 - acc: 0.2519 - val_loss: 1.8129 - val_acc: 0.2497\n",
      "epoch set: 6 lr value: 1.52588e-07\n",
      "Epoch 1/5\n",
      "112/112 [==============================] - 143s - loss: 1.8100 - acc: 0.2514 - val_loss: 1.8116 - val_acc: 0.2489\n",
      "Epoch 2/5\n",
      "112/112 [==============================] - 141s - loss: 1.8097 - acc: 0.2510 - val_loss: 1.8153 - val_acc: 0.2488\n",
      "Epoch 3/5\n",
      "112/112 [==============================] - 141s - loss: 1.8095 - acc: 0.2521 - val_loss: 1.8165 - val_acc: 0.2470\n",
      "Epoch 4/5\n",
      "112/112 [==============================] - 142s - loss: 1.8097 - acc: 0.2506 - val_loss: 1.8060 - val_acc: 0.2551\n",
      "Epoch 5/5\n",
      "112/112 [==============================] - 143s - loss: 1.8086 - acc: 0.2535 - val_loss: 1.8145 - val_acc: 0.2392\n",
      "epoch set: 7 lr value: 7.62939e-08\n",
      "Epoch 1/5\n",
      "112/112 [==============================] - 142s - loss: 1.8098 - acc: 0.2515 - val_loss: 1.8116 - val_acc: 0.2492\n",
      "Epoch 2/5\n",
      "112/112 [==============================] - 139s - loss: 1.8099 - acc: 0.2503 - val_loss: 1.8126 - val_acc: 0.2449\n",
      "Epoch 3/5\n",
      "112/112 [==============================] - 140s - loss: 1.8087 - acc: 0.2517 - val_loss: 1.8090 - val_acc: 0.2512\n",
      "Epoch 4/5\n",
      "112/112 [==============================] - 139s - loss: 1.8113 - acc: 0.2523 - val_loss: 1.8106 - val_acc: 0.2524\n",
      "Epoch 5/5\n",
      "112/112 [==============================] - 140s - loss: 1.8100 - acc: 0.2511 - val_loss: 1.8150 - val_acc: 0.2425\n",
      "epoch set: 8 lr value: 3.8147e-08\n",
      "Epoch 1/5\n",
      "112/112 [==============================] - 141s - loss: 1.8100 - acc: 0.2514 - val_loss: 1.8112 - val_acc: 0.2497\n",
      "Epoch 2/5\n",
      "112/112 [==============================] - 140s - loss: 1.8096 - acc: 0.2514 - val_loss: 1.8163 - val_acc: 0.2506\n",
      "Epoch 3/5\n",
      "112/112 [==============================] - 140s - loss: 1.8086 - acc: 0.2519 - val_loss: 1.8093 - val_acc: 0.2485\n",
      "Epoch 4/5\n",
      "112/112 [==============================] - 140s - loss: 1.8107 - acc: 0.2508 - val_loss: 1.8147 - val_acc: 0.2488\n",
      "Epoch 5/5\n",
      "112/112 [==============================] - 140s - loss: 1.8095 - acc: 0.2517 - val_loss: 1.8145 - val_acc: 0.2488\n",
      "epoch set: 9 lr value: 1.90735e-08\n",
      "Epoch 1/5\n",
      "112/112 [==============================] - 145s - loss: 1.8099 - acc: 0.2514 - val_loss: 1.8113 - val_acc: 0.2494\n",
      "Epoch 2/5\n",
      "112/112 [==============================] - 149s - loss: 1.8092 - acc: 0.2514 - val_loss: 1.8184 - val_acc: 0.2446\n",
      "Epoch 3/5\n",
      "112/112 [==============================] - 149s - loss: 1.8101 - acc: 0.2501 - val_loss: 1.8127 - val_acc: 0.2515\n",
      "Epoch 4/5\n",
      "112/112 [==============================] - 146s - loss: 1.8111 - acc: 0.2518 - val_loss: 1.8145 - val_acc: 0.2437\n",
      "Epoch 5/5\n",
      "112/112 [==============================] - 145s - loss: 1.8100 - acc: 0.2506 - val_loss: 1.8137 - val_acc: 0.2551\n"
     ]
    }
   ],
   "source": [
    "# continue training\n",
    "epoch_sets = 10\n",
    "epochs = 5\n",
    "batch_size = 256\n",
    "for es in range(epoch_sets):\n",
    "    # get current lr\n",
    "    lr = k.get_value(sgd.lr)\n",
    "    print('epoch set:', es, 'lr value:', lr)\n",
    "    \n",
    "    # fit model\n",
    "    model.fit_generator(train_datagen.flow(x_train, y_train_onehot, batch_size=batch_size)\n",
    "                        , steps_per_epoch=n_train//batch_size\n",
    "                        , epochs=epochs\n",
    "                        , validation_data=test_datagen.flow(x_test, y_test_onehot, batch_size=batch_size)\n",
    "                        , validation_steps=n_test//batch_size)\n",
    "    # decay lr by half\n",
    "    lr = lr / 2\n",
    "    k.set_value(sgd.lr, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save and evaluate model\n",
    "optimizer_name = 'sgd_decay2'\n",
    "model.save('./Model/' + model_name + '_' + optimizer_name +'.h5')\n",
    "model.save_weights('./Model/' + model_name + '_' + optimizer_name + '_weights.h5')\n",
    "# print(model.evaluate_generator(test_datagen.flow(x_test, y_test_onehot, batch_size=batch_size), steps=batch_size))\n",
    "\n",
    "# test model\n",
    "# test_datagen.fit(x_private)\n",
    "# print(model.evaluate_generator(test_datagen.flow(x_private, y_private_onehot, batch_size=batch_size), steps=batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocess: horizontal_flip + truncated_normal initialization + sgd_decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files (x86)\\Python35-64bit\\lib\\site-packages\\keras\\preprocessing\\image.py:648: UserWarning: Expected input to be images (as Numpy array) following the data format convention \"channels_last\" (channels on axis 3), i.e. expected either 1, 3 or 4 channels on axis 3. However, it was passed an array with shape (28698, 48, 48, 1) (1 channels).\n",
      "  ' (' + str(x.shape[self.channel_axis]) + ' channels).')\n",
      "C:\\Program Files (x86)\\Python35-64bit\\lib\\site-packages\\keras\\preprocessing\\image.py:648: UserWarning: Expected input to be images (as Numpy array) following the data format convention \"channels_last\" (channels on axis 3), i.e. expected either 1, 3 or 4 channels on axis 3. However, it was passed an array with shape (3588, 48, 48, 1) (1 channels).\n",
      "  ' (' + str(x.shape[self.channel_axis]) + ' channels).')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "cropping2d_2 (Cropping2D)    (None, 42, 42, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 42, 42, 32)        832       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 21, 21, 32)        0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_2 (ZeroPaddin (None, 23, 23, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 20, 20, 32)        16416     \n",
      "_________________________________________________________________\n",
      "average_pooling2d_3 (Average (None, 10, 10, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 10, 10, 64)        51264     \n",
      "_________________________________________________________________\n",
      "average_pooling2d_4 (Average (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 3072)              4918272   \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 7)                 21511     \n",
      "=================================================================\n",
      "Total params: 5,008,295\n",
      "Trainable params: 5,008,295\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "epoch set: 0 lr value: 0.01\n",
      "Epoch 1/5\n",
      "112/112 [==============================] - 139s - loss: 1.8351 - acc: 0.2454 - val_loss: 1.8155 - val_acc: 0.2492\n",
      "Epoch 2/5\n",
      "112/112 [==============================] - 136s - loss: 1.8143 - acc: 0.2524 - val_loss: 1.8043 - val_acc: 0.2542\n",
      "Epoch 3/5\n",
      "112/112 [==============================] - 136s - loss: 1.8123 - acc: 0.2514 - val_loss: 1.8138 - val_acc: 0.2443\n",
      "Epoch 4/5\n",
      "112/112 [==============================] - 136s - loss: 1.8119 - acc: 0.2531 - val_loss: 1.8102 - val_acc: 0.2485\n",
      "Epoch 5/5\n",
      "112/112 [==============================] - 136s - loss: 1.8115 - acc: 0.2505 - val_loss: 1.8137 - val_acc: 0.2485\n",
      "epoch set: 1 lr value: 0.005\n",
      "Epoch 1/5\n",
      "112/112 [==============================] - 138s - loss: 1.8111 - acc: 0.2514 - val_loss: 1.8119 - val_acc: 0.2497\n",
      "Epoch 2/5\n",
      "112/112 [==============================] - 136s - loss: 1.8095 - acc: 0.2519 - val_loss: 1.8083 - val_acc: 0.2401\n",
      "Epoch 3/5\n",
      "112/112 [==============================] - 137s - loss: 1.8126 - acc: 0.2495 - val_loss: 1.8071 - val_acc: 0.2461\n",
      "Epoch 4/5\n",
      "112/112 [==============================] - 140s - loss: 1.8117 - acc: 0.2518 - val_loss: 1.8200 - val_acc: 0.2452\n",
      "Epoch 5/5\n",
      "112/112 [==============================] - 135s - loss: 1.8080 - acc: 0.2533 - val_loss: 1.8062 - val_acc: 0.2524\n",
      "epoch set: 2 lr value: 0.0025\n",
      "Epoch 1/5\n",
      "112/112 [==============================] - 136s - loss: 1.8102 - acc: 0.2514 - val_loss: 1.8112 - val_acc: 0.2494\n",
      "Epoch 2/5\n",
      "112/112 [==============================] - 137s - loss: 1.8100 - acc: 0.2516 - val_loss: 1.8020 - val_acc: 0.2563\n",
      "Epoch 3/5\n",
      "112/112 [==============================] - 137s - loss: 1.8099 - acc: 0.2514 - val_loss: 1.8111 - val_acc: 0.2497\n",
      "Epoch 4/5\n",
      "112/112 [==============================] - 136s - loss: 1.8099 - acc: 0.2524 - val_loss: 1.8141 - val_acc: 0.2464\n",
      "Epoch 5/5\n",
      "112/112 [==============================] - 136s - loss: 1.8120 - acc: 0.2504 - val_loss: 1.8125 - val_acc: 0.2530\n",
      "epoch set: 3 lr value: 0.00125\n",
      "Epoch 1/5\n",
      "112/112 [==============================] - 137s - loss: 1.8102 - acc: 0.2515 - val_loss: 1.8109 - val_acc: 0.2497\n",
      "Epoch 2/5\n",
      "112/112 [==============================] - 136s - loss: 1.8106 - acc: 0.2505 - val_loss: 1.8123 - val_acc: 0.2575\n",
      "Epoch 3/5\n",
      "112/112 [==============================] - 136s - loss: 1.8112 - acc: 0.2496 - val_loss: 1.8129 - val_acc: 0.2488\n",
      "Epoch 4/5\n",
      "112/112 [==============================] - 137s - loss: 1.8086 - acc: 0.2517 - val_loss: 1.8128 - val_acc: 0.2497\n",
      "Epoch 5/5\n",
      "112/112 [==============================] - 136s - loss: 1.8105 - acc: 0.2499 - val_loss: 1.8132 - val_acc: 0.2455\n",
      "epoch set: 4 lr value: 0.000625\n",
      "Epoch 1/5\n",
      "112/112 [==============================] - 137s - loss: 1.8099 - acc: 0.2513 - val_loss: 1.8110 - val_acc: 0.2497\n",
      "Epoch 2/5\n",
      "112/112 [==============================] - 136s - loss: 1.8095 - acc: 0.2522 - val_loss: 1.8154 - val_acc: 0.2473\n",
      "Epoch 3/5\n",
      "112/112 [==============================] - 136s - loss: 1.8096 - acc: 0.2506 - val_loss: 1.8064 - val_acc: 0.2539\n",
      "Epoch 4/5\n",
      "112/112 [==============================] - 135s - loss: 1.8095 - acc: 0.2525 - val_loss: 1.8157 - val_acc: 0.2476\n",
      "Epoch 5/5\n",
      "112/112 [==============================] - 136s - loss: 1.8117 - acc: 0.2519 - val_loss: 1.8082 - val_acc: 0.2539\n",
      "epoch set: 5 lr value: 0.0003125\n",
      "Epoch 1/5\n",
      "112/112 [==============================] - 137s - loss: 1.8101 - acc: 0.2514 - val_loss: 1.8108 - val_acc: 0.2497\n",
      "Epoch 2/5\n",
      "112/112 [==============================] - 133s - loss: 1.8097 - acc: 0.2511 - val_loss: 1.8080 - val_acc: 0.2569\n",
      "Epoch 3/5\n",
      "112/112 [==============================] - 132s - loss: 1.8089 - acc: 0.2534 - val_loss: 1.8109 - val_acc: 0.2506\n",
      "Epoch 4/5\n",
      "112/112 [==============================] - 132s - loss: 1.8101 - acc: 0.2506 - val_loss: 1.8149 - val_acc: 0.2383\n",
      "Epoch 5/5\n",
      "112/112 [==============================] - 132s - loss: 1.8099 - acc: 0.2519 - val_loss: 1.8117 - val_acc: 0.2470\n",
      "epoch set: 6 lr value: 0.00015625\n",
      "Epoch 1/5\n",
      "112/112 [==============================] - 134s - loss: 1.8098 - acc: 0.2515 - val_loss: 1.8111 - val_acc: 0.2497\n",
      "Epoch 2/5\n",
      "112/112 [==============================] - 133s - loss: 1.8107 - acc: 0.2507 - val_loss: 1.8117 - val_acc: 0.2455\n",
      "Epoch 3/5\n",
      "112/112 [==============================] - 132s - loss: 1.8095 - acc: 0.2510 - val_loss: 1.8126 - val_acc: 0.2464\n",
      "Epoch 4/5\n",
      "112/112 [==============================] - 133s - loss: 1.8091 - acc: 0.2515 - val_loss: 1.8229 - val_acc: 0.2380\n",
      "Epoch 5/5\n",
      "112/112 [==============================] - 133s - loss: 1.8104 - acc: 0.2513 - val_loss: 1.8142 - val_acc: 0.2488\n",
      "epoch set: 7 lr value: 7.8125e-05\n",
      "Epoch 1/5\n",
      "112/112 [==============================] - 135s - loss: 1.8096 - acc: 0.2515 - val_loss: 1.8113 - val_acc: 0.2492\n",
      "Epoch 2/5\n",
      "112/112 [==============================] - 132s - loss: 1.8116 - acc: 0.2502 - val_loss: 1.8123 - val_acc: 0.2497\n",
      "Epoch 3/5\n",
      "112/112 [==============================] - 133s - loss: 1.8084 - acc: 0.2525 - val_loss: 1.8033 - val_acc: 0.2596\n",
      "Epoch 4/5\n",
      "112/112 [==============================] - 140s - loss: 1.8128 - acc: 0.2505 - val_loss: 1.8146 - val_acc: 0.2485\n",
      "Epoch 5/5\n",
      "112/112 [==============================] - 136s - loss: 1.8099 - acc: 0.2512 - val_loss: 1.8111 - val_acc: 0.2524\n",
      "epoch set: 8 lr value: 3.90625e-05\n",
      "Epoch 1/5\n",
      "112/112 [==============================] - 138s - loss: 1.8102 - acc: 0.2514 - val_loss: 1.8112 - val_acc: 0.2494\n",
      "Epoch 2/5\n",
      "112/112 [==============================] - 136s - loss: 1.8103 - acc: 0.2505 - val_loss: 1.8129 - val_acc: 0.2479\n",
      "Epoch 3/5\n",
      "112/112 [==============================] - 141s - loss: 1.8092 - acc: 0.2522 - val_loss: 1.8100 - val_acc: 0.2530\n",
      "Epoch 4/5\n",
      "112/112 [==============================] - 137s - loss: 1.8101 - acc: 0.2512 - val_loss: 1.8044 - val_acc: 0.2569\n",
      "Epoch 5/5\n",
      "112/112 [==============================] - 138s - loss: 1.8109 - acc: 0.2511 - val_loss: 1.8153 - val_acc: 0.2476\n",
      "epoch set: 9 lr value: 1.95312e-05\n",
      "Epoch 1/5\n",
      "112/112 [==============================] - 140s - loss: 1.8096 - acc: 0.2514 - val_loss: 1.8103 - val_acc: 0.2497\n",
      "Epoch 2/5\n",
      "112/112 [==============================] - 139s - loss: 1.8102 - acc: 0.2514 - val_loss: 1.8145 - val_acc: 0.2518\n",
      "Epoch 3/5\n",
      "112/112 [==============================] - 139s - loss: 1.8114 - acc: 0.2499 - val_loss: 1.8196 - val_acc: 0.2401\n",
      "Epoch 4/5\n",
      "112/112 [==============================] - 139s - loss: 1.8089 - acc: 0.2519 - val_loss: 1.8134 - val_acc: 0.2464\n",
      "Epoch 5/5\n",
      "112/112 [==============================] - 139s - loss: 1.8090 - acc: 0.2518 - val_loss: 1.8049 - val_acc: 0.2548\n"
     ]
    }
   ],
   "source": [
    "# preprocess\n",
    "rescale_factor = 255\n",
    "train_datagen = ImageDataGenerator(featurewise_center=True, featurewise_std_normalization=True\n",
    "                                   , rotation_range=45., width_shift_range=0.2, height_shift_range=0.2\n",
    "                                   , horizontal_flip=True, rescale=1./rescale_factor)\n",
    "train_datagen.fit(x_train)\n",
    "\n",
    "test_datagen = ImageDataGenerator(featurewise_center=True, featurewise_std_normalization=True\n",
    "                                  , rescale=1./rescale_factor)\n",
    "test_datagen.fit(x_test)\n",
    "\n",
    "# build model\n",
    "model_name = '1st_place'\n",
    "model = Sequential(name=model_name)\n",
    "model.add(Cropping2D(cropping=3, input_shape=image_shape))\n",
    "model.add(Conv2D(32, (5,5), padding='same', activation='relu', kernel_initializer='TruncatedNormal'))\n",
    "model.add(MaxPooling2D((2,2), strides=(2,2), padding='valid'))\n",
    "\n",
    "model.add(ZeroPadding2D(padding=1))\n",
    "model.add(Conv2D(32, (4,4), padding='valid', activation='relu', kernel_initializer='TruncatedNormal'))\n",
    "model.add(AveragePooling2D((2,2), strides=(2,2), padding='valid'))\n",
    "\n",
    "model.add(Conv2D(64, (5,5), padding='same', activation='relu', kernel_initializer='TruncatedNormal'))\n",
    "model.add(AveragePooling2D((2,2), strides=(2,2), padding='valid'))\n",
    "\n",
    "model.add(Flatten())\n",
    "# TODO is this where dropout should be?\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(3072, activation='relu', kernel_initializer='TruncatedNormal'))\n",
    "# TODO svm activation function\n",
    "model.add(Dense(n_classes, activation='softmax', kernel_initializer='TruncatedNormal'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# compile model\n",
    "sgd = optimizers.SGD(momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "# fit model\n",
    "epoch_sets = 10\n",
    "epochs = 5\n",
    "batch_size = 256\n",
    "for es in range(epoch_sets):\n",
    "    # get current lr\n",
    "    lr = k.get_value(sgd.lr)\n",
    "    print('epoch set:', es, 'lr value:', lr)\n",
    "    \n",
    "    # fit model\n",
    "    model.fit_generator(train_datagen.flow(x_train, y_train_onehot, batch_size=batch_size)\n",
    "                        , steps_per_epoch=n_train//batch_size\n",
    "                        , epochs=epochs\n",
    "                        , validation_data=test_datagen.flow(x_test, y_test_onehot, batch_size=batch_size)\n",
    "                        , validation_steps=n_test//batch_size)\n",
    "    # decay lr by half\n",
    "    lr = lr / 2\n",
    "    k.set_value(sgd.lr, lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocess: horizontal_flip, test on train + truncated_normal initialization + sgd_decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files (x86)\\Python35-64bit\\lib\\site-packages\\keras\\preprocessing\\image.py:648: UserWarning: Expected input to be images (as Numpy array) following the data format convention \"channels_last\" (channels on axis 3), i.e. expected either 1, 3 or 4 channels on axis 3. However, it was passed an array with shape (28698, 48, 48, 1) (1 channels).\n",
      "  ' (' + str(x.shape[self.channel_axis]) + ' channels).')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "cropping2d_3 (Cropping2D)    (None, 42, 42, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 42, 42, 32)        832       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 21, 21, 32)        0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_3 (ZeroPaddin (None, 23, 23, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 20, 20, 32)        16416     \n",
      "_________________________________________________________________\n",
      "average_pooling2d_5 (Average (None, 10, 10, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 10, 10, 64)        51264     \n",
      "_________________________________________________________________\n",
      "average_pooling2d_6 (Average (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 3072)              4918272   \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 7)                 21511     \n",
      "=================================================================\n",
      "Total params: 5,008,295\n",
      "Trainable params: 5,008,295\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "epoch set: 0 lr value: 0.01\n",
      "Epoch 1/5\n",
      "112/112 [==============================] - 146s - loss: 1.8206 - acc: 0.2477 - val_loss: 1.8126 - val_acc: 0.2494\n",
      "Epoch 2/5\n",
      "112/112 [==============================] - 140s - loss: 1.8119 - acc: 0.2521 - val_loss: 1.8121 - val_acc: 0.2473\n",
      "Epoch 3/5\n",
      "112/112 [==============================] - 139s - loss: 1.8129 - acc: 0.2504 - val_loss: 1.8073 - val_acc: 0.2566\n",
      "Epoch 4/5\n",
      "112/112 [==============================] - 139s - loss: 1.8106 - acc: 0.2534 - val_loss: 1.8219 - val_acc: 0.2527\n",
      "Epoch 5/5\n",
      "112/112 [==============================] - 139s - loss: 1.8129 - acc: 0.2510 - val_loss: 1.8078 - val_acc: 0.2551\n",
      "epoch set: 1 lr value: 0.005\n",
      "Epoch 1/5\n",
      "112/112 [==============================] - 140s - loss: 1.8112 - acc: 0.2512 - val_loss: 1.8109 - val_acc: 0.2494\n",
      "Epoch 2/5\n",
      "112/112 [==============================] - 138s - loss: 1.8093 - acc: 0.2530 - val_loss: 1.8178 - val_acc: 0.2470\n",
      "Epoch 3/5\n",
      "112/112 [==============================] - 139s - loss: 1.8128 - acc: 0.2496 - val_loss: 1.8040 - val_acc: 0.2530\n",
      "Epoch 4/5\n",
      "112/112 [==============================] - 138s - loss: 1.8099 - acc: 0.2533 - val_loss: 1.8159 - val_acc: 0.2470\n",
      "Epoch 5/5\n",
      "112/112 [==============================] - 139s - loss: 1.8093 - acc: 0.2516 - val_loss: 1.8233 - val_acc: 0.2443\n",
      "epoch set: 2 lr value: 0.0025\n",
      "Epoch 1/5\n",
      "112/112 [==============================] - 140s - loss: 1.8106 - acc: 0.2515 - val_loss: 1.8113 - val_acc: 0.2494\n",
      "Epoch 2/5\n",
      "112/112 [==============================] - 139s - loss: 1.8104 - acc: 0.2504 - val_loss: 1.8275 - val_acc: 0.2410\n",
      "Epoch 3/5\n",
      "112/112 [==============================] - 140s - loss: 1.8100 - acc: 0.2521 - val_loss: 1.8117 - val_acc: 0.2407\n",
      "Epoch 4/5\n",
      "112/112 [==============================] - 136s - loss: 1.8091 - acc: 0.2514 - val_loss: 1.8190 - val_acc: 0.2407\n",
      "Epoch 5/5\n",
      "112/112 [==============================] - 135s - loss: 1.8104 - acc: 0.2525 - val_loss: 1.8123 - val_acc: 0.2473\n",
      "epoch set: 3 lr value: 0.00125\n",
      "Epoch 1/5\n",
      "112/112 [==============================] - 137s - loss: 1.8101 - acc: 0.2515 - val_loss: 1.8114 - val_acc: 0.2494\n",
      "Epoch 2/5\n",
      "112/112 [==============================] - 136s - loss: 1.8109 - acc: 0.2506 - val_loss: 1.8155 - val_acc: 0.2389\n",
      "Epoch 3/5\n",
      "112/112 [==============================] - 135s - loss: 1.8114 - acc: 0.2520 - val_loss: 1.8039 - val_acc: 0.2542\n",
      "Epoch 4/5\n",
      "112/112 [==============================] - 136s - loss: 1.8111 - acc: 0.2504 - val_loss: 1.8141 - val_acc: 0.2434\n",
      "Epoch 5/5\n",
      "112/112 [==============================] - 136s - loss: 1.8108 - acc: 0.2498 - val_loss: 1.8022 - val_acc: 0.2563\n",
      "epoch set: 4 lr value: 0.000625\n",
      "Epoch 1/5\n",
      "112/112 [==============================] - 138s - loss: 1.8100 - acc: 0.2514 - val_loss: 1.8112 - val_acc: 0.2494\n",
      "Epoch 2/5\n",
      "112/112 [==============================] - 136s - loss: 1.8099 - acc: 0.2517 - val_loss: 1.8135 - val_acc: 0.2524\n",
      "Epoch 3/5\n",
      "112/112 [==============================] - 135s - loss: 1.8094 - acc: 0.2504 - val_loss: 1.8092 - val_acc: 0.2470\n",
      "Epoch 4/5\n",
      "112/112 [==============================] - 135s - loss: 1.8099 - acc: 0.2523 - val_loss: 1.8130 - val_acc: 0.2446\n",
      "Epoch 5/5\n",
      "112/112 [==============================] - 135s - loss: 1.8108 - acc: 0.2497 - val_loss: 1.8031 - val_acc: 0.2542\n",
      "epoch set: 5 lr value: 0.0003125\n",
      "Epoch 1/5\n",
      "112/112 [==============================] - 137s - loss: 1.8098 - acc: 0.2514 - val_loss: 1.8111 - val_acc: 0.2497\n",
      "Epoch 2/5\n",
      "112/112 [==============================] - 135s - loss: 1.8103 - acc: 0.2517 - val_loss: 1.8160 - val_acc: 0.2410\n",
      "Epoch 3/5\n",
      "112/112 [==============================] - 136s - loss: 1.8107 - acc: 0.2502 - val_loss: 1.8080 - val_acc: 0.2515\n",
      "Epoch 4/5\n",
      "112/112 [==============================] - 135s - loss: 1.8086 - acc: 0.2525 - val_loss: 1.8187 - val_acc: 0.2458\n",
      "Epoch 5/5\n",
      "112/112 [==============================] - 135s - loss: 1.8091 - acc: 0.2504 - val_loss: 1.8044 - val_acc: 0.2464\n",
      "epoch set: 6 lr value: 0.00015625\n",
      "Epoch 1/5\n",
      "112/112 [==============================] - 137s - loss: 1.8100 - acc: 0.2513 - val_loss: 1.8111 - val_acc: 0.2494\n",
      "Epoch 2/5\n",
      "112/112 [==============================] - 136s - loss: 1.8090 - acc: 0.2526 - val_loss: 1.8159 - val_acc: 0.2452\n",
      "Epoch 3/5\n",
      "112/112 [==============================] - 136s - loss: 1.8103 - acc: 0.2513 - val_loss: 1.8101 - val_acc: 0.2518\n",
      "Epoch 4/5\n",
      "112/112 [==============================] - 136s - loss: 1.8095 - acc: 0.2530 - val_loss: 1.8163 - val_acc: 0.2488\n",
      "Epoch 5/5\n",
      "112/112 [==============================] - 138s - loss: 1.8085 - acc: 0.2517 - val_loss: 1.8107 - val_acc: 0.2497\n",
      "epoch set: 7 lr value: 7.8125e-05\n",
      "Epoch 1/5\n",
      "112/112 [==============================] - 139s - loss: 1.8101 - acc: 0.2514 - val_loss: 1.8107 - val_acc: 0.2494\n",
      "Epoch 2/5\n",
      "112/112 [==============================] - 137s - loss: 1.8095 - acc: 0.2516 - val_loss: 1.8139 - val_acc: 0.2452\n",
      "Epoch 3/5\n",
      "112/112 [==============================] - 137s - loss: 1.8096 - acc: 0.2523 - val_loss: 1.8138 - val_acc: 0.2488\n",
      "Epoch 4/5\n",
      "112/112 [==============================] - 137s - loss: 1.8094 - acc: 0.2523 - val_loss: 1.8134 - val_acc: 0.2467\n",
      "Epoch 5/5\n",
      "112/112 [==============================] - 137s - loss: 1.8102 - acc: 0.2503 - val_loss: 1.8161 - val_acc: 0.2392\n",
      "epoch set: 8 lr value: 3.90625e-05\n",
      "Epoch 1/5\n",
      "112/112 [==============================] - 141s - loss: 1.8099 - acc: 0.2514 - val_loss: 1.8113 - val_acc: 0.2494\n",
      "Epoch 2/5\n",
      "112/112 [==============================] - 139s - loss: 1.8100 - acc: 0.2507 - val_loss: 1.8108 - val_acc: 0.2485\n",
      "Epoch 3/5\n",
      "112/112 [==============================] - 139s - loss: 1.8098 - acc: 0.2523 - val_loss: 1.8033 - val_acc: 0.2554\n",
      "Epoch 4/5\n",
      "112/112 [==============================] - 140s - loss: 1.8108 - acc: 0.2491 - val_loss: 1.7971 - val_acc: 0.2590\n",
      "Epoch 5/5\n",
      "112/112 [==============================] - 139s - loss: 1.8097 - acc: 0.2519 - val_loss: 1.8156 - val_acc: 0.2416\n",
      "epoch set: 9 lr value: 1.95312e-05\n",
      "Epoch 1/5\n",
      "112/112 [==============================] - 141s - loss: 1.8103 - acc: 0.2513 - val_loss: 1.8113 - val_acc: 0.2494\n",
      "Epoch 2/5\n",
      "112/112 [==============================] - 148s - loss: 1.8093 - acc: 0.2522 - val_loss: 1.8200 - val_acc: 0.2383\n",
      "Epoch 3/5\n",
      "112/112 [==============================] - 141s - loss: 1.8093 - acc: 0.2518 - val_loss: 1.8088 - val_acc: 0.2452\n",
      "Epoch 4/5\n",
      "112/112 [==============================] - 140s - loss: 1.8107 - acc: 0.2516 - val_loss: 1.8108 - val_acc: 0.2524\n",
      "Epoch 5/5\n",
      "112/112 [==============================] - 143s - loss: 1.8077 - acc: 0.2536 - val_loss: 1.8179 - val_acc: 0.2452\n"
     ]
    }
   ],
   "source": [
    "# preprocess\n",
    "rescale_factor = 255\n",
    "train_datagen = ImageDataGenerator(featurewise_center=True, featurewise_std_normalization=True\n",
    "                                   , rotation_range=45., width_shift_range=0.2, height_shift_range=0.2\n",
    "                                   , horizontal_flip=True, rescale=1./rescale_factor)\n",
    "train_datagen.fit(x_train)\n",
    "\n",
    "test_datagen = ImageDataGenerator(featurewise_center=True, featurewise_std_normalization=True\n",
    "                                  , rescale=1./rescale_factor)\n",
    "test_datagen.fit(x_train)\n",
    "\n",
    "# build model\n",
    "model_name = '1st_place'\n",
    "model = Sequential(name=model_name)\n",
    "model.add(Cropping2D(cropping=3, input_shape=image_shape))\n",
    "model.add(Conv2D(32, (5,5), padding='same', activation='relu', kernel_initializer='TruncatedNormal'))\n",
    "model.add(MaxPooling2D((2,2), strides=(2,2), padding='valid'))\n",
    "\n",
    "model.add(ZeroPadding2D(padding=1))\n",
    "model.add(Conv2D(32, (4,4), padding='valid', activation='relu', kernel_initializer='TruncatedNormal'))\n",
    "model.add(AveragePooling2D((2,2), strides=(2,2), padding='valid'))\n",
    "\n",
    "model.add(Conv2D(64, (5,5), padding='same', activation='relu', kernel_initializer='TruncatedNormal'))\n",
    "model.add(AveragePooling2D((2,2), strides=(2,2), padding='valid'))\n",
    "\n",
    "model.add(Flatten())\n",
    "# TODO is this where dropout should be?\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(3072, activation='relu', kernel_initializer='TruncatedNormal'))\n",
    "# TODO svm activation function\n",
    "model.add(Dense(n_classes, activation='softmax', kernel_initializer='TruncatedNormal'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# compile model\n",
    "sgd = optimizers.SGD(momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "# fit model\n",
    "epoch_sets = 10\n",
    "epochs = 5\n",
    "batch_size = 256\n",
    "for es in range(epoch_sets):\n",
    "    # get current lr\n",
    "    lr = k.get_value(sgd.lr)\n",
    "    print('epoch set:', es, 'lr value:', lr)\n",
    "    \n",
    "    # fit model\n",
    "    model.fit_generator(train_datagen.flow(x_train, y_train_onehot, batch_size=batch_size)\n",
    "                        , steps_per_epoch=n_train//batch_size\n",
    "                        , epochs=epochs\n",
    "                        , validation_data=test_datagen.flow(x_test, y_test_onehot, batch_size=batch_size)\n",
    "                        , validation_steps=n_test//batch_size)\n",
    "    # decay lr by half\n",
    "    lr = lr / 2\n",
    "    k.set_value(sgd.lr, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save and evaluate model\n",
    "optimizer_name = 'sgd_decay'\n",
    "model.save('./Model/' + model_name + '_' + optimizer_name +'.h5')\n",
    "model.save_weights('./Model/' + model_name + '_' + optimizer_name + '_weights.h5')\n",
    "# print(model.evaluate_generator(test_datagen.flow(x_test, y_test_onehot, batch_size=batch_size), steps=batch_size))\n",
    "\n",
    "# test model\n",
    "# print(model.evaluate_generator(test_datagen.flow(x_private, y_private_onehot, batch_size=batch_size), steps=batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocess: horizontal_flip, test on train + truncated_normal initialization + sgd_decay on tiny subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files (x86)\\Python35-64bit\\lib\\site-packages\\keras\\preprocessing\\image.py:648: UserWarning: Expected input to be images (as Numpy array) following the data format convention \"channels_last\" (channels on axis 3), i.e. expected either 1, 3 or 4 channels on axis 3. However, it was passed an array with shape (50, 48, 48, 1) (1 channels).\n",
      "  ' (' + str(x.shape[self.channel_axis]) + ' channels).')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "cropping2d_3 (Cropping2D)    (None, 42, 42, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 42, 42, 32)        832       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 21, 21, 32)        0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_3 (ZeroPaddin (None, 23, 23, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 20, 20, 32)        16416     \n",
      "_________________________________________________________________\n",
      "average_pooling2d_5 (Average (None, 10, 10, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 10, 10, 64)        51264     \n",
      "_________________________________________________________________\n",
      "average_pooling2d_6 (Average (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 3072)              4918272   \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 7)                 21511     \n",
      "=================================================================\n",
      "Total params: 5,008,295\n",
      "Trainable params: 5,008,295\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "epoch set: 0 lr value: 0.01\n",
      "Epoch 1/5\n",
      "3/3 [==============================] - 1s - loss: 1.9163 - acc: 0.4375 - val_loss: 1.6668 - val_acc: 0.3542\n",
      "Epoch 2/5\n",
      "3/3 [==============================] - 0s - loss: 1.7606 - acc: 0.3851 - val_loss: 0.6956 - val_acc: 0.5882\n",
      "Epoch 3/5\n",
      "3/3 [==============================] - 0s - loss: 4.2770 - acc: 0.3906 - val_loss: 2.4054 - val_acc: 0.6176\n",
      "Epoch 4/5\n",
      "3/3 [==============================] - 0s - loss: 2.0193 - acc: 0.5689 - val_loss: 1.3290 - val_acc: 0.6471\n",
      "Epoch 5/5\n",
      "3/3 [==============================] - 0s - loss: 1.3567 - acc: 0.6042 - val_loss: 0.9135 - val_acc: 0.7647\n",
      "epoch set: 1 lr value: 0.005\n",
      "Epoch 1/5\n",
      "3/3 [==============================] - 0s - loss: 1.3583 - acc: 0.5833 - val_loss: 1.1423 - val_acc: 0.6250\n",
      "Epoch 2/5\n",
      "3/3 [==============================] - 0s - loss: 0.9352 - acc: 0.7472 - val_loss: 1.0296 - val_acc: 0.6471\n",
      "Epoch 3/5\n",
      "3/3 [==============================] - 0s - loss: 1.0250 - acc: 0.6783 - val_loss: 0.9883 - val_acc: 0.6176\n",
      "Epoch 4/5\n",
      "3/3 [==============================] - 0s - loss: 0.9862 - acc: 0.6553 - val_loss: 0.9206 - val_acc: 0.6176\n",
      "Epoch 5/5\n",
      "3/3 [==============================] - 0s - loss: 0.9528 - acc: 0.6042 - val_loss: 0.8527 - val_acc: 0.6176\n",
      "epoch set: 2 lr value: 0.0025\n",
      "Epoch 1/5\n",
      "3/3 [==============================] - 0s - loss: 0.9036 - acc: 0.5833 - val_loss: 0.7767 - val_acc: 0.6667\n",
      "Epoch 2/5\n",
      "3/3 [==============================] - 0s - loss: 0.6510 - acc: 0.7702 - val_loss: 0.7558 - val_acc: 0.6250\n",
      "Epoch 3/5\n",
      "3/3 [==============================] - 0s - loss: 1.0055 - acc: 0.3676 - val_loss: 0.7856 - val_acc: 0.6176\n",
      "Epoch 4/5\n",
      "3/3 [==============================] - 0s - loss: 0.7325 - acc: 0.6149 - val_loss: 0.8285 - val_acc: 0.2941\n",
      "Epoch 5/5\n",
      "3/3 [==============================] - 0s - loss: 0.7533 - acc: 0.6042 - val_loss: 0.7962 - val_acc: 0.4412\n",
      "epoch set: 3 lr value: 0.00125\n",
      "Epoch 1/5\n",
      "3/3 [==============================] - 0s - loss: 0.7483 - acc: 0.4792 - val_loss: 0.8151 - val_acc: 0.3750\n",
      "Epoch 2/5\n",
      "3/3 [==============================] - 0s - loss: 0.7299 - acc: 0.4311 - val_loss: 0.7983 - val_acc: 0.3824\n",
      "Epoch 3/5\n",
      "3/3 [==============================] - 0s - loss: 0.6945 - acc: 0.5689 - val_loss: 0.8068 - val_acc: 0.3235\n",
      "Epoch 4/5\n",
      "3/3 [==============================] - 0s - loss: 0.7245 - acc: 0.5000 - val_loss: 0.7995 - val_acc: 0.3235\n",
      "Epoch 5/5\n",
      "3/3 [==============================] - 0s - loss: 0.6911 - acc: 0.5833 - val_loss: 0.7478 - val_acc: 0.4412\n",
      "epoch set: 4 lr value: 0.000625\n",
      "Epoch 1/5\n",
      "3/3 [==============================] - 0s - loss: 0.6954 - acc: 0.5625 - val_loss: 0.7740 - val_acc: 0.3542\n",
      "Epoch 2/5\n",
      "3/3 [==============================] - 0s - loss: 0.6511 - acc: 0.5689 - val_loss: 0.7457 - val_acc: 0.3542\n",
      "Epoch 3/5\n",
      "3/3 [==============================] - 0s - loss: 0.7149 - acc: 0.5919 - val_loss: 0.7378 - val_acc: 0.3542\n",
      "Epoch 4/5\n",
      "3/3 [==============================] - 0s - loss: 0.7304 - acc: 0.5689 - val_loss: 0.7376 - val_acc: 0.3824\n",
      "Epoch 5/5\n",
      "3/3 [==============================] - 0s - loss: 0.6958 - acc: 0.6250 - val_loss: 0.7349 - val_acc: 0.4118\n",
      "epoch set: 5 lr value: 0.0003125\n",
      "Epoch 1/5\n",
      "3/3 [==============================] - 0s - loss: 0.6845 - acc: 0.6042 - val_loss: 0.7629 - val_acc: 0.3333\n",
      "Epoch 2/5\n",
      "3/3 [==============================] - 0s - loss: 0.7043 - acc: 0.5689 - val_loss: 0.7709 - val_acc: 0.3333\n",
      "Epoch 3/5\n",
      "3/3 [==============================] - 0s - loss: 0.7357 - acc: 0.4136 - val_loss: 0.7763 - val_acc: 0.3750\n",
      "Epoch 4/5\n",
      "3/3 [==============================] - 0s - loss: 0.5797 - acc: 0.7702 - val_loss: 0.7956 - val_acc: 0.3529\n",
      "Epoch 5/5\n",
      "3/3 [==============================] - 0s - loss: 0.6962 - acc: 0.5833 - val_loss: 0.7709 - val_acc: 0.4118\n",
      "epoch set: 6 lr value: 0.00015625\n",
      "Epoch 1/5\n",
      "3/3 [==============================] - 0s - loss: 0.6881 - acc: 0.5833 - val_loss: 0.7892 - val_acc: 0.3750\n",
      "Epoch 2/5\n",
      "3/3 [==============================] - 0s - loss: 0.7864 - acc: 0.5689 - val_loss: 0.8116 - val_acc: 0.3333\n",
      "Epoch 3/5\n",
      "3/3 [==============================] - 0s - loss: 0.7030 - acc: 0.4825 - val_loss: 0.7637 - val_acc: 0.4412\n",
      "Epoch 4/5\n",
      "3/3 [==============================] - 0s - loss: 0.7423 - acc: 0.4311 - val_loss: 0.7822 - val_acc: 0.4118\n",
      "Epoch 5/5\n",
      "3/3 [==============================] - 0s - loss: 0.6697 - acc: 0.6667 - val_loss: 0.8457 - val_acc: 0.2941\n",
      "epoch set: 7 lr value: 7.8125e-05\n",
      "Epoch 1/5\n",
      "3/3 [==============================] - 0s - loss: 0.7066 - acc: 0.6250 - val_loss: 0.8037 - val_acc: 0.3750\n",
      "Epoch 2/5\n",
      "3/3 [==============================] - 0s - loss: 0.6654 - acc: 0.5689 - val_loss: 0.8045 - val_acc: 0.3750\n",
      "Epoch 3/5\n",
      "3/3 [==============================] - 0s - loss: 0.6735 - acc: 0.5230 - val_loss: 0.8171 - val_acc: 0.3529\n",
      "Epoch 4/5\n",
      "3/3 [==============================] - 0s - loss: 0.7267 - acc: 0.5460 - val_loss: 0.8011 - val_acc: 0.3824\n",
      "Epoch 5/5\n",
      "3/3 [==============================] - 0s - loss: 0.7384 - acc: 0.5208 - val_loss: 0.7698 - val_acc: 0.4412\n",
      "epoch set: 8 lr value: 3.90625e-05\n",
      "Epoch 1/5\n",
      "3/3 [==============================] - 0s - loss: 0.6998 - acc: 0.5625 - val_loss: 0.8153 - val_acc: 0.3542\n",
      "Epoch 2/5\n",
      "3/3 [==============================] - 0s - loss: 0.7023 - acc: 0.5689 - val_loss: 0.8618 - val_acc: 0.2647\n",
      "Epoch 3/5\n",
      "3/3 [==============================] - 0s - loss: 0.7375 - acc: 0.4770 - val_loss: 0.7840 - val_acc: 0.4118\n",
      "Epoch 4/5\n",
      "3/3 [==============================] - 0s - loss: 0.6871 - acc: 0.6553 - val_loss: 0.7837 - val_acc: 0.4118\n",
      "Epoch 5/5\n",
      "3/3 [==============================] - 0s - loss: 0.7250 - acc: 0.6042 - val_loss: 0.7833 - val_acc: 0.4118\n",
      "epoch set: 9 lr value: 1.95312e-05\n",
      "Epoch 1/5\n",
      "3/3 [==============================] - 0s - loss: 0.7214 - acc: 0.5208 - val_loss: 0.8131 - val_acc: 0.3542\n",
      "Epoch 2/5\n",
      "3/3 [==============================] - 0s - loss: 0.6917 - acc: 0.4770 - val_loss: 0.8011 - val_acc: 0.3750\n",
      "Epoch 3/5\n",
      "3/3 [==============================] - 0s - loss: 0.5929 - acc: 0.7932 - val_loss: 0.7956 - val_acc: 0.3824\n",
      "Epoch 4/5\n",
      "3/3 [==============================] - 0s - loss: 0.6858 - acc: 0.6094 - val_loss: 0.8238 - val_acc: 0.3235\n",
      "Epoch 5/5\n",
      "3/3 [==============================] - 0s - loss: 0.6835 - acc: 0.6458 - val_loss: 0.7925 - val_acc: 0.3824\n"
     ]
    }
   ],
   "source": [
    "# preprocess\n",
    "rescale_factor = 255\n",
    "train_datagen = ImageDataGenerator(featurewise_center=True, featurewise_std_normalization=True\n",
    "                                   , rotation_range=45., width_shift_range=0.2, height_shift_range=0.2\n",
    "                                   , horizontal_flip=True, rescale=1./rescale_factor)\n",
    "train_datagen.fit(x_train)\n",
    "\n",
    "test_datagen = ImageDataGenerator(featurewise_center=True, featurewise_std_normalization=True\n",
    "                                  , rescale=1./rescale_factor)\n",
    "test_datagen.fit(x_train)\n",
    "\n",
    "# build model\n",
    "model_name = '1st_place'\n",
    "model = Sequential(name=model_name)\n",
    "model.add(Cropping2D(cropping=3, input_shape=image_shape))\n",
    "model.add(Conv2D(32, (5,5), padding='same', activation='relu', kernel_initializer='TruncatedNormal'))\n",
    "model.add(MaxPooling2D((2,2), strides=(2,2), padding='valid'))\n",
    "\n",
    "model.add(ZeroPadding2D(padding=1))\n",
    "model.add(Conv2D(32, (4,4), padding='valid', activation='relu', kernel_initializer='TruncatedNormal'))\n",
    "model.add(AveragePooling2D((2,2), strides=(2,2), padding='valid'))\n",
    "\n",
    "model.add(Conv2D(64, (5,5), padding='same', activation='relu', kernel_initializer='TruncatedNormal'))\n",
    "model.add(AveragePooling2D((2,2), strides=(2,2), padding='valid'))\n",
    "\n",
    "model.add(Flatten())\n",
    "# TODO is this where dropout should be?\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(3072, activation='relu', kernel_initializer='TruncatedNormal'))\n",
    "# TODO svm activation function\n",
    "model.add(Dense(n_classes, activation='softmax', kernel_initializer='TruncatedNormal'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# compile model\n",
    "sgd = optimizers.SGD(momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "# fit model\n",
    "epoch_sets = 10\n",
    "epochs = 5\n",
    "batch_size = 16\n",
    "for es in range(epoch_sets):\n",
    "    # get current lr\n",
    "    lr = k.get_value(sgd.lr)\n",
    "    print('epoch set:', es, 'lr value:', lr)\n",
    "    \n",
    "    # fit model\n",
    "    model.fit_generator(train_datagen.flow(x_train, y_train_onehot, batch_size=batch_size)\n",
    "                        , steps_per_epoch=n_train//batch_size\n",
    "                        , epochs=epochs\n",
    "                        , validation_data=test_datagen.flow(x_test, y_test_onehot, batch_size=batch_size)\n",
    "                        , validation_steps=n_test//batch_size)\n",
    "    # decay lr by half\n",
    "    lr = lr / 2\n",
    "    k.set_value(sgd.lr, lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocess: featurewise + truncated_normal initialization + sgd_decay on tiny subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files (x86)\\Python35-64bit\\lib\\site-packages\\keras\\preprocessing\\image.py:648: UserWarning: Expected input to be images (as Numpy array) following the data format convention \"channels_last\" (channels on axis 3), i.e. expected either 1, 3 or 4 channels on axis 3. However, it was passed an array with shape (50, 48, 48, 1) (1 channels).\n",
      "  ' (' + str(x.shape[self.channel_axis]) + ' channels).')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "cropping2d_3 (Cropping2D)    (None, 42, 42, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 42, 42, 32)        832       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 21, 21, 32)        0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_3 (ZeroPaddin (None, 23, 23, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 20, 20, 32)        16416     \n",
      "_________________________________________________________________\n",
      "average_pooling2d_5 (Average (None, 10, 10, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 10, 10, 64)        51264     \n",
      "_________________________________________________________________\n",
      "average_pooling2d_6 (Average (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 3072)              4918272   \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 7)                 21511     \n",
      "=================================================================\n",
      "Total params: 5,008,295\n",
      "Trainable params: 5,008,295\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "epoch set: 0 lr value: 0.01\n",
      "Epoch 1/100\n",
      "3/3 [==============================] - 0s - loss: 1.4222 - acc: 0.3750 - val_loss: 0.7354 - val_acc: 0.6458\n",
      "Epoch 2/100\n",
      "3/3 [==============================] - 0s - loss: 1.1503 - acc: 0.6553 - val_loss: 1.0234 - val_acc: 0.3824\n",
      "Epoch 3/100\n",
      "3/3 [==============================] - 0s - loss: 0.8393 - acc: 0.6324 - val_loss: 0.6972 - val_acc: 0.6765\n",
      "Epoch 4/100\n",
      "3/3 [==============================] - 0s - loss: 0.7950 - acc: 0.4136 - val_loss: 0.9504 - val_acc: 0.4118\n",
      "Epoch 5/100\n",
      "3/3 [==============================] - 0s - loss: 0.7491 - acc: 0.5417 - val_loss: 0.6536 - val_acc: 0.7353\n",
      "Epoch 6/100\n",
      "3/3 [==============================] - 0s - loss: 0.5702 - acc: 0.7932 - val_loss: 0.6414 - val_acc: 0.6765\n",
      "Epoch 7/100\n",
      "3/3 [==============================] - 0s - loss: 0.5794 - acc: 0.6379 - val_loss: 0.5976 - val_acc: 0.7353\n",
      "Epoch 8/100\n",
      "3/3 [==============================] - 0s - loss: 0.3381 - acc: 0.8392 - val_loss: 0.8826 - val_acc: 0.6471\n",
      "Epoch 9/100\n",
      "3/3 [==============================] - 0s - loss: 0.4473 - acc: 0.7917 - val_loss: 1.0405 - val_acc: 0.5294\n",
      "Epoch 10/100\n",
      "3/3 [==============================] - 0s - loss: 0.2943 - acc: 0.8621 - val_loss: 0.9115 - val_acc: 0.6176\n",
      "Epoch 11/100\n",
      "3/3 [==============================] - 0s - loss: 0.7206 - acc: 0.7068 - val_loss: 1.2298 - val_acc: 0.5588\n",
      "Epoch 12/100\n",
      "3/3 [==============================] - 0s - loss: 0.8388 - acc: 0.5515 - val_loss: 1.0279 - val_acc: 0.5588\n",
      "Epoch 13/100\n",
      "3/3 [==============================] - 0s - loss: 0.5473 - acc: 0.6667 - val_loss: 0.6688 - val_acc: 0.6471\n",
      "Epoch 14/100\n",
      "3/3 [==============================] - 0s - loss: 0.4726 - acc: 0.7702 - val_loss: 0.8157 - val_acc: 0.5882\n",
      "Epoch 15/100\n",
      "3/3 [==============================] - 0s - loss: 0.4802 - acc: 0.6838 - val_loss: 1.0938 - val_acc: 0.4118\n",
      "Epoch 16/100\n",
      "3/3 [==============================] - 0s - loss: 0.4238 - acc: 0.8392 - val_loss: 0.8782 - val_acc: 0.6176\n",
      "Epoch 17/100\n",
      "3/3 [==============================] - 0s - loss: 0.3156 - acc: 0.8542 - val_loss: 1.3012 - val_acc: 0.5294\n",
      "Epoch 18/100\n",
      "3/3 [==============================] - 0s - loss: 0.3265 - acc: 0.8851 - val_loss: 1.4513 - val_acc: 0.6471\n",
      "Epoch 19/100\n",
      "3/3 [==============================] - 0s - loss: 0.5063 - acc: 0.6838 - val_loss: 0.8617 - val_acc: 0.6471\n",
      "Epoch 20/100\n",
      "3/3 [==============================] - 0s - loss: 0.4726 - acc: 0.7987 - val_loss: 1.2382 - val_acc: 0.4118\n",
      "Epoch 21/100\n",
      "3/3 [==============================] - 0s - loss: 0.5828 - acc: 0.7292 - val_loss: 0.9356 - val_acc: 0.6176\n",
      "Epoch 22/100\n",
      "3/3 [==============================] - 0s - loss: 0.4729 - acc: 0.7298 - val_loss: 1.0494 - val_acc: 0.5294\n",
      "Epoch 23/100\n",
      "3/3 [==============================] - 0s - loss: 0.5448 - acc: 0.7702 - val_loss: 0.8037 - val_acc: 0.6471\n",
      "Epoch 24/100\n",
      "3/3 [==============================] - 0s - loss: 0.2821 - acc: 0.8621 - val_loss: 0.8283 - val_acc: 0.6471\n",
      "Epoch 25/100\n",
      "3/3 [==============================] - 0s - loss: 0.3182 - acc: 0.8958 - val_loss: 1.0887 - val_acc: 0.6765\n",
      "Epoch 26/100\n",
      "3/3 [==============================] - 0s - loss: 0.1501 - acc: 0.9311 - val_loss: 1.6310 - val_acc: 0.5294\n",
      "Epoch 27/100\n",
      "3/3 [==============================] - 0s - loss: 0.4911 - acc: 0.7068 - val_loss: 1.5977 - val_acc: 0.5882\n",
      "Epoch 28/100\n",
      "3/3 [==============================] - 0s - loss: 0.2406 - acc: 0.9081 - val_loss: 1.7218 - val_acc: 0.5882\n",
      "Epoch 29/100\n",
      "3/3 [==============================] - 0s - loss: 0.1617 - acc: 0.9375 - val_loss: 1.2946 - val_acc: 0.6176\n",
      "Epoch 30/100\n",
      "3/3 [==============================] - 0s - loss: 0.2080 - acc: 0.9540 - val_loss: 1.6682 - val_acc: 0.5882\n",
      "Epoch 31/100\n",
      "3/3 [==============================] - 0s - loss: 0.0931 - acc: 0.9540 - val_loss: 1.4612 - val_acc: 0.6176\n",
      "Epoch 32/100\n",
      "3/3 [==============================] - 0s - loss: 0.2063 - acc: 0.8447 - val_loss: 3.4318 - val_acc: 0.5882\n",
      "Epoch 33/100\n",
      "3/3 [==============================] - 0s - loss: 1.5259 - acc: 0.7083 - val_loss: 0.8603 - val_acc: 0.6471\n",
      "Epoch 34/100\n",
      "3/3 [==============================] - 0s - loss: 1.2494 - acc: 0.5460 - val_loss: 1.0218 - val_acc: 0.5294\n",
      "Epoch 35/100\n",
      "3/3 [==============================] - 0s - loss: 0.5942 - acc: 0.7068 - val_loss: 1.5517 - val_acc: 0.3824\n",
      "Epoch 36/100\n",
      "3/3 [==============================] - 0s - loss: 0.7480 - acc: 0.6149 - val_loss: 1.2775 - val_acc: 0.5294\n",
      "Epoch 37/100\n",
      "3/3 [==============================] - 0s - loss: 0.5384 - acc: 0.7708 - val_loss: 1.0493 - val_acc: 0.5294\n",
      "Epoch 38/100\n",
      "3/3 [==============================] - 0s - loss: 0.8258 - acc: 0.6608 - val_loss: 0.9168 - val_acc: 0.5000\n",
      "Epoch 39/100\n",
      "3/3 [==============================] - 0s - loss: 0.3831 - acc: 0.8851 - val_loss: 1.0822 - val_acc: 0.5882\n",
      "Epoch 40/100\n",
      "3/3 [==============================] - 0s - loss: 0.5187 - acc: 0.8162 - val_loss: 0.9773 - val_acc: 0.5588\n",
      "Epoch 41/100\n",
      "3/3 [==============================] - 0s - loss: 0.1740 - acc: 0.9375 - val_loss: 1.2475 - val_acc: 0.5882\n",
      "Epoch 42/100\n",
      "3/3 [==============================] - 0s - loss: 0.0533 - acc: 1.0000 - val_loss: 1.5581 - val_acc: 0.6176\n",
      "Epoch 43/100\n",
      "3/3 [==============================] - 0s - loss: 0.0693 - acc: 0.9770 - val_loss: 1.9020 - val_acc: 0.5882\n",
      "Epoch 44/100\n",
      "3/3 [==============================] - 0s - loss: 0.1481 - acc: 0.9311 - val_loss: 2.4084 - val_acc: 0.5882\n",
      "Epoch 45/100\n",
      "3/3 [==============================] - 0s - loss: 0.0705 - acc: 0.9792 - val_loss: 1.8680 - val_acc: 0.7059\n",
      "Epoch 46/100\n",
      "3/3 [==============================] - 0s - loss: 0.0809 - acc: 0.9770 - val_loss: 2.4691 - val_acc: 0.7059\n",
      "Epoch 47/100\n",
      "3/3 [==============================] - 0s - loss: 0.1086 - acc: 0.9770 - val_loss: 1.7192 - val_acc: 0.6176\n",
      "Epoch 48/100\n",
      "3/3 [==============================] - 0s - loss: 0.1200 - acc: 0.9770 - val_loss: 3.6350 - val_acc: 0.5882\n",
      "Epoch 49/100\n",
      "3/3 [==============================] - 0s - loss: 0.2604 - acc: 0.9375 - val_loss: 1.7227 - val_acc: 0.5588\n",
      "Epoch 50/100\n",
      "3/3 [==============================] - 0s - loss: 0.3443 - acc: 0.8851 - val_loss: 3.0121 - val_acc: 0.5882\n",
      "Epoch 51/100\n",
      "3/3 [==============================] - 0s - loss: 0.1921 - acc: 0.9311 - val_loss: 2.6968 - val_acc: 0.5588\n",
      "Epoch 52/100\n",
      "3/3 [==============================] - 0s - loss: 0.0782 - acc: 0.9540 - val_loss: 1.7008 - val_acc: 0.6176\n",
      "Epoch 53/100\n",
      "3/3 [==============================] - 0s - loss: 0.0683 - acc: 0.9792 - val_loss: 2.3118 - val_acc: 0.6176\n",
      "Epoch 54/100\n",
      "3/3 [==============================] - 0s - loss: 0.0816 - acc: 0.9540 - val_loss: 2.6270 - val_acc: 0.6176\n",
      "Epoch 55/100\n",
      "3/3 [==============================] - 0s - loss: 0.0626 - acc: 1.0000 - val_loss: 2.4698 - val_acc: 0.6176\n",
      "Epoch 56/100\n",
      "3/3 [==============================] - 0s - loss: 0.1213 - acc: 0.9770 - val_loss: 3.6608 - val_acc: 0.5882\n",
      "Epoch 57/100\n",
      "3/3 [==============================] - 0s - loss: 0.0785 - acc: 0.9792 - val_loss: 3.0603 - val_acc: 0.5882\n",
      "Epoch 58/100\n",
      "3/3 [==============================] - 0s - loss: 0.1035 - acc: 0.9770 - val_loss: 4.0143 - val_acc: 0.5294\n",
      "Epoch 59/100\n",
      "3/3 [==============================] - 0s - loss: 0.1233 - acc: 0.9540 - val_loss: 3.4172 - val_acc: 0.7059\n",
      "Epoch 60/100\n",
      "3/3 [==============================] - 0s - loss: 0.0731 - acc: 0.9770 - val_loss: 3.9442 - val_acc: 0.5588\n",
      "Epoch 61/100\n",
      "3/3 [==============================] - 0s - loss: 0.0455 - acc: 0.9792 - val_loss: 3.7924 - val_acc: 0.4706\n",
      "Epoch 62/100\n",
      "3/3 [==============================] - 0s - loss: 1.9501 - acc: 0.5919 - val_loss: 3.9055 - val_acc: 0.6471\n",
      "Epoch 63/100\n",
      "3/3 [==============================] - 0s - loss: 2.9085 - acc: 0.5689 - val_loss: 5.1305 - val_acc: 0.6471\n",
      "Epoch 64/100\n",
      "3/3 [==============================] - 0s - loss: 3.0469 - acc: 0.7702 - val_loss: 1.9653 - val_acc: 0.5294\n",
      "Epoch 65/100\n",
      "3/3 [==============================] - 0s - loss: 0.9609 - acc: 0.6458 - val_loss: 1.4618 - val_acc: 0.6176\n",
      "Epoch 66/100\n",
      "3/3 [==============================] - 0s - loss: 4.8070 - acc: 0.4136 - val_loss: 0.6527 - val_acc: 0.6471\n",
      "Epoch 67/100\n",
      "3/3 [==============================] - 0s - loss: 1.0235 - acc: 0.4596 - val_loss: 1.0148 - val_acc: 0.5882\n",
      "Epoch 68/100\n",
      "3/3 [==============================] - 0s - loss: 0.9479 - acc: 0.5919 - val_loss: 0.7755 - val_acc: 0.6176\n",
      "Epoch 69/100\n",
      "3/3 [==============================] - 0s - loss: 0.7758 - acc: 0.6458 - val_loss: 1.0194 - val_acc: 0.5588\n",
      "Epoch 70/100\n",
      "3/3 [==============================] - 0s - loss: 0.8301 - acc: 0.5000 - val_loss: 0.7490 - val_acc: 0.4118\n",
      "Epoch 71/100\n",
      "3/3 [==============================] - 0s - loss: 0.5622 - acc: 0.7472 - val_loss: 0.6297 - val_acc: 0.6176\n",
      "Epoch 72/100\n",
      "3/3 [==============================] - 0s - loss: 0.4336 - acc: 0.8392 - val_loss: 1.5169 - val_acc: 0.5294\n",
      "Epoch 73/100\n",
      "3/3 [==============================] - 0s - loss: 0.6501 - acc: 0.6667 - val_loss: 0.7723 - val_acc: 0.6765\n",
      "Epoch 74/100\n",
      "3/3 [==============================] - 0s - loss: 0.3617 - acc: 0.8621 - val_loss: 0.6253 - val_acc: 0.7353\n",
      "Epoch 75/100\n",
      "3/3 [==============================] - 0s - loss: 0.4423 - acc: 0.8621 - val_loss: 0.5984 - val_acc: 0.7647\n",
      "Epoch 76/100\n",
      "3/3 [==============================] - 0s - loss: 0.3851 - acc: 0.8162 - val_loss: 0.5896 - val_acc: 0.7353\n",
      "Epoch 77/100\n",
      "3/3 [==============================] - 0s - loss: 0.2930 - acc: 0.8958 - val_loss: 1.1544 - val_acc: 0.5588\n",
      "Epoch 78/100\n",
      "3/3 [==============================] - 0s - loss: 0.4205 - acc: 0.8851 - val_loss: 2.1712 - val_acc: 0.4118\n",
      "Epoch 79/100\n",
      "3/3 [==============================] - 0s - loss: 0.1549 - acc: 0.9540 - val_loss: 1.8516 - val_acc: 0.4412\n",
      "Epoch 80/100\n",
      "3/3 [==============================] - 0s - loss: 0.7027 - acc: 0.7757 - val_loss: 1.7366 - val_acc: 0.5294\n",
      "Epoch 81/100\n",
      "3/3 [==============================] - 0s - loss: 0.2975 - acc: 0.8542 - val_loss: 1.6316 - val_acc: 0.5588\n",
      "Epoch 82/100\n",
      "3/3 [==============================] - 0s - loss: 0.1919 - acc: 0.9081 - val_loss: 1.2873 - val_acc: 0.5882\n",
      "Epoch 83/100\n",
      "3/3 [==============================] - 0s - loss: 0.2328 - acc: 0.9081 - val_loss: 1.1456 - val_acc: 0.6176\n",
      "Epoch 84/100\n",
      "3/3 [==============================] - 0s - loss: 0.1264 - acc: 0.9770 - val_loss: 1.0001 - val_acc: 0.7647\n",
      "Epoch 85/100\n",
      "3/3 [==============================] - 0s - loss: 0.2366 - acc: 0.9167 - val_loss: 1.9097 - val_acc: 0.5882\n",
      "Epoch 86/100\n",
      "3/3 [==============================] - 0s - loss: 0.0821 - acc: 0.9770 - val_loss: 2.0514 - val_acc: 0.5882\n",
      "Epoch 87/100\n",
      "3/3 [==============================] - 0s - loss: 0.0592 - acc: 0.9770 - val_loss: 1.9104 - val_acc: 0.5882\n",
      "Epoch 88/100\n",
      "3/3 [==============================] - 0s - loss: 0.0801 - acc: 1.0000 - val_loss: 2.5795 - val_acc: 0.5882\n",
      "Epoch 89/100\n",
      "3/3 [==============================] - 0s - loss: 0.0781 - acc: 0.9583 - val_loss: 2.4134 - val_acc: 0.5294\n",
      "Epoch 90/100\n",
      "3/3 [==============================] - 0s - loss: 0.0628 - acc: 1.0000 - val_loss: 1.8376 - val_acc: 0.7059\n",
      "Epoch 91/100\n",
      "3/3 [==============================] - 0s - loss: 0.0476 - acc: 0.9770 - val_loss: 1.2373 - val_acc: 0.6176\n",
      "Epoch 92/100\n",
      "3/3 [==============================] - 0s - loss: 1.1181 - acc: 0.7987 - val_loss: 4.0066 - val_acc: 0.4412\n",
      "Epoch 93/100\n",
      "3/3 [==============================] - 0s - loss: 0.8086 - acc: 0.7917 - val_loss: 0.7072 - val_acc: 0.6765\n",
      "Epoch 94/100\n",
      "3/3 [==============================] - 0s - loss: 0.6384 - acc: 0.5515 - val_loss: 0.7992 - val_acc: 0.6176\n",
      "Epoch 95/100\n",
      "3/3 [==============================] - 0s - loss: 0.4292 - acc: 0.7298 - val_loss: 0.9324 - val_acc: 0.6471\n",
      "Epoch 96/100\n",
      "3/3 [==============================] - 0s - loss: 0.3057 - acc: 0.8392 - val_loss: 1.2101 - val_acc: 0.5882\n",
      "Epoch 97/100\n",
      "3/3 [==============================] - 0s - loss: 0.3190 - acc: 0.8750 - val_loss: 1.3887 - val_acc: 0.5417\n",
      "Epoch 98/100\n",
      "3/3 [==============================] - 0s - loss: 0.2123 - acc: 0.9311 - val_loss: 1.7931 - val_acc: 0.6250\n",
      "Epoch 99/100\n",
      "3/3 [==============================] - 0s - loss: 0.4231 - acc: 0.7987 - val_loss: 2.3497 - val_acc: 0.5294\n",
      "Epoch 100/100\n",
      "3/3 [==============================] - 0s - loss: 0.5694 - acc: 0.7757 - val_loss: 3.3256 - val_acc: 0.5882\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1bbd835d208>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAEKCAYAAAARnO4WAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n+MYY4xEW8I0xxiMs4BtjjEf8PyCvKCLUB8+PAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1bbd8cf3f60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# preprocess\n",
    "rescale_factor = 255\n",
    "train_datagen = ImageDataGenerator(featurewise_center=True, featurewise_std_normalization=True)\n",
    "train_datagen.fit(x_train)\n",
    "\n",
    "test_datagen = ImageDataGenerator(featurewise_center=True, featurewise_std_normalization=True)\n",
    "test_datagen.fit(x_train)\n",
    "\n",
    "# build model\n",
    "model_name = '1st_place'\n",
    "model = Sequential(name=model_name)\n",
    "model.add(Cropping2D(cropping=3, input_shape=image_shape))\n",
    "model.add(Conv2D(32, (5,5), padding='same', activation='relu', kernel_initializer='TruncatedNormal'))\n",
    "model.add(MaxPooling2D((2,2), strides=(2,2), padding='valid'))\n",
    "\n",
    "model.add(ZeroPadding2D(padding=1))\n",
    "model.add(Conv2D(32, (4,4), padding='valid', activation='relu', kernel_initializer='TruncatedNormal'))\n",
    "model.add(AveragePooling2D((2,2), strides=(2,2), padding='valid'))\n",
    "\n",
    "model.add(Conv2D(64, (5,5), padding='same', activation='relu', kernel_initializer='TruncatedNormal'))\n",
    "model.add(AveragePooling2D((2,2), strides=(2,2), padding='valid'))\n",
    "\n",
    "model.add(Flatten())\n",
    "# TODO is this where dropout should be?\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(3072, activation='relu', kernel_initializer='TruncatedNormal'))\n",
    "# TODO svm activation function\n",
    "model.add(Dense(n_classes, activation='softmax', kernel_initializer='TruncatedNormal'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# compile model\n",
    "sgd = optimizers.SGD(momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "# fit model\n",
    "epoch_sets = 1\n",
    "epochs = 100\n",
    "batch_size = 16\n",
    "for es in range(epoch_sets):\n",
    "    # get current lr\n",
    "    lr = k.get_value(sgd.lr)\n",
    "    print('epoch set:', es, 'lr value:', lr)\n",
    "    \n",
    "    # fit model\n",
    "    history = model.fit_generator(train_datagen.flow(x_train, y_train_onehot, batch_size=batch_size)\n",
    "                        , steps_per_epoch=n_train//batch_size\n",
    "                        , epochs=epochs\n",
    "                        , validation_data=test_datagen.flow(x_test, y_test_onehot, batch_size=batch_size)\n",
    "                        , validation_steps=n_test//batch_size)\n",
    "    # decay lr by half\n",
    "    lr = lr / 2\n",
    "    k.set_value(sgd.lr, lr)\n",
    "    \n",
    "# visualize loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocess: featurewise + truncated_normal initialization + sgd_decay on tiny subset + batchsize=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files (x86)\\Python35-64bit\\lib\\site-packages\\keras\\preprocessing\\image.py:648: UserWarning: Expected input to be images (as Numpy array) following the data format convention \"channels_last\" (channels on axis 3), i.e. expected either 1, 3 or 4 channels on axis 3. However, it was passed an array with shape (50, 48, 48, 1) (1 channels).\n",
      "  ' (' + str(x.shape[self.channel_axis]) + ' channels).')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "cropping2d_4 (Cropping2D)    (None, 42, 42, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 42, 42, 32)        832       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 21, 21, 32)        0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_4 (ZeroPaddin (None, 23, 23, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 20, 20, 32)        16416     \n",
      "_________________________________________________________________\n",
      "average_pooling2d_7 (Average (None, 10, 10, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 10, 10, 64)        51264     \n",
      "_________________________________________________________________\n",
      "average_pooling2d_8 (Average (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 3072)              4918272   \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 7)                 21511     \n",
      "=================================================================\n",
      "Total params: 5,008,295\n",
      "Trainable params: 5,008,295\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "epoch set: 0 lr value: 0.01\n",
      "Epoch 1/100\n",
      "1/1 [==============================] - 0s - loss: 2.0784 - acc: 0.1200 - val_loss: 1.5254 - val_acc: 0.6400\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - 0s - loss: 1.5697 - acc: 0.6000 - val_loss: 1.0824 - val_acc: 0.6400\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - 0s - loss: 1.1343 - acc: 0.6000 - val_loss: 0.8031 - val_acc: 0.6400\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - 0s - loss: 0.8287 - acc: 0.6000 - val_loss: 0.7673 - val_acc: 0.4000\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - 0s - loss: 0.6713 - acc: 0.6000 - val_loss: 0.6539 - val_acc: 0.6400\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - 0s - loss: 0.6774 - acc: 0.6000 - val_loss: 1.0568 - val_acc: 0.3600\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - 0s - loss: 1.0020 - acc: 0.4000 - val_loss: 1.8830 - val_acc: 0.6400\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - 0s - loss: 2.4190 - acc: 0.6000 - val_loss: 0.6919 - val_acc: 0.7400\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - 0s - loss: 0.6521 - acc: 0.7600 - val_loss: 0.7472 - val_acc: 0.5600\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - 0s - loss: 0.7032 - acc: 0.7200 - val_loss: 0.7478 - val_acc: 0.6200\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - 0s - loss: 0.7171 - acc: 0.7200 - val_loss: 0.7055 - val_acc: 0.7600\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - 0s - loss: 0.6868 - acc: 0.7600 - val_loss: 0.6604 - val_acc: 0.6600\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - 0s - loss: 0.6231 - acc: 0.7000 - val_loss: 0.6296 - val_acc: 0.6400\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - 0s - loss: 0.5957 - acc: 0.7800 - val_loss: 0.6123 - val_acc: 0.6400\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - 0s - loss: 0.5707 - acc: 0.7000 - val_loss: 0.6009 - val_acc: 0.6600\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - 0s - loss: 0.5276 - acc: 0.7000 - val_loss: 0.5953 - val_acc: 0.7600\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - 0s - loss: 0.5074 - acc: 0.8200 - val_loss: 0.5943 - val_acc: 0.7600\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - 0s - loss: 0.5069 - acc: 0.7800 - val_loss: 0.5973 - val_acc: 0.7600\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - 0s - loss: 0.4610 - acc: 0.8400 - val_loss: 0.6182 - val_acc: 0.7200\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - 0s - loss: 0.4589 - acc: 0.8200 - val_loss: 0.6500 - val_acc: 0.6400\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - 0s - loss: 0.3987 - acc: 0.8400 - val_loss: 0.6734 - val_acc: 0.7200\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - 0s - loss: 0.4369 - acc: 0.7600 - val_loss: 0.6844 - val_acc: 0.6600\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - 0s - loss: 0.3374 - acc: 0.8600 - val_loss: 0.7496 - val_acc: 0.7000\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - 0s - loss: 0.3828 - acc: 0.7800 - val_loss: 0.9463 - val_acc: 0.6200\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - 0s - loss: 0.5140 - acc: 0.7400 - val_loss: 1.4669 - val_acc: 0.6400\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - 0s - loss: 1.0695 - acc: 0.6400 - val_loss: 1.1471 - val_acc: 0.4800\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - 0s - loss: 0.5949 - acc: 0.6200 - val_loss: 0.8756 - val_acc: 0.6400\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - 0s - loss: 0.3597 - acc: 0.8000 - val_loss: 0.7791 - val_acc: 0.6400\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - 0s - loss: 0.2843 - acc: 0.8800 - val_loss: 0.8080 - val_acc: 0.6200\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - 0s - loss: 0.2903 - acc: 0.8800 - val_loss: 0.8269 - val_acc: 0.5800\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - 0s - loss: 0.2933 - acc: 0.8800 - val_loss: 0.8527 - val_acc: 0.6200\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - 0s - loss: 0.2231 - acc: 0.9200 - val_loss: 0.8602 - val_acc: 0.6000\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - 0s - loss: 0.2562 - acc: 0.9400 - val_loss: 0.8880 - val_acc: 0.6200\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - 0s - loss: 0.2326 - acc: 0.9200 - val_loss: 0.9004 - val_acc: 0.6000\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - 0s - loss: 0.2035 - acc: 0.9000 - val_loss: 0.9134 - val_acc: 0.6000\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - 0s - loss: 0.2007 - acc: 0.9000 - val_loss: 0.9276 - val_acc: 0.6000\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - 0s - loss: 0.1481 - acc: 0.9800 - val_loss: 0.9568 - val_acc: 0.6200\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - 0s - loss: 0.1761 - acc: 0.9600 - val_loss: 0.9969 - val_acc: 0.6000\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - 0s - loss: 0.1653 - acc: 0.9600 - val_loss: 1.0440 - val_acc: 0.6000\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - 0s - loss: 0.2053 - acc: 0.9400 - val_loss: 1.1027 - val_acc: 0.6000\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - 0s - loss: 0.1024 - acc: 0.9800 - val_loss: 1.1687 - val_acc: 0.6000\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - 0s - loss: 0.1146 - acc: 0.9800 - val_loss: 1.2481 - val_acc: 0.6000\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - 0s - loss: 0.1227 - acc: 0.9800 - val_loss: 1.2841 - val_acc: 0.6000\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - 0s - loss: 0.0897 - acc: 0.9800 - val_loss: 1.3394 - val_acc: 0.6000\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - 0s - loss: 0.1137 - acc: 0.9600 - val_loss: 1.3888 - val_acc: 0.6000\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - 0s - loss: 0.0731 - acc: 1.0000 - val_loss: 1.4802 - val_acc: 0.6200\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - 0s - loss: 0.1337 - acc: 0.9600 - val_loss: 1.5249 - val_acc: 0.5200\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - 0s - loss: 0.1254 - acc: 0.9400 - val_loss: 1.4994 - val_acc: 0.6400\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - 0s - loss: 0.0813 - acc: 0.9800 - val_loss: 1.5203 - val_acc: 0.6400\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - 0s - loss: 0.0869 - acc: 0.9800 - val_loss: 1.7158 - val_acc: 0.6600\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - 0s - loss: 0.0810 - acc: 0.9600 - val_loss: 1.6997 - val_acc: 0.6000\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - 0s - loss: 0.0788 - acc: 0.9600 - val_loss: 1.7078 - val_acc: 0.6200\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - 0s - loss: 0.0814 - acc: 0.9600 - val_loss: 1.6953 - val_acc: 0.6600\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - 0s - loss: 0.0381 - acc: 1.0000 - val_loss: 1.6649 - val_acc: 0.6800\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - 0s - loss: 0.0398 - acc: 1.0000 - val_loss: 1.6583 - val_acc: 0.6400\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - 0s - loss: 0.0409 - acc: 0.9800 - val_loss: 1.6510 - val_acc: 0.6200\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - 0s - loss: 0.0338 - acc: 1.0000 - val_loss: 1.6347 - val_acc: 0.6600\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - 0s - loss: 0.0309 - acc: 1.0000 - val_loss: 1.6691 - val_acc: 0.6400\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - 0s - loss: 0.0470 - acc: 0.9800 - val_loss: 1.6764 - val_acc: 0.6600\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - 0s - loss: 0.0254 - acc: 1.0000 - val_loss: 1.6810 - val_acc: 0.6400\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - 0s - loss: 0.0232 - acc: 0.9800 - val_loss: 1.7226 - val_acc: 0.6400\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - 0s - loss: 0.0519 - acc: 0.9600 - val_loss: 1.7376 - val_acc: 0.6600\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - 0s - loss: 0.0284 - acc: 1.0000 - val_loss: 1.7154 - val_acc: 0.6600\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - 0s - loss: 0.0109 - acc: 1.0000 - val_loss: 1.7275 - val_acc: 0.6600\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - 0s - loss: 0.0219 - acc: 1.0000 - val_loss: 1.7431 - val_acc: 0.6800\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - 0s - loss: 0.0117 - acc: 1.0000 - val_loss: 1.7649 - val_acc: 0.6800\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - 0s - loss: 0.0216 - acc: 1.0000 - val_loss: 1.7580 - val_acc: 0.6800\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - 0s - loss: 0.0090 - acc: 1.0000 - val_loss: 1.7575 - val_acc: 0.6600\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - 0s - loss: 0.0170 - acc: 1.0000 - val_loss: 1.7420 - val_acc: 0.6800\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - 0s - loss: 0.0400 - acc: 0.9600 - val_loss: 1.7241 - val_acc: 0.7000\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - 0s - loss: 0.0125 - acc: 1.0000 - val_loss: 1.7222 - val_acc: 0.6800\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - 0s - loss: 0.0342 - acc: 1.0000 - val_loss: 1.7978 - val_acc: 0.6400\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - 0s - loss: 0.0494 - acc: 0.9800 - val_loss: 1.7792 - val_acc: 0.6600\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - 0s - loss: 0.0224 - acc: 0.9800 - val_loss: 1.8297 - val_acc: 0.6600\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - 0s - loss: 0.0269 - acc: 0.9800 - val_loss: 1.9050 - val_acc: 0.6600\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - 0s - loss: 0.0178 - acc: 1.0000 - val_loss: 1.9909 - val_acc: 0.6600\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - 0s - loss: 0.0127 - acc: 1.0000 - val_loss: 2.0438 - val_acc: 0.6600\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - 0s - loss: 0.0158 - acc: 1.0000 - val_loss: 2.0340 - val_acc: 0.6600\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - 0s - loss: 0.0134 - acc: 1.0000 - val_loss: 2.0103 - val_acc: 0.6600\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - 0s - loss: 0.0061 - acc: 1.0000 - val_loss: 2.0183 - val_acc: 0.6600\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - 0s - loss: 0.0210 - acc: 0.9800 - val_loss: 2.0417 - val_acc: 0.6600\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - 0s - loss: 0.0196 - acc: 1.0000 - val_loss: 2.0587 - val_acc: 0.6600\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - 0s - loss: 0.0097 - acc: 1.0000 - val_loss: 2.0905 - val_acc: 0.6600\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - 0s - loss: 0.0153 - acc: 1.0000 - val_loss: 2.1296 - val_acc: 0.6400\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - 0s - loss: 0.0099 - acc: 1.0000 - val_loss: 2.1463 - val_acc: 0.6400\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - 0s - loss: 0.0104 - acc: 1.0000 - val_loss: 2.1783 - val_acc: 0.6400\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - 0s - loss: 0.0033 - acc: 1.0000 - val_loss: 2.2040 - val_acc: 0.6400\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - 0s - loss: 0.0060 - acc: 1.0000 - val_loss: 2.2046 - val_acc: 0.6600\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - 0s - loss: 0.0070 - acc: 1.0000 - val_loss: 2.2034 - val_acc: 0.6400\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - 0s - loss: 0.0036 - acc: 1.0000 - val_loss: 2.2115 - val_acc: 0.6400\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - 0s - loss: 0.0063 - acc: 1.0000 - val_loss: 2.2135 - val_acc: 0.6400\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - 0s - loss: 0.0091 - acc: 1.0000 - val_loss: 2.2212 - val_acc: 0.6400\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - 0s - loss: 0.0057 - acc: 1.0000 - val_loss: 2.2151 - val_acc: 0.6600\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - 0s - loss: 0.0049 - acc: 1.0000 - val_loss: 2.2068 - val_acc: 0.6800\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - 0s - loss: 0.0038 - acc: 1.0000 - val_loss: 2.1968 - val_acc: 0.6800\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - 0s - loss: 0.0033 - acc: 1.0000 - val_loss: 2.1900 - val_acc: 0.6800\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - 0s - loss: 0.0019 - acc: 1.0000 - val_loss: 2.1874 - val_acc: 0.6800\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - 0s - loss: 0.0029 - acc: 1.0000 - val_loss: 2.1862 - val_acc: 0.6800\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - 0s - loss: 0.0252 - acc: 0.9800 - val_loss: 2.1387 - val_acc: 0.6200\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - 0s - loss: 0.0186 - acc: 0.9800 - val_loss: 2.0423 - val_acc: 0.6600\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1bbe27132b0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nESZBQQghRNj/BzNXoOMz1Zi8AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1bbddd4fd30>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# preprocess\n",
    "rescale_factor = 255\n",
    "train_datagen = ImageDataGenerator(featurewise_center=True, featurewise_std_normalization=True)\n",
    "train_datagen.fit(x_train)\n",
    "\n",
    "test_datagen = ImageDataGenerator(featurewise_center=True, featurewise_std_normalization=True)\n",
    "test_datagen.fit(x_train)\n",
    "\n",
    "# build model\n",
    "model_name = '1st_place'\n",
    "model = Sequential(name=model_name)\n",
    "model.add(Cropping2D(cropping=3, input_shape=image_shape))\n",
    "model.add(Conv2D(32, (5,5), padding='same', activation='relu', kernel_initializer='TruncatedNormal'))\n",
    "model.add(MaxPooling2D((2,2), strides=(2,2), padding='valid'))\n",
    "\n",
    "model.add(ZeroPadding2D(padding=1))\n",
    "model.add(Conv2D(32, (4,4), padding='valid', activation='relu', kernel_initializer='TruncatedNormal'))\n",
    "model.add(AveragePooling2D((2,2), strides=(2,2), padding='valid'))\n",
    "\n",
    "model.add(Conv2D(64, (5,5), padding='same', activation='relu', kernel_initializer='TruncatedNormal'))\n",
    "model.add(AveragePooling2D((2,2), strides=(2,2), padding='valid'))\n",
    "\n",
    "model.add(Flatten())\n",
    "# TODO is this where dropout should be?\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(3072, activation='relu', kernel_initializer='TruncatedNormal'))\n",
    "# TODO svm activation function\n",
    "model.add(Dense(n_classes, activation='softmax', kernel_initializer='TruncatedNormal'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# compile model\n",
    "sgd = optimizers.SGD(momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "# fit model\n",
    "epoch_sets = 1\n",
    "epochs = 100\n",
    "batch_size = 50\n",
    "for es in range(epoch_sets):\n",
    "    # get current lr\n",
    "    lr = k.get_value(sgd.lr)\n",
    "    print('epoch set:', es, 'lr value:', lr)\n",
    "    \n",
    "    # fit model\n",
    "    history = model.fit_generator(train_datagen.flow(x_train, y_train_onehot, batch_size=batch_size)\n",
    "                        , steps_per_epoch=n_train//batch_size\n",
    "                        , epochs=epochs\n",
    "                        , validation_data=test_datagen.flow(x_test, y_test_onehot, batch_size=batch_size)\n",
    "                        , validation_steps=n_test//batch_size)\n",
    "    # decay lr by half\n",
    "    lr = lr / 2\n",
    "    k.set_value(sgd.lr, lr)\n",
    "    \n",
    "# visualize loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocess: none + sgd_decay on tiny subset + batchsize=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "cropping2d_5 (Cropping2D)    (None, 42, 42, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 42, 42, 32)        832       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 21, 21, 32)        0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_5 (ZeroPaddin (None, 23, 23, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 20, 20, 32)        16416     \n",
      "_________________________________________________________________\n",
      "average_pooling2d_9 (Average (None, 10, 10, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 10, 10, 64)        51264     \n",
      "_________________________________________________________________\n",
      "average_pooling2d_10 (Averag (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 3072)              4918272   \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 7)                 21511     \n",
      "=================================================================\n",
      "Total params: 5,008,295\n",
      "Trainable params: 5,008,295\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "epoch set: 0 lr value: 0.01\n",
      "Epoch 1/100\n",
      "1/1 [==============================] - 0s - loss: 11.0651 - acc: 0.2400 - val_loss: 10.3156 - val_acc: 0.3600\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - 0s - loss: 9.6709 - acc: 0.4000 - val_loss: 10.3156 - val_acc: 0.3600\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - 0s - loss: 9.6709 - acc: 0.4000 - val_loss: 10.3156 - val_acc: 0.3600\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - 0s - loss: 9.6709 - acc: 0.4000 - val_loss: 10.3156 - val_acc: 0.3600\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - 0s - loss: 9.6709 - acc: 0.4000 - val_loss: 10.3156 - val_acc: 0.3600\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - 0s - loss: 9.6709 - acc: 0.4000 - val_loss: 10.3156 - val_acc: 0.3600\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - 0s - loss: 9.6709 - acc: 0.4000 - val_loss: 10.3156 - val_acc: 0.3600\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - 0s - loss: 9.6709 - acc: 0.4000 - val_loss: 10.3156 - val_acc: 0.3600\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - 0s - loss: 9.6709 - acc: 0.4000 - val_loss: 10.3156 - val_acc: 0.3600\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - 0s - loss: 9.6709 - acc: 0.4000 - val_loss: 10.3156 - val_acc: 0.3600\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - 0s - loss: 9.6709 - acc: 0.4000 - val_loss: 10.3156 - val_acc: 0.3600\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - 0s - loss: 9.6709 - acc: 0.4000 - val_loss: 10.3156 - val_acc: 0.3600\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - 0s - loss: 9.6709 - acc: 0.4000 - val_loss: 10.3156 - val_acc: 0.3600\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - 0s - loss: 9.6709 - acc: 0.4000 - val_loss: 10.3156 - val_acc: 0.3600\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - 0s - loss: 9.6709 - acc: 0.4000 - val_loss: 10.3156 - val_acc: 0.3600\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - 0s - loss: 9.6709 - acc: 0.4000 - val_loss: 10.3156 - val_acc: 0.3600\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - 0s - loss: 9.6709 - acc: 0.4000 - val_loss: 10.3156 - val_acc: 0.3600\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - 0s - loss: 9.6709 - acc: 0.4000 - val_loss: 10.3156 - val_acc: 0.3600\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - 0s - loss: 9.6709 - acc: 0.4000 - val_loss: 10.3156 - val_acc: 0.3600\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - 0s - loss: 9.6709 - acc: 0.4000 - val_loss: 10.3156 - val_acc: 0.3600\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - 0s - loss: 9.6709 - acc: 0.4000 - val_loss: 10.3156 - val_acc: 0.3600\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - 0s - loss: 9.6709 - acc: 0.4000 - val_loss: 10.3156 - val_acc: 0.3600\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - 0s - loss: 9.6709 - acc: 0.4000 - val_loss: 10.3156 - val_acc: 0.3600\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - 0s - loss: 9.6709 - acc: 0.4000 - val_loss: 10.3156 - val_acc: 0.3600\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - 0s - loss: 9.6709 - acc: 0.4000 - val_loss: 10.3156 - val_acc: 0.3600\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - 0s - loss: 9.6709 - acc: 0.4000 - val_loss: 10.3156 - val_acc: 0.3600\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - 0s - loss: 9.6709 - acc: 0.4000 - val_loss: 10.3156 - val_acc: 0.3600\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - 0s - loss: 9.6709 - acc: 0.4000 - val_loss: 10.3156 - val_acc: 0.3600\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - 0s - loss: 9.6709 - acc: 0.4000 - val_loss: 10.3156 - val_acc: 0.3600\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - 0s - loss: 9.6709 - acc: 0.4000 - val_loss: 10.3156 - val_acc: 0.3600\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - 0s - loss: 9.6709 - acc: 0.4000 - val_loss: 10.3156 - val_acc: 0.3600\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - 0s - loss: 9.6709 - acc: 0.4000 - val_loss: 10.3156 - val_acc: 0.3600\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - 0s - loss: 9.6709 - acc: 0.4000 - val_loss: 10.3156 - val_acc: 0.3600\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - 0s - loss: 9.6709 - acc: 0.4000 - val_loss: 10.3156 - val_acc: 0.3600\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - 0s - loss: 9.6709 - acc: 0.4000 - val_loss: 10.3156 - val_acc: 0.3600\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - 0s - loss: 9.6709 - acc: 0.4000 - val_loss: 10.3156 - val_acc: 0.3600\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - 0s - loss: 9.6709 - acc: 0.4000 - val_loss: 10.3156 - val_acc: 0.3600\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - 0s - loss: 9.6709 - acc: 0.4000 - val_loss: 10.3156 - val_acc: 0.3600\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - 0s - loss: 9.6709 - acc: 0.4000 - val_loss: 10.3156 - val_acc: 0.3600\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - 0s - loss: 9.6709 - acc: 0.4000 - val_loss: 10.3156 - val_acc: 0.3600\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - 0s - loss: 9.6709 - acc: 0.4000 - val_loss: 10.3156 - val_acc: 0.3600\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - 0s - loss: 9.6709 - acc: 0.4000 - val_loss: 10.3156 - val_acc: 0.3600\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - 0s - loss: 9.6709 - acc: 0.4000 - val_loss: 10.3156 - val_acc: 0.3600\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - 0s - loss: 9.6709 - acc: 0.4000 - val_loss: 10.3156 - val_acc: 0.3600\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - 0s - loss: 9.6709 - acc: 0.4000 - val_loss: 10.3156 - val_acc: 0.3600\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - 0s - loss: 9.6709 - acc: 0.4000 - val_loss: 10.3156 - val_acc: 0.3600\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - 0s - loss: 9.6709 - acc: 0.4000 - val_loss: 10.3156 - val_acc: 0.3600\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - 0s - loss: 9.6709 - acc: 0.4000 - val_loss: 10.3156 - val_acc: 0.3600\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - 0s - loss: 9.6709 - acc: 0.4000 - val_loss: 10.3156 - val_acc: 0.3600\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - 0s - loss: 9.6709 - acc: 0.4000 - val_loss: 10.3156 - val_acc: 0.3600\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - 0s - loss: 9.6709 - acc: 0.4000 - val_loss: 10.3156 - val_acc: 0.3600\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - 0s - loss: 9.6709 - acc: 0.4000 - val_loss: 10.3156 - val_acc: 0.3600\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - 0s - loss: 9.6709 - acc: 0.4000 - val_loss: 10.3156 - val_acc: 0.3600\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - 0s - loss: 9.6709 - acc: 0.4000 - val_loss: 10.3156 - val_acc: 0.3600\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - 0s - loss: 9.6709 - acc: 0.4000 - val_loss: 10.3156 - val_acc: 0.3600\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - 0s - loss: 9.6709 - acc: 0.4000 - val_loss: 10.3156 - val_acc: 0.3600\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - 0s - loss: 9.6709 - acc: 0.4000 - val_loss: 10.3156 - val_acc: 0.3600\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - 0s - loss: 9.6709 - acc: 0.4000 - val_loss: 10.3156 - val_acc: 0.3600\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - 0s - loss: 9.6709 - acc: 0.4000 - val_loss: 10.3156 - val_acc: 0.3600\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - 0s - loss: 9.6709 - acc: 0.4000 - val_loss: 10.3156 - val_acc: 0.3600\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - 0s - loss: 9.6709 - acc: 0.4000 - val_loss: 10.3156 - val_acc: 0.3600\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - 0s - loss: 9.6709 - acc: 0.4000 - val_loss: 10.3156 - val_acc: 0.3600\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - 0s - loss: 9.6709 - acc: 0.4000 - val_loss: 10.3156 - val_acc: 0.3600\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - 0s - loss: 9.6709 - acc: 0.4000 - val_loss: 10.3156 - val_acc: 0.3600\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - 0s - loss: 9.6709 - acc: 0.4000 - val_loss: 10.3156 - val_acc: 0.3600\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - 0s - loss: 9.6709 - acc: 0.4000 - val_loss: 10.3156 - val_acc: 0.3600\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - 0s - loss: 9.6709 - acc: 0.4000 - val_loss: 10.3156 - val_acc: 0.3600\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - 0s - loss: 9.6709 - acc: 0.4000 - val_loss: 10.3156 - val_acc: 0.3600\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - 0s - loss: 9.6709 - acc: 0.4000 - val_loss: 10.3156 - val_acc: 0.3600\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - 0s - loss: 9.6709 - acc: 0.4000 - val_loss: 10.3156 - val_acc: 0.3600\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - 0s - loss: 9.6709 - acc: 0.4000 - val_loss: 10.3156 - val_acc: 0.3600\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - 0s - loss: 9.6709 - acc: 0.4000 - val_loss: 10.3156 - val_acc: 0.3600\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - 0s - loss: 9.6709 - acc: 0.4000 - val_loss: 10.3156 - val_acc: 0.3600\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - 0s - loss: 9.6709 - acc: 0.4000 - val_loss: 10.3156 - val_acc: 0.3600\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - 0s - loss: 9.6709 - acc: 0.4000 - val_loss: 10.3156 - val_acc: 0.3600\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - 0s - loss: 9.6709 - acc: 0.4000 - val_loss: 10.3156 - val_acc: 0.3600\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - 0s - loss: 9.6709 - acc: 0.4000 - val_loss: 10.3156 - val_acc: 0.3600\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - 0s - loss: 9.6709 - acc: 0.4000 - val_loss: 10.3156 - val_acc: 0.3600\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - 0s - loss: 9.6709 - acc: 0.4000 - val_loss: 10.3156 - val_acc: 0.3600\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - 0s - loss: 9.6709 - acc: 0.4000 - val_loss: 10.3156 - val_acc: 0.3600\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - 0s - loss: 9.6709 - acc: 0.4000 - val_loss: 10.3156 - val_acc: 0.3600\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - 0s - loss: 9.6709 - acc: 0.4000 - val_loss: 10.3156 - val_acc: 0.3600\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - 0s - loss: 9.6709 - acc: 0.4000 - val_loss: 10.3156 - val_acc: 0.3600\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - 0s - loss: 9.6709 - acc: 0.4000 - val_loss: 10.3156 - val_acc: 0.3600\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - 0s - loss: 9.6709 - acc: 0.4000 - val_loss: 10.3156 - val_acc: 0.3600\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - 0s - loss: 9.6709 - acc: 0.4000 - val_loss: 10.3156 - val_acc: 0.3600\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - 0s - loss: 9.6709 - acc: 0.4000 - val_loss: 10.3156 - val_acc: 0.3600\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - 0s - loss: 9.6709 - acc: 0.4000 - val_loss: 10.3156 - val_acc: 0.3600\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - 0s - loss: 9.6709 - acc: 0.4000 - val_loss: 10.3156 - val_acc: 0.3600\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - 0s - loss: 9.6709 - acc: 0.4000 - val_loss: 10.3156 - val_acc: 0.3600\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - 0s - loss: 9.6709 - acc: 0.4000 - val_loss: 10.3156 - val_acc: 0.3600\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - 0s - loss: 9.6709 - acc: 0.4000 - val_loss: 10.3156 - val_acc: 0.3600\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - 0s - loss: 9.6709 - acc: 0.4000 - val_loss: 10.3156 - val_acc: 0.3600\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - 0s - loss: 9.6709 - acc: 0.4000 - val_loss: 10.3156 - val_acc: 0.3600\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - 0s - loss: 9.6709 - acc: 0.4000 - val_loss: 10.3156 - val_acc: 0.3600\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - 0s - loss: 9.6709 - acc: 0.4000 - val_loss: 10.3156 - val_acc: 0.3600\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - 0s - loss: 9.6709 - acc: 0.4000 - val_loss: 10.3156 - val_acc: 0.3600\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - 0s - loss: 9.6709 - acc: 0.4000 - val_loss: 10.3156 - val_acc: 0.3600\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - 0s - loss: 9.6709 - acc: 0.4000 - val_loss: 10.3156 - val_acc: 0.3600\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - 0s - loss: 9.6709 - acc: 0.4000 - val_loss: 10.3156 - val_acc: 0.3600\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1bbe743ffd0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEKCAYAAADjDHn2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nMzOry2FhZmZ1OSzMzKwuh4WZmdX1/wHST24vbgZA6AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1bbe27882b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# preprocess\n",
    "rescale_factor = 255\n",
    "train_datagen = ImageDataGenerator()\n",
    "# train_datagen.fit(x_train)\n",
    "\n",
    "test_datagen = ImageDataGenerator()\n",
    "# test_datagen.fit(x_train)\n",
    "\n",
    "# build model\n",
    "model_name = '1st_place'\n",
    "model = Sequential(name=model_name)\n",
    "model.add(Cropping2D(cropping=3, input_shape=image_shape))\n",
    "model.add(Conv2D(32, (5,5), padding='same', activation='relu', kernel_initializer='TruncatedNormal'))\n",
    "model.add(MaxPooling2D((2,2), strides=(2,2), padding='valid'))\n",
    "\n",
    "model.add(ZeroPadding2D(padding=1))\n",
    "model.add(Conv2D(32, (4,4), padding='valid', activation='relu', kernel_initializer='TruncatedNormal'))\n",
    "model.add(AveragePooling2D((2,2), strides=(2,2), padding='valid'))\n",
    "\n",
    "model.add(Conv2D(64, (5,5), padding='same', activation='relu', kernel_initializer='TruncatedNormal'))\n",
    "model.add(AveragePooling2D((2,2), strides=(2,2), padding='valid'))\n",
    "\n",
    "model.add(Flatten())\n",
    "# TODO is this where dropout should be?\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(3072, activation='relu', kernel_initializer='TruncatedNormal'))\n",
    "# TODO svm activation function\n",
    "model.add(Dense(n_classes, activation='softmax', kernel_initializer='TruncatedNormal'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# compile model\n",
    "sgd = optimizers.SGD(momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "# fit model\n",
    "epoch_sets = 1\n",
    "epochs = 100\n",
    "batch_size = 50\n",
    "for es in range(epoch_sets):\n",
    "    # get current lr\n",
    "    lr = k.get_value(sgd.lr)\n",
    "    print('epoch set:', es, 'lr value:', lr)\n",
    "    \n",
    "    # fit model\n",
    "    history = model.fit_generator(train_datagen.flow(x_train, y_train_onehot, batch_size=batch_size)\n",
    "                        , steps_per_epoch=n_train//batch_size\n",
    "                        , epochs=epochs\n",
    "                        , validation_data=test_datagen.flow(x_test, y_test_onehot, batch_size=batch_size)\n",
    "                        , validation_steps=n_test//batch_size)\n",
    "    # decay lr by half\n",
    "    lr = lr / 2\n",
    "    k.set_value(sgd.lr, lr)\n",
    "    \n",
    "# visualize loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocess: featurewise, horizontal_flip + sgd_decay on tiny subset + batchsize=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files (x86)\\Python35-64bit\\lib\\site-packages\\keras\\preprocessing\\image.py:648: UserWarning: Expected input to be images (as Numpy array) following the data format convention \"channels_last\" (channels on axis 3), i.e. expected either 1, 3 or 4 channels on axis 3. However, it was passed an array with shape (50, 48, 48, 1) (1 channels).\n",
      "  ' (' + str(x.shape[self.channel_axis]) + ' channels).')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "cropping2d_1 (Cropping2D)    (None, 42, 42, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 42, 42, 32)        832       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 21, 21, 32)        0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_1 (ZeroPaddin (None, 23, 23, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 20, 20, 32)        16416     \n",
      "_________________________________________________________________\n",
      "average_pooling2d_1 (Average (None, 10, 10, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 10, 10, 64)        51264     \n",
      "_________________________________________________________________\n",
      "average_pooling2d_2 (Average (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 3072)              4918272   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 7)                 21511     \n",
      "=================================================================\n",
      "Total params: 5,008,295\n",
      "Trainable params: 5,008,295\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "epoch set: 0 lr value: 0.01\n",
      "Epoch 1/150\n",
      "1/1 [==============================] - 0s - loss: 2.0936 - acc: 0.0000e+00 - val_loss: 1.6837 - val_acc: 0.6600\n",
      "Epoch 2/150\n",
      "1/1 [==============================] - 0s - loss: 1.7154 - acc: 0.5600 - val_loss: 1.2937 - val_acc: 0.6400\n",
      "Epoch 3/150\n",
      "1/1 [==============================] - 0s - loss: 1.3017 - acc: 0.6000 - val_loss: 0.8587 - val_acc: 0.6400\n",
      "Epoch 4/150\n",
      "1/1 [==============================] - 0s - loss: 0.9606 - acc: 0.6000 - val_loss: 0.6968 - val_acc: 0.6600\n",
      "Epoch 5/150\n",
      "1/1 [==============================] - 0s - loss: 0.7065 - acc: 0.5800 - val_loss: 0.7549 - val_acc: 0.3800\n",
      "Epoch 6/150\n",
      "1/1 [==============================] - 0s - loss: 0.6737 - acc: 0.5600 - val_loss: 0.8101 - val_acc: 0.6400\n",
      "Epoch 7/150\n",
      "1/1 [==============================] - 0s - loss: 1.0612 - acc: 0.6000 - val_loss: 2.5490 - val_acc: 0.3600\n",
      "Epoch 8/150\n",
      "1/1 [==============================] - 0s - loss: 2.3726 - acc: 0.4000 - val_loss: 0.8427 - val_acc: 0.6400\n",
      "Epoch 9/150\n",
      "1/1 [==============================] - 0s - loss: 1.0472 - acc: 0.6000 - val_loss: 0.8419 - val_acc: 0.6400\n",
      "Epoch 10/150\n",
      "1/1 [==============================] - 0s - loss: 0.8816 - acc: 0.6000 - val_loss: 0.8797 - val_acc: 0.6400\n",
      "Epoch 11/150\n",
      "1/1 [==============================] - 0s - loss: 0.8897 - acc: 0.6000 - val_loss: 0.8151 - val_acc: 0.6400\n",
      "Epoch 12/150\n",
      "1/1 [==============================] - 0s - loss: 0.8372 - acc: 0.6000 - val_loss: 0.7434 - val_acc: 0.6400\n",
      "Epoch 13/150\n",
      "1/1 [==============================] - 0s - loss: 0.7394 - acc: 0.6000 - val_loss: 0.6976 - val_acc: 0.6400\n",
      "Epoch 14/150\n",
      "1/1 [==============================] - 0s - loss: 0.6699 - acc: 0.6400 - val_loss: 0.6974 - val_acc: 0.5200\n",
      "Epoch 15/150\n",
      "1/1 [==============================] - 0s - loss: 0.6753 - acc: 0.6400 - val_loss: 0.6710 - val_acc: 0.6000\n",
      "Epoch 16/150\n",
      "1/1 [==============================] - 0s - loss: 0.6594 - acc: 0.6200 - val_loss: 0.6317 - val_acc: 0.6600\n",
      "Epoch 17/150\n",
      "1/1 [==============================] - 0s - loss: 0.6560 - acc: 0.6200 - val_loss: 0.6534 - val_acc: 0.6400\n",
      "Epoch 18/150\n",
      "1/1 [==============================] - 0s - loss: 0.6629 - acc: 0.6400 - val_loss: 0.6187 - val_acc: 0.6200\n",
      "Epoch 19/150\n",
      "1/1 [==============================] - 0s - loss: 0.6044 - acc: 0.7200 - val_loss: 0.6885 - val_acc: 0.5600\n",
      "Epoch 20/150\n",
      "1/1 [==============================] - 0s - loss: 0.6111 - acc: 0.6600 - val_loss: 0.6144 - val_acc: 0.6600\n",
      "Epoch 21/150\n",
      "1/1 [==============================] - 0s - loss: 0.7447 - acc: 0.6200 - val_loss: 2.2088 - val_acc: 0.3600\n",
      "Epoch 22/150\n",
      "1/1 [==============================] - 0s - loss: 1.8982 - acc: 0.4000 - val_loss: 1.4444 - val_acc: 0.6400\n",
      "Epoch 23/150\n",
      "1/1 [==============================] - 0s - loss: 1.8067 - acc: 0.6000 - val_loss: 0.7505 - val_acc: 0.6400\n",
      "Epoch 24/150\n",
      "1/1 [==============================] - 0s - loss: 0.8037 - acc: 0.6000 - val_loss: 0.7536 - val_acc: 0.6400\n",
      "Epoch 25/150\n",
      "1/1 [==============================] - 0s - loss: 0.7638 - acc: 0.6000 - val_loss: 0.7848 - val_acc: 0.6400\n",
      "Epoch 26/150\n",
      "1/1 [==============================] - 0s - loss: 0.8098 - acc: 0.6000 - val_loss: 0.7720 - val_acc: 0.6400\n",
      "Epoch 27/150\n",
      "1/1 [==============================] - 0s - loss: 0.7887 - acc: 0.6000 - val_loss: 0.7400 - val_acc: 0.6400\n",
      "Epoch 28/150\n",
      "1/1 [==============================] - 0s - loss: 0.7640 - acc: 0.6000 - val_loss: 0.7032 - val_acc: 0.6400\n",
      "Epoch 29/150\n",
      "1/1 [==============================] - 0s - loss: 0.7080 - acc: 0.6000 - val_loss: 0.6729 - val_acc: 0.6400\n",
      "Epoch 30/150\n",
      "1/1 [==============================] - 0s - loss: 0.6722 - acc: 0.6000 - val_loss: 0.6540 - val_acc: 0.6400\n",
      "Epoch 31/150\n",
      "1/1 [==============================] - 0s - loss: 0.6539 - acc: 0.6400 - val_loss: 0.6466 - val_acc: 0.6800\n",
      "Epoch 32/150\n",
      "1/1 [==============================] - 0s - loss: 0.6388 - acc: 0.6400 - val_loss: 0.6449 - val_acc: 0.6400\n",
      "Epoch 33/150\n",
      "1/1 [==============================] - 0s - loss: 0.6249 - acc: 0.6800 - val_loss: 0.6416 - val_acc: 0.6600\n",
      "Epoch 34/150\n",
      "1/1 [==============================] - 0s - loss: 0.5875 - acc: 0.7000 - val_loss: 0.6289 - val_acc: 0.6800\n",
      "Epoch 35/150\n",
      "1/1 [==============================] - 0s - loss: 0.5667 - acc: 0.7600 - val_loss: 0.6390 - val_acc: 0.6000\n",
      "Epoch 36/150\n",
      "1/1 [==============================] - 0s - loss: 0.5947 - acc: 0.6800 - val_loss: 0.6262 - val_acc: 0.6600\n",
      "Epoch 37/150\n",
      "1/1 [==============================] - 0s - loss: 0.5437 - acc: 0.7400 - val_loss: 0.6280 - val_acc: 0.6400\n",
      "Epoch 38/150\n",
      "1/1 [==============================] - 0s - loss: 0.5748 - acc: 0.7200 - val_loss: 0.6316 - val_acc: 0.6400\n",
      "Epoch 39/150\n",
      "1/1 [==============================] - 0s - loss: 0.4866 - acc: 0.8200 - val_loss: 0.6345 - val_acc: 0.6200\n",
      "Epoch 40/150\n",
      "1/1 [==============================] - 0s - loss: 0.5624 - acc: 0.7800 - val_loss: 0.6433 - val_acc: 0.6200\n",
      "Epoch 41/150\n",
      "1/1 [==============================] - 0s - loss: 0.5230 - acc: 0.7400 - val_loss: 0.6411 - val_acc: 0.6800\n",
      "Epoch 42/150\n",
      "1/1 [==============================] - 0s - loss: 0.4971 - acc: 0.7400 - val_loss: 0.7149 - val_acc: 0.5800\n",
      "Epoch 43/150\n",
      "1/1 [==============================] - 0s - loss: 0.5613 - acc: 0.7000 - val_loss: 0.9096 - val_acc: 0.6200\n",
      "Epoch 44/150\n",
      "1/1 [==============================] - 0s - loss: 0.7319 - acc: 0.6600 - val_loss: 1.0121 - val_acc: 0.4600\n",
      "Epoch 45/150\n",
      "1/1 [==============================] - 0s - loss: 0.7904 - acc: 0.6000 - val_loss: 0.9059 - val_acc: 0.6400\n",
      "Epoch 46/150\n",
      "1/1 [==============================] - 0s - loss: 0.7025 - acc: 0.6400 - val_loss: 0.6187 - val_acc: 0.6400\n",
      "Epoch 47/150\n",
      "1/1 [==============================] - 0s - loss: 0.4603 - acc: 0.8200 - val_loss: 0.6309 - val_acc: 0.6600\n",
      "Epoch 48/150\n",
      "1/1 [==============================] - 0s - loss: 0.4337 - acc: 0.7800 - val_loss: 0.5906 - val_acc: 0.7200\n",
      "Epoch 49/150\n",
      "1/1 [==============================] - 0s - loss: 0.4599 - acc: 0.7800 - val_loss: 0.5971 - val_acc: 0.7200\n",
      "Epoch 50/150\n",
      "1/1 [==============================] - 0s - loss: 0.4599 - acc: 0.7800 - val_loss: 0.6086 - val_acc: 0.7400\n",
      "Epoch 51/150\n",
      "1/1 [==============================] - 0s - loss: 0.4136 - acc: 0.8600 - val_loss: 0.6114 - val_acc: 0.7200\n",
      "Epoch 52/150\n",
      "1/1 [==============================] - 0s - loss: 0.4271 - acc: 0.8200 - val_loss: 0.6126 - val_acc: 0.7000\n",
      "Epoch 53/150\n",
      "1/1 [==============================] - 0s - loss: 0.3889 - acc: 0.8600 - val_loss: 0.6111 - val_acc: 0.7200\n",
      "Epoch 54/150\n",
      "1/1 [==============================] - 0s - loss: 0.4051 - acc: 0.7800 - val_loss: 0.6144 - val_acc: 0.7200\n",
      "Epoch 55/150\n",
      "1/1 [==============================] - 0s - loss: 0.3966 - acc: 0.8000 - val_loss: 0.6165 - val_acc: 0.7200\n",
      "Epoch 56/150\n",
      "1/1 [==============================] - 0s - loss: 0.3388 - acc: 0.8800 - val_loss: 0.6129 - val_acc: 0.7200\n",
      "Epoch 57/150\n",
      "1/1 [==============================] - 0s - loss: 0.3604 - acc: 0.8200 - val_loss: 0.6206 - val_acc: 0.7200\n",
      "Epoch 58/150\n",
      "1/1 [==============================] - 0s - loss: 0.3098 - acc: 0.8600 - val_loss: 0.6364 - val_acc: 0.7200\n",
      "Epoch 59/150\n",
      "1/1 [==============================] - 0s - loss: 0.3012 - acc: 0.8800 - val_loss: 0.6541 - val_acc: 0.7200\n",
      "Epoch 60/150\n",
      "1/1 [==============================] - 0s - loss: 0.2993 - acc: 0.9200 - val_loss: 0.6639 - val_acc: 0.7200\n",
      "Epoch 61/150\n",
      "1/1 [==============================] - 0s - loss: 0.3387 - acc: 0.8400 - val_loss: 0.6775 - val_acc: 0.7200\n",
      "Epoch 62/150\n",
      "1/1 [==============================] - 0s - loss: 0.3152 - acc: 0.8400 - val_loss: 0.6635 - val_acc: 0.7200\n",
      "Epoch 63/150\n",
      "1/1 [==============================] - 0s - loss: 0.2446 - acc: 0.9200 - val_loss: 0.6569 - val_acc: 0.7200\n",
      "Epoch 64/150\n",
      "1/1 [==============================] - 0s - loss: 0.2828 - acc: 0.8600 - val_loss: 0.6571 - val_acc: 0.7400\n",
      "Epoch 65/150\n",
      "1/1 [==============================] - 0s - loss: 0.2454 - acc: 0.9000 - val_loss: 0.6698 - val_acc: 0.7600\n",
      "Epoch 66/150\n",
      "1/1 [==============================] - 0s - loss: 0.2168 - acc: 0.9000 - val_loss: 0.6800 - val_acc: 0.7600\n",
      "Epoch 67/150\n",
      "1/1 [==============================] - 0s - loss: 0.2444 - acc: 0.8800 - val_loss: 0.6839 - val_acc: 0.7200\n",
      "Epoch 68/150\n",
      "1/1 [==============================] - 0s - loss: 0.2393 - acc: 0.8800 - val_loss: 0.7057 - val_acc: 0.7400\n",
      "Epoch 69/150\n",
      "1/1 [==============================] - 0s - loss: 0.2471 - acc: 0.8800 - val_loss: 0.7382 - val_acc: 0.7400\n",
      "Epoch 70/150\n",
      "1/1 [==============================] - 0s - loss: 0.2668 - acc: 0.8800 - val_loss: 0.7445 - val_acc: 0.7200\n",
      "Epoch 71/150\n",
      "1/1 [==============================] - 0s - loss: 0.2173 - acc: 0.8600 - val_loss: 0.7420 - val_acc: 0.7800\n",
      "Epoch 72/150\n",
      "1/1 [==============================] - 0s - loss: 0.1908 - acc: 0.9400 - val_loss: 0.7592 - val_acc: 0.7600\n",
      "Epoch 73/150\n",
      "1/1 [==============================] - 0s - loss: 0.1848 - acc: 0.9400 - val_loss: 0.7950 - val_acc: 0.7600\n",
      "Epoch 74/150\n",
      "1/1 [==============================] - 0s - loss: 0.1573 - acc: 0.9600 - val_loss: 0.8126 - val_acc: 0.7600\n",
      "Epoch 75/150\n",
      "1/1 [==============================] - 0s - loss: 0.1720 - acc: 0.9200 - val_loss: 0.8507 - val_acc: 0.7400\n",
      "Epoch 76/150\n",
      "1/1 [==============================] - 0s - loss: 0.1766 - acc: 0.9000 - val_loss: 0.8331 - val_acc: 0.7400\n",
      "Epoch 77/150\n",
      "1/1 [==============================] - 0s - loss: 0.1569 - acc: 0.9600 - val_loss: 0.8323 - val_acc: 0.7600\n",
      "Epoch 78/150\n",
      "1/1 [==============================] - 0s - loss: 0.1486 - acc: 0.9600 - val_loss: 0.9626 - val_acc: 0.6600\n",
      "Epoch 79/150\n",
      "1/1 [==============================] - 0s - loss: 0.1660 - acc: 0.9200 - val_loss: 0.8038 - val_acc: 0.7600\n",
      "Epoch 80/150\n",
      "1/1 [==============================] - 0s - loss: 0.1160 - acc: 0.9800 - val_loss: 0.8100 - val_acc: 0.7400\n",
      "Epoch 81/150\n",
      "1/1 [==============================] - 0s - loss: 0.1190 - acc: 0.9600 - val_loss: 0.8668 - val_acc: 0.7600\n",
      "Epoch 82/150\n",
      "1/1 [==============================] - 0s - loss: 0.1126 - acc: 0.9400 - val_loss: 0.8788 - val_acc: 0.7200\n",
      "Epoch 83/150\n",
      "1/1 [==============================] - 0s - loss: 0.1074 - acc: 0.9600 - val_loss: 0.8871 - val_acc: 0.7400\n",
      "Epoch 84/150\n",
      "1/1 [==============================] - 0s - loss: 0.0848 - acc: 0.9800 - val_loss: 0.8930 - val_acc: 0.7400\n",
      "Epoch 85/150\n",
      "1/1 [==============================] - 0s - loss: 0.0735 - acc: 0.9800 - val_loss: 0.9238 - val_acc: 0.7400\n",
      "Epoch 86/150\n",
      "1/1 [==============================] - 0s - loss: 0.0922 - acc: 0.9800 - val_loss: 0.9421 - val_acc: 0.7600\n",
      "Epoch 87/150\n",
      "1/1 [==============================] - 0s - loss: 0.0830 - acc: 1.0000 - val_loss: 0.9161 - val_acc: 0.7800\n",
      "Epoch 88/150\n",
      "1/1 [==============================] - 0s - loss: 0.1230 - acc: 0.9400 - val_loss: 0.9753 - val_acc: 0.7600\n",
      "Epoch 89/150\n",
      "1/1 [==============================] - 0s - loss: 0.0938 - acc: 0.9600 - val_loss: 0.9721 - val_acc: 0.7600\n",
      "Epoch 90/150\n",
      "1/1 [==============================] - 0s - loss: 0.0522 - acc: 0.9800 - val_loss: 1.0096 - val_acc: 0.7600\n",
      "Epoch 91/150\n",
      "1/1 [==============================] - 0s - loss: 0.0653 - acc: 1.0000 - val_loss: 1.0222 - val_acc: 0.7600\n",
      "Epoch 92/150\n",
      "1/1 [==============================] - 0s - loss: 0.0578 - acc: 0.9800 - val_loss: 1.1085 - val_acc: 0.6800\n",
      "Epoch 93/150\n",
      "1/1 [==============================] - 0s - loss: 0.0735 - acc: 1.0000 - val_loss: 0.9616 - val_acc: 0.7600\n",
      "Epoch 94/150\n",
      "1/1 [==============================] - 0s - loss: 0.0758 - acc: 0.9600 - val_loss: 0.9905 - val_acc: 0.7600\n",
      "Epoch 95/150\n",
      "1/1 [==============================] - 0s - loss: 0.0445 - acc: 1.0000 - val_loss: 1.0147 - val_acc: 0.7600\n",
      "Epoch 96/150\n",
      "1/1 [==============================] - 0s - loss: 0.0516 - acc: 1.0000 - val_loss: 1.0251 - val_acc: 0.7600\n",
      "Epoch 97/150\n",
      "1/1 [==============================] - 0s - loss: 0.0453 - acc: 0.9800 - val_loss: 1.0615 - val_acc: 0.7600\n",
      "Epoch 98/150\n",
      "1/1 [==============================] - 0s - loss: 0.0637 - acc: 0.9800 - val_loss: 1.0776 - val_acc: 0.7200\n",
      "Epoch 99/150\n",
      "1/1 [==============================] - 0s - loss: 0.0716 - acc: 0.9600 - val_loss: 1.0969 - val_acc: 0.7600\n",
      "Epoch 100/150\n",
      "1/1 [==============================] - 0s - loss: 0.0413 - acc: 1.0000 - val_loss: 1.1552 - val_acc: 0.7400\n",
      "Epoch 101/150\n",
      "1/1 [==============================] - 0s - loss: 0.0546 - acc: 1.0000 - val_loss: 1.1208 - val_acc: 0.7600\n",
      "Epoch 102/150\n",
      "1/1 [==============================] - 0s - loss: 0.0401 - acc: 1.0000 - val_loss: 1.1421 - val_acc: 0.7600\n",
      "Epoch 103/150\n",
      "1/1 [==============================] - 0s - loss: 0.0549 - acc: 1.0000 - val_loss: 1.1716 - val_acc: 0.7400\n",
      "Epoch 104/150\n",
      "1/1 [==============================] - 0s - loss: 0.0517 - acc: 0.9800 - val_loss: 1.1820 - val_acc: 0.7600\n",
      "Epoch 105/150\n",
      "1/1 [==============================] - 0s - loss: 0.0310 - acc: 1.0000 - val_loss: 1.2240 - val_acc: 0.7600\n",
      "Epoch 106/150\n",
      "1/1 [==============================] - 0s - loss: 0.0292 - acc: 1.0000 - val_loss: 1.2733 - val_acc: 0.7600\n",
      "Epoch 107/150\n",
      "1/1 [==============================] - 0s - loss: 0.0219 - acc: 1.0000 - val_loss: 1.2749 - val_acc: 0.7600\n",
      "Epoch 108/150\n",
      "1/1 [==============================] - 0s - loss: 0.0267 - acc: 1.0000 - val_loss: 1.2880 - val_acc: 0.7600\n",
      "Epoch 109/150\n",
      "1/1 [==============================] - 0s - loss: 0.0568 - acc: 0.9800 - val_loss: 1.2900 - val_acc: 0.7600\n",
      "Epoch 110/150\n",
      "1/1 [==============================] - 0s - loss: 0.0246 - acc: 1.0000 - val_loss: 1.2647 - val_acc: 0.7600\n",
      "Epoch 111/150\n",
      "1/1 [==============================] - 0s - loss: 0.0194 - acc: 1.0000 - val_loss: 1.2211 - val_acc: 0.7600\n",
      "Epoch 112/150\n",
      "1/1 [==============================] - 0s - loss: 0.0232 - acc: 1.0000 - val_loss: 1.2113 - val_acc: 0.7600\n",
      "Epoch 113/150\n",
      "1/1 [==============================] - 0s - loss: 0.0354 - acc: 0.9800 - val_loss: 1.2389 - val_acc: 0.7600\n",
      "Epoch 114/150\n",
      "1/1 [==============================] - 0s - loss: 0.0173 - acc: 1.0000 - val_loss: 1.2504 - val_acc: 0.7400\n",
      "Epoch 115/150\n",
      "1/1 [==============================] - 0s - loss: 0.0207 - acc: 1.0000 - val_loss: 1.1921 - val_acc: 0.7600\n",
      "Epoch 116/150\n",
      "1/1 [==============================] - 0s - loss: 0.0241 - acc: 1.0000 - val_loss: 1.1874 - val_acc: 0.7600\n",
      "Epoch 117/150\n",
      "1/1 [==============================] - 0s - loss: 0.0229 - acc: 1.0000 - val_loss: 1.1655 - val_acc: 0.7600\n",
      "Epoch 118/150\n",
      "1/1 [==============================] - 0s - loss: 0.0094 - acc: 1.0000 - val_loss: 1.1622 - val_acc: 0.7600\n",
      "Epoch 119/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s - loss: 0.0295 - acc: 0.9800 - val_loss: 1.1966 - val_acc: 0.7600\n",
      "Epoch 120/150\n",
      "1/1 [==============================] - 0s - loss: 0.0117 - acc: 1.0000 - val_loss: 1.2355 - val_acc: 0.7600\n",
      "Epoch 121/150\n",
      "1/1 [==============================] - 0s - loss: 0.0073 - acc: 1.0000 - val_loss: 1.2578 - val_acc: 0.7600\n",
      "Epoch 122/150\n",
      "1/1 [==============================] - 0s - loss: 0.0163 - acc: 1.0000 - val_loss: 1.2737 - val_acc: 0.7600\n",
      "Epoch 123/150\n",
      "1/1 [==============================] - 0s - loss: 0.0267 - acc: 1.0000 - val_loss: 1.2544 - val_acc: 0.7400\n",
      "Epoch 124/150\n",
      "1/1 [==============================] - 0s - loss: 0.0189 - acc: 1.0000 - val_loss: 1.2794 - val_acc: 0.7600\n",
      "Epoch 125/150\n",
      "1/1 [==============================] - 0s - loss: 0.0070 - acc: 1.0000 - val_loss: 1.3042 - val_acc: 0.7400\n",
      "Epoch 126/150\n",
      "1/1 [==============================] - 0s - loss: 0.0095 - acc: 1.0000 - val_loss: 1.3361 - val_acc: 0.7400\n",
      "Epoch 127/150\n",
      "1/1 [==============================] - 0s - loss: 0.0186 - acc: 1.0000 - val_loss: 1.3986 - val_acc: 0.7600\n",
      "Epoch 128/150\n",
      "1/1 [==============================] - 0s - loss: 0.0239 - acc: 1.0000 - val_loss: 1.5326 - val_acc: 0.7000\n",
      "Epoch 129/150\n",
      "1/1 [==============================] - 0s - loss: 0.0099 - acc: 1.0000 - val_loss: 1.5943 - val_acc: 0.7000\n",
      "Epoch 130/150\n",
      "1/1 [==============================] - 0s - loss: 0.0107 - acc: 1.0000 - val_loss: 1.6447 - val_acc: 0.7000\n",
      "Epoch 131/150\n",
      "1/1 [==============================] - 0s - loss: 0.0229 - acc: 1.0000 - val_loss: 1.6133 - val_acc: 0.7200\n",
      "Epoch 132/150\n",
      "1/1 [==============================] - 0s - loss: 0.0084 - acc: 1.0000 - val_loss: 1.6168 - val_acc: 0.7800\n",
      "Epoch 133/150\n",
      "1/1 [==============================] - 0s - loss: 0.0157 - acc: 1.0000 - val_loss: 1.6155 - val_acc: 0.7800\n",
      "Epoch 134/150\n",
      "1/1 [==============================] - 0s - loss: 0.0096 - acc: 1.0000 - val_loss: 1.6310 - val_acc: 0.7600\n",
      "Epoch 135/150\n",
      "1/1 [==============================] - 0s - loss: 0.0096 - acc: 1.0000 - val_loss: 1.6373 - val_acc: 0.7600\n",
      "Epoch 136/150\n",
      "1/1 [==============================] - 0s - loss: 0.0078 - acc: 1.0000 - val_loss: 1.6274 - val_acc: 0.7600\n",
      "Epoch 137/150\n",
      "1/1 [==============================] - 0s - loss: 0.0215 - acc: 1.0000 - val_loss: 1.6121 - val_acc: 0.7800\n",
      "Epoch 138/150\n",
      "1/1 [==============================] - 0s - loss: 0.0070 - acc: 1.0000 - val_loss: 1.6028 - val_acc: 0.7800\n",
      "Epoch 139/150\n",
      "1/1 [==============================] - 0s - loss: 0.0118 - acc: 1.0000 - val_loss: 1.6329 - val_acc: 0.7800\n",
      "Epoch 140/150\n",
      "1/1 [==============================] - 0s - loss: 0.0083 - acc: 1.0000 - val_loss: 1.6433 - val_acc: 0.7800\n",
      "Epoch 141/150\n",
      "1/1 [==============================] - 0s - loss: 0.0036 - acc: 1.0000 - val_loss: 1.6674 - val_acc: 0.7200\n",
      "Epoch 142/150\n",
      "1/1 [==============================] - 0s - loss: 0.0063 - acc: 1.0000 - val_loss: 1.6637 - val_acc: 0.7200\n",
      "Epoch 143/150\n",
      "1/1 [==============================] - 0s - loss: 0.0068 - acc: 1.0000 - val_loss: 1.6672 - val_acc: 0.7600\n",
      "Epoch 144/150\n",
      "1/1 [==============================] - 0s - loss: 0.0023 - acc: 1.0000 - val_loss: 1.6738 - val_acc: 0.7600\n",
      "Epoch 145/150\n",
      "1/1 [==============================] - 0s - loss: 0.0112 - acc: 1.0000 - val_loss: 1.6555 - val_acc: 0.7400\n",
      "Epoch 146/150\n",
      "1/1 [==============================] - 0s - loss: 0.0048 - acc: 1.0000 - val_loss: 1.6554 - val_acc: 0.7800\n",
      "Epoch 147/150\n",
      "1/1 [==============================] - 0s - loss: 0.0138 - acc: 1.0000 - val_loss: 1.6775 - val_acc: 0.7600\n",
      "Epoch 148/150\n",
      "1/1 [==============================] - 0s - loss: 0.0023 - acc: 1.0000 - val_loss: 1.7125 - val_acc: 0.7400\n",
      "Epoch 149/150\n",
      "1/1 [==============================] - 0s - loss: 0.0070 - acc: 1.0000 - val_loss: 1.7236 - val_acc: 0.7400\n",
      "Epoch 150/150\n",
      "1/1 [==============================] - 0s - loss: 0.0104 - acc: 1.0000 - val_loss: 1.7337 - val_acc: 0.7600\n"
     ]
    }
   ],
   "source": [
    "# preprocess\n",
    "rescale_factor = 255\n",
    "train_datagen = ImageDataGenerator(featurewise_center=True, featurewise_std_normalization=True\n",
    "                                   , horizontal_flip=True)\n",
    "train_datagen.fit(x_train)\n",
    "\n",
    "test_datagen = ImageDataGenerator(featurewise_center=True, featurewise_std_normalization=True)\n",
    "test_datagen.fit(x_train)\n",
    "\n",
    "# build model\n",
    "model_name = '1st_place'\n",
    "model = Sequential(name=model_name)\n",
    "model.add(Cropping2D(cropping=3, input_shape=image_shape))\n",
    "model.add(Conv2D(32, (5,5), padding='same', activation='relu', kernel_initializer='TruncatedNormal'))\n",
    "model.add(MaxPooling2D((2,2), strides=(2,2), padding='valid'))\n",
    "\n",
    "model.add(ZeroPadding2D(padding=1))\n",
    "model.add(Conv2D(32, (4,4), padding='valid', activation='relu', kernel_initializer='TruncatedNormal'))\n",
    "model.add(AveragePooling2D((2,2), strides=(2,2), padding='valid'))\n",
    "\n",
    "model.add(Conv2D(64, (5,5), padding='same', activation='relu', kernel_initializer='TruncatedNormal'))\n",
    "model.add(AveragePooling2D((2,2), strides=(2,2), padding='valid'))\n",
    "\n",
    "model.add(Flatten())\n",
    "# TODO is this where dropout should be?\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(3072, activation='relu', kernel_initializer='TruncatedNormal'))\n",
    "# TODO svm activation function\n",
    "model.add(Dense(n_classes, activation='softmax', kernel_initializer='TruncatedNormal'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# compile model\n",
    "sgd = optimizers.SGD(momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "# fit model\n",
    "epoch_sets = 1\n",
    "epochs = 150\n",
    "batch_size = 50\n",
    "for es in range(epoch_sets):\n",
    "    # get current lr\n",
    "    lr = k.get_value(sgd.lr)\n",
    "    print('epoch set:', es, 'lr value:', lr)\n",
    "    \n",
    "    # fit model\n",
    "    history = model.fit_generator(train_datagen.flow(x_train, y_train_onehot, batch_size=batch_size)\n",
    "                        , steps_per_epoch=n_train//batch_size\n",
    "                        , epochs=epochs\n",
    "                        , validation_data=test_datagen.flow(x_test, y_test_onehot, batch_size=batch_size)\n",
    "                        , validation_steps=n_test//batch_size)\n",
    "    # decay lr by half\n",
    "    lr = lr / 2\n",
    "    k.set_value(sgd.lr, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1bbd8063908>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1bb9a558b38>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocess: featurewise, horizontal_flip + sgd_decay on tiny subset + batchsize=16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files (x86)\\Python35-64bit\\lib\\site-packages\\keras\\preprocessing\\image.py:648: UserWarning: Expected input to be images (as Numpy array) following the data format convention \"channels_last\" (channels on axis 3), i.e. expected either 1, 3 or 4 channels on axis 3. However, it was passed an array with shape (50, 48, 48, 1) (1 channels).\n",
      "  ' (' + str(x.shape[self.channel_axis]) + ' channels).')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "cropping2d_2 (Cropping2D)    (None, 42, 42, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 42, 42, 32)        832       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 21, 21, 32)        0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_2 (ZeroPaddin (None, 23, 23, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 20, 20, 32)        16416     \n",
      "_________________________________________________________________\n",
      "average_pooling2d_3 (Average (None, 10, 10, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 10, 10, 64)        51264     \n",
      "_________________________________________________________________\n",
      "average_pooling2d_4 (Average (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 3072)              4918272   \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 7)                 21511     \n",
      "=================================================================\n",
      "Total params: 5,008,295\n",
      "Trainable params: 5,008,295\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "epoch set: 0 lr value: 0.01\n",
      "Epoch 1/150\n",
      "3/3 [==============================] - 0s - loss: 1.7085 - acc: 0.3125 - val_loss: 0.8444 - val_acc: 0.6250\n",
      "Epoch 2/150\n",
      "3/3 [==============================] - 0s - loss: 0.8026 - acc: 0.5460 - val_loss: 0.7420 - val_acc: 0.5882\n",
      "Epoch 3/150\n",
      "3/3 [==============================] - 0s - loss: 3.3244 - acc: 0.4366 - val_loss: 1.2569 - val_acc: 0.5588\n",
      "Epoch 4/150\n",
      "3/3 [==============================] - 0s - loss: 1.1533 - acc: 0.6553 - val_loss: 1.3452 - val_acc: 0.6176\n",
      "Epoch 5/150\n",
      "3/3 [==============================] - 0s - loss: 1.2866 - acc: 0.6250 - val_loss: 0.8571 - val_acc: 0.2941\n",
      "Epoch 6/150\n",
      "3/3 [==============================] - 0s - loss: 1.1591 - acc: 0.2987 - val_loss: 0.9117 - val_acc: 0.2941\n",
      "Epoch 7/150\n",
      "3/3 [==============================] - 0s - loss: 0.9046 - acc: 0.6553 - val_loss: 1.0127 - val_acc: 0.5588\n",
      "Epoch 8/150\n",
      "3/3 [==============================] - 0s - loss: 1.0674 - acc: 0.6553 - val_loss: 0.6656 - val_acc: 0.7353\n",
      "Epoch 9/150\n",
      "3/3 [==============================] - 0s - loss: 0.8325 - acc: 0.6042 - val_loss: 0.6984 - val_acc: 0.5882\n",
      "Epoch 10/150\n",
      "3/3 [==============================] - 0s - loss: 0.7139 - acc: 0.6608 - val_loss: 0.5835 - val_acc: 0.6765\n",
      "Epoch 11/150\n",
      "3/3 [==============================] - 0s - loss: 0.9801 - acc: 0.3447 - val_loss: 0.5790 - val_acc: 0.7059\n",
      "Epoch 12/150\n",
      "3/3 [==============================] - 0s - loss: 0.6973 - acc: 0.5689 - val_loss: 0.6090 - val_acc: 0.6765\n",
      "Epoch 13/150\n",
      "3/3 [==============================] - 0s - loss: 0.6931 - acc: 0.5833 - val_loss: 0.6509 - val_acc: 0.6471\n",
      "Epoch 14/150\n",
      "3/3 [==============================] - 0s - loss: 0.6721 - acc: 0.7013 - val_loss: 0.6297 - val_acc: 0.6471\n",
      "Epoch 15/150\n",
      "3/3 [==============================] - 0s - loss: 0.6905 - acc: 0.5919 - val_loss: 0.6605 - val_acc: 0.5588\n",
      "Epoch 16/150\n",
      "3/3 [==============================] - 0s - loss: 0.7089 - acc: 0.6149 - val_loss: 1.1246 - val_acc: 0.3824\n",
      "Epoch 17/150\n",
      "3/3 [==============================] - 0s - loss: 0.9214 - acc: 0.4167 - val_loss: 0.6410 - val_acc: 0.5882\n",
      "Epoch 18/150\n",
      "3/3 [==============================] - 0s - loss: 1.0898 - acc: 0.6783 - val_loss: 0.5425 - val_acc: 0.7353\n",
      "Epoch 19/150\n",
      "3/3 [==============================] - 0s - loss: 0.6088 - acc: 0.6149 - val_loss: 0.6413 - val_acc: 0.6765\n",
      "Epoch 20/150\n",
      "3/3 [==============================] - 0s - loss: 0.5424 - acc: 0.7702 - val_loss: 0.6019 - val_acc: 0.7059\n",
      "Epoch 21/150\n",
      "3/3 [==============================] - 0s - loss: 0.6108 - acc: 0.6875 - val_loss: 0.6246 - val_acc: 0.5294\n",
      "Epoch 22/150\n",
      "3/3 [==============================] - 0s - loss: 0.4814 - acc: 0.7068 - val_loss: 0.5854 - val_acc: 0.7941\n",
      "Epoch 23/150\n",
      "3/3 [==============================] - 0s - loss: 1.0080 - acc: 0.5689 - val_loss: 0.9817 - val_acc: 0.4412\n",
      "Epoch 24/150\n",
      "3/3 [==============================] - 0s - loss: 0.7519 - acc: 0.3851 - val_loss: 0.7211 - val_acc: 0.6765\n",
      "Epoch 25/150\n",
      "3/3 [==============================] - 0s - loss: 0.8011 - acc: 0.6458 - val_loss: 0.6509 - val_acc: 0.6471\n",
      "Epoch 26/150\n",
      "3/3 [==============================] - 0s - loss: 0.7280 - acc: 0.4136 - val_loss: 0.6129 - val_acc: 0.7059\n",
      "Epoch 27/150\n",
      "3/3 [==============================] - 0s - loss: 0.7725 - acc: 0.6324 - val_loss: 0.8758 - val_acc: 0.4706\n",
      "Epoch 28/150\n",
      "3/3 [==============================] - 0s - loss: 0.7655 - acc: 0.5460 - val_loss: 0.7390 - val_acc: 0.5294\n",
      "Epoch 29/150\n",
      "3/3 [==============================] - 0s - loss: 0.6092 - acc: 0.7083 - val_loss: 0.6260 - val_acc: 0.6765\n",
      "Epoch 30/150\n",
      "3/3 [==============================] - 0s - loss: 0.5023 - acc: 0.8162 - val_loss: 0.6177 - val_acc: 0.6471\n",
      "Epoch 31/150\n",
      "3/3 [==============================] - 0s - loss: 0.5900 - acc: 0.6608 - val_loss: 0.5482 - val_acc: 0.7647\n",
      "Epoch 32/150\n",
      "3/3 [==============================] - 0s - loss: 0.4479 - acc: 0.8392 - val_loss: 0.7188 - val_acc: 0.6765\n",
      "Epoch 33/150\n",
      "3/3 [==============================] - 0s - loss: 0.4765 - acc: 0.7708 - val_loss: 0.6199 - val_acc: 0.7353\n",
      "Epoch 34/150\n",
      "3/3 [==============================] - 0s - loss: 0.4655 - acc: 0.8392 - val_loss: 0.7387 - val_acc: 0.5588\n",
      "Epoch 35/150\n",
      "3/3 [==============================] - 0s - loss: 0.4004 - acc: 0.8162 - val_loss: 0.9951 - val_acc: 0.6176\n",
      "Epoch 36/150\n",
      "3/3 [==============================] - 0s - loss: 0.6562 - acc: 0.6608 - val_loss: 2.1464 - val_acc: 0.4412\n",
      "Epoch 37/150\n",
      "3/3 [==============================] - 0s - loss: 1.1142 - acc: 0.5833 - val_loss: 0.7770 - val_acc: 0.5294\n",
      "Epoch 38/150\n",
      "3/3 [==============================] - 0s - loss: 0.5272 - acc: 0.7472 - val_loss: 0.7494 - val_acc: 0.5588\n",
      "Epoch 39/150\n",
      "3/3 [==============================] - 0s - loss: 0.4838 - acc: 0.8162 - val_loss: 0.7473 - val_acc: 0.5000\n",
      "Epoch 40/150\n",
      "3/3 [==============================] - 0s - loss: 0.6691 - acc: 0.6379 - val_loss: 0.8453 - val_acc: 0.4412\n",
      "Epoch 41/150\n",
      "3/3 [==============================] - 0s - loss: 0.5256 - acc: 0.7292 - val_loss: 0.8456 - val_acc: 0.5882\n",
      "Epoch 42/150\n",
      "3/3 [==============================] - 0s - loss: 0.4035 - acc: 0.8851 - val_loss: 0.8809 - val_acc: 0.5294\n",
      "Epoch 43/150\n",
      "3/3 [==============================] - 0s - loss: 0.3235 - acc: 0.8851 - val_loss: 0.8850 - val_acc: 0.5882\n",
      "Epoch 44/150\n",
      "3/3 [==============================] - 0s - loss: 0.3865 - acc: 0.8851 - val_loss: 0.7791 - val_acc: 0.6765\n",
      "Epoch 45/150\n",
      "3/3 [==============================] - 0s - loss: 0.2956 - acc: 0.8333 - val_loss: 1.0753 - val_acc: 0.6471\n",
      "Epoch 46/150\n",
      "3/3 [==============================] - 0s - loss: 0.8908 - acc: 0.6149 - val_loss: 1.3343 - val_acc: 0.5294\n",
      "Epoch 47/150\n",
      "3/3 [==============================] - 0s - loss: 0.9529 - acc: 0.6838 - val_loss: 0.8991 - val_acc: 0.5882\n",
      "Epoch 48/150\n",
      "3/3 [==============================] - 0s - loss: 0.4080 - acc: 0.8162 - val_loss: 0.8401 - val_acc: 0.5294\n",
      "Epoch 49/150\n",
      "3/3 [==============================] - 0s - loss: 0.3965 - acc: 0.8125 - val_loss: 0.7674 - val_acc: 0.5882\n",
      "Epoch 50/150\n",
      "3/3 [==============================] - 0s - loss: 0.1973 - acc: 0.9081 - val_loss: 0.8143 - val_acc: 0.5588\n",
      "Epoch 51/150\n",
      "3/3 [==============================] - 0s - loss: 0.6155 - acc: 0.7472 - val_loss: 1.3681 - val_acc: 0.5000\n",
      "Epoch 52/150\n",
      "3/3 [==============================] - 0s - loss: 0.5729 - acc: 0.6379 - val_loss: 0.6562 - val_acc: 0.7353\n",
      "Epoch 53/150\n",
      "3/3 [==============================] - 0s - loss: 0.4697 - acc: 0.7917 - val_loss: 0.9471 - val_acc: 0.6176\n",
      "Epoch 54/150\n",
      "3/3 [==============================] - 0s - loss: 0.4285 - acc: 0.8392 - val_loss: 1.2758 - val_acc: 0.6176\n",
      "Epoch 55/150\n",
      "3/3 [==============================] - 0s - loss: 0.6383 - acc: 0.7013 - val_loss: 0.8741 - val_acc: 0.7292\n",
      "Epoch 56/150\n",
      "3/3 [==============================] - 0s - loss: 0.3070 - acc: 0.8162 - val_loss: 0.9209 - val_acc: 0.7292\n",
      "Epoch 57/150\n",
      "3/3 [==============================] - 0s - loss: 0.4891 - acc: 0.7917 - val_loss: 0.9994 - val_acc: 0.7083\n",
      "Epoch 58/150\n",
      "3/3 [==============================] - 0s - loss: 1.8491 - acc: 0.6149 - val_loss: 1.2543 - val_acc: 0.5417\n",
      "Epoch 59/150\n",
      "3/3 [==============================] - 0s - loss: 0.7431 - acc: 0.6379 - val_loss: 1.0566 - val_acc: 0.5417\n",
      "Epoch 60/150\n",
      "3/3 [==============================] - 0s - loss: 0.5908 - acc: 0.6838 - val_loss: 1.3181 - val_acc: 0.5294\n",
      "Epoch 61/150\n",
      "3/3 [==============================] - 0s - loss: 0.8543 - acc: 0.6042 - val_loss: 1.5375 - val_acc: 0.4118\n",
      "Epoch 62/150\n",
      "3/3 [==============================] - 0s - loss: 0.9672 - acc: 0.6379 - val_loss: 0.6336 - val_acc: 0.6765\n",
      "Epoch 63/150\n",
      "3/3 [==============================] - 0s - loss: 0.7172 - acc: 0.3906 - val_loss: 0.8003 - val_acc: 0.5000\n",
      "Epoch 64/150\n",
      "3/3 [==============================] - 0s - loss: 0.7483 - acc: 0.5000 - val_loss: 1.1799 - val_acc: 0.5882\n",
      "Epoch 65/150\n",
      "3/3 [==============================] - 0s - loss: 1.0139 - acc: 0.6667 - val_loss: 0.6061 - val_acc: 0.7353\n",
      "Epoch 66/150\n",
      "3/3 [==============================] - 0s - loss: 0.6148 - acc: 0.7702 - val_loss: 0.5072 - val_acc: 0.7059\n",
      "Epoch 67/150\n",
      "3/3 [==============================] - 0s - loss: 0.7724 - acc: 0.6149 - val_loss: 0.6739 - val_acc: 0.6471\n",
      "Epoch 68/150\n",
      "3/3 [==============================] - 0s - loss: 0.5597 - acc: 0.7528 - val_loss: 0.7233 - val_acc: 0.5588\n",
      "Epoch 69/150\n",
      "3/3 [==============================] - 0s - loss: 0.5138 - acc: 0.7292 - val_loss: 0.7159 - val_acc: 0.6471\n",
      "Epoch 70/150\n",
      "3/3 [==============================] - 0s - loss: 0.4553 - acc: 0.7472 - val_loss: 0.8588 - val_acc: 0.4706\n",
      "Epoch 71/150\n",
      "3/3 [==============================] - 0s - loss: 0.3927 - acc: 0.8162 - val_loss: 0.7478 - val_acc: 0.6765\n",
      "Epoch 72/150\n",
      "3/3 [==============================] - 0s - loss: 0.5118 - acc: 0.5744 - val_loss: 0.8329 - val_acc: 0.6176\n",
      "Epoch 73/150\n",
      "3/3 [==============================] - 0s - loss: 0.4787 - acc: 0.7292 - val_loss: 0.9290 - val_acc: 0.6471\n",
      "Epoch 74/150\n",
      "3/3 [==============================] - 0s - loss: 0.5717 - acc: 0.6608 - val_loss: 0.6612 - val_acc: 0.7647\n",
      "Epoch 75/150\n",
      "3/3 [==============================] - 0s - loss: 0.3290 - acc: 0.8851 - val_loss: 0.6106 - val_acc: 0.7647\n",
      "Epoch 76/150\n",
      "3/3 [==============================] - 0s - loss: 0.4387 - acc: 0.8162 - val_loss: 0.7809 - val_acc: 0.5882\n",
      "Epoch 77/150\n",
      "3/3 [==============================] - 0s - loss: 0.5377 - acc: 0.7708 - val_loss: 0.5954 - val_acc: 0.6765\n",
      "Epoch 78/150\n",
      "3/3 [==============================] - 0s - loss: 0.3497 - acc: 0.8851 - val_loss: 0.8357 - val_acc: 0.6765\n",
      "Epoch 79/150\n",
      "3/3 [==============================] - 0s - loss: 0.5982 - acc: 0.6379 - val_loss: 1.1394 - val_acc: 0.6176\n",
      "Epoch 80/150\n",
      "3/3 [==============================] - 0s - loss: 0.5218 - acc: 0.8621 - val_loss: 0.9842 - val_acc: 0.6471\n",
      "Epoch 81/150\n",
      "3/3 [==============================] - 0s - loss: 0.3308 - acc: 0.8750 - val_loss: 1.0422 - val_acc: 0.6176\n",
      "Epoch 82/150\n",
      "3/3 [==============================] - 0s - loss: 0.2297 - acc: 0.8851 - val_loss: 1.0581 - val_acc: 0.6765\n",
      "Epoch 83/150\n",
      "3/3 [==============================] - 0s - loss: 0.1846 - acc: 0.9081 - val_loss: 1.1944 - val_acc: 0.6765\n",
      "Epoch 84/150\n",
      "3/3 [==============================] - 0s - loss: 0.3406 - acc: 0.8392 - val_loss: 1.0360 - val_acc: 0.6471\n",
      "Epoch 85/150\n",
      "3/3 [==============================] - 0s - loss: 0.4226 - acc: 0.8542 - val_loss: 0.6782 - val_acc: 0.8235\n",
      "Epoch 86/150\n",
      "3/3 [==============================] - 0s - loss: 0.3629 - acc: 0.7757 - val_loss: 0.7091 - val_acc: 0.6765\n",
      "Epoch 87/150\n",
      "3/3 [==============================] - 0s - loss: 0.3700 - acc: 0.7528 - val_loss: 0.7290 - val_acc: 0.7353\n",
      "Epoch 88/150\n",
      "3/3 [==============================] - 0s - loss: 0.2185 - acc: 0.9081 - val_loss: 0.7649 - val_acc: 0.7353\n",
      "Epoch 89/150\n",
      "3/3 [==============================] - 0s - loss: 0.2177 - acc: 0.8958 - val_loss: 0.7563 - val_acc: 0.7941\n",
      "Epoch 90/150\n",
      "3/3 [==============================] - 0s - loss: 0.2158 - acc: 0.8851 - val_loss: 0.9641 - val_acc: 0.7647\n",
      "Epoch 91/150\n",
      "3/3 [==============================] - 0s - loss: 0.4778 - acc: 0.7528 - val_loss: 0.5919 - val_acc: 0.7941\n",
      "Epoch 92/150\n",
      "3/3 [==============================] - 0s - loss: 0.2675 - acc: 0.9081 - val_loss: 0.6164 - val_acc: 0.7647\n",
      "Epoch 93/150\n",
      "3/3 [==============================] - 0s - loss: 0.3065 - acc: 0.8333 - val_loss: 1.2651 - val_acc: 0.6471\n",
      "Epoch 94/150\n",
      "3/3 [==============================] - 0s - loss: 0.1365 - acc: 0.9081 - val_loss: 1.1231 - val_acc: 0.6458\n",
      "Epoch 95/150\n",
      "3/3 [==============================] - 0s - loss: 0.1606 - acc: 0.8851 - val_loss: 1.2094 - val_acc: 0.6458\n",
      "Epoch 96/150\n",
      "3/3 [==============================] - 0s - loss: 0.2361 - acc: 0.9081 - val_loss: 1.1329 - val_acc: 0.6176\n",
      "Epoch 97/150\n",
      "3/3 [==============================] - 0s - loss: 0.1456 - acc: 0.9167 - val_loss: 1.3638 - val_acc: 0.5882\n",
      "Epoch 98/150\n",
      "3/3 [==============================] - 0s - loss: 0.2527 - acc: 0.9311 - val_loss: 1.3180 - val_acc: 0.6765\n",
      "Epoch 99/150\n",
      "3/3 [==============================] - 0s - loss: 0.1068 - acc: 0.9770 - val_loss: 0.7226 - val_acc: 0.7647\n",
      "Epoch 100/150\n",
      "3/3 [==============================] - 0s - loss: 0.1847 - acc: 0.9311 - val_loss: 1.6608 - val_acc: 0.6765\n",
      "Epoch 101/150\n",
      "3/3 [==============================] - 0s - loss: 0.2063 - acc: 0.8750 - val_loss: 1.4622 - val_acc: 0.6471\n",
      "Epoch 102/150\n",
      "3/3 [==============================] - 0s - loss: 0.1460 - acc: 0.9540 - val_loss: 1.4627 - val_acc: 0.6471\n",
      "Epoch 103/150\n",
      "3/3 [==============================] - 0s - loss: 0.0459 - acc: 1.0000 - val_loss: 1.2948 - val_acc: 0.6765\n",
      "Epoch 104/150\n",
      "3/3 [==============================] - 0s - loss: 0.1335 - acc: 0.9311 - val_loss: 1.9712 - val_acc: 0.6471\n",
      "Epoch 105/150\n",
      "3/3 [==============================] - 0s - loss: 0.1842 - acc: 0.9375 - val_loss: 1.6549 - val_acc: 0.6471\n",
      "Epoch 106/150\n",
      "3/3 [==============================] - 0s - loss: 0.1615 - acc: 0.9770 - val_loss: 1.6723 - val_acc: 0.5882\n",
      "Epoch 107/150\n",
      "3/3 [==============================] - 0s - loss: 0.0422 - acc: 0.9770 - val_loss: 1.4191 - val_acc: 0.7059\n",
      "Epoch 108/150\n",
      "3/3 [==============================] - 0s - loss: 0.2295 - acc: 0.9311 - val_loss: 1.7183 - val_acc: 0.6765\n",
      "Epoch 109/150\n",
      "3/3 [==============================] - 0s - loss: 0.2466 - acc: 0.8958 - val_loss: 1.3328 - val_acc: 0.6471\n",
      "Epoch 110/150\n",
      "3/3 [==============================] - 0s - loss: 0.0845 - acc: 0.9770 - val_loss: 1.1322 - val_acc: 0.7353\n",
      "Epoch 111/150\n",
      "3/3 [==============================] - 0s - loss: 0.1124 - acc: 0.9770 - val_loss: 0.6970 - val_acc: 0.8235\n",
      "Epoch 112/150\n",
      "3/3 [==============================] - 0s - loss: 0.0888 - acc: 0.9770 - val_loss: 0.7483 - val_acc: 0.7059\n",
      "Epoch 113/150\n",
      "3/3 [==============================] - 0s - loss: 0.1370 - acc: 0.9583 - val_loss: 1.8372 - val_acc: 0.5882\n",
      "Epoch 114/150\n",
      "3/3 [==============================] - 0s - loss: 0.0665 - acc: 0.9770 - val_loss: 1.2746 - val_acc: 0.6176\n",
      "Epoch 115/150\n",
      "3/3 [==============================] - 0s - loss: 0.0651 - acc: 0.9540 - val_loss: 1.4583 - val_acc: 0.6765\n",
      "Epoch 116/150\n",
      "3/3 [==============================] - 0s - loss: 0.0984 - acc: 0.9540 - val_loss: 2.2220 - val_acc: 0.6471\n",
      "Epoch 117/150\n",
      "3/3 [==============================] - 0s - loss: 0.0945 - acc: 0.9583 - val_loss: 1.6141 - val_acc: 0.7059\n",
      "Epoch 118/150\n",
      "3/3 [==============================] - 0s - loss: 0.0328 - acc: 0.9770 - val_loss: 1.9859 - val_acc: 0.6471\n",
      "Epoch 119/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s - loss: 0.1100 - acc: 1.0000 - val_loss: 1.7391 - val_acc: 0.6471\n",
      "Epoch 120/150\n",
      "3/3 [==============================] - 0s - loss: 0.0787 - acc: 0.9311 - val_loss: 1.0783 - val_acc: 0.8235\n",
      "Epoch 121/150\n",
      "3/3 [==============================] - 0s - loss: 0.0878 - acc: 0.9167 - val_loss: 2.0136 - val_acc: 0.7941\n",
      "Epoch 122/150\n",
      "3/3 [==============================] - 0s - loss: 0.0381 - acc: 1.0000 - val_loss: 2.0324 - val_acc: 0.6765\n",
      "Epoch 123/150\n",
      "3/3 [==============================] - 0s - loss: 0.0264 - acc: 1.0000 - val_loss: 2.2081 - val_acc: 0.6471\n",
      "Epoch 124/150\n",
      "3/3 [==============================] - 0s - loss: 0.0741 - acc: 0.9770 - val_loss: 1.8004 - val_acc: 0.6471\n",
      "Epoch 125/150\n",
      "3/3 [==============================] - 0s - loss: 0.0380 - acc: 1.0000 - val_loss: 2.0932 - val_acc: 0.6765\n",
      "Epoch 126/150\n",
      "3/3 [==============================] - 0s - loss: 0.0313 - acc: 0.9770 - val_loss: 2.6802 - val_acc: 0.5882\n",
      "Epoch 127/150\n",
      "3/3 [==============================] - 0s - loss: 0.0265 - acc: 0.9770 - val_loss: 2.6826 - val_acc: 0.6765\n",
      "Epoch 128/150\n",
      "3/3 [==============================] - 0s - loss: 0.0386 - acc: 0.9770 - val_loss: 2.5479 - val_acc: 0.5588\n",
      "Epoch 129/150\n",
      "3/3 [==============================] - 0s - loss: 0.0049 - acc: 1.0000 - val_loss: 2.5779 - val_acc: 0.6471\n",
      "Epoch 130/150\n",
      "3/3 [==============================] - 0s - loss: 0.0041 - acc: 1.0000 - val_loss: 1.6584 - val_acc: 0.5588\n",
      "Epoch 131/150\n",
      "3/3 [==============================] - 0s - loss: 0.0192 - acc: 1.0000 - val_loss: 1.9457 - val_acc: 0.6176\n",
      "Epoch 132/150\n",
      "3/3 [==============================] - 0s - loss: 0.0458 - acc: 0.9540 - val_loss: 2.4362 - val_acc: 0.6471\n",
      "Epoch 133/150\n",
      "3/3 [==============================] - 0s - loss: 0.1078 - acc: 0.9583 - val_loss: 2.3373 - val_acc: 0.6176\n",
      "Epoch 134/150\n",
      "3/3 [==============================] - 0s - loss: 0.0161 - acc: 1.0000 - val_loss: 2.4550 - val_acc: 0.6176\n",
      "Epoch 135/150\n",
      "3/3 [==============================] - 0s - loss: 0.0131 - acc: 1.0000 - val_loss: 1.7543 - val_acc: 0.6471\n",
      "Epoch 136/150\n",
      "3/3 [==============================] - 0s - loss: 0.0037 - acc: 1.0000 - val_loss: 2.4079 - val_acc: 0.6176\n",
      "Epoch 137/150\n",
      "3/3 [==============================] - 0s - loss: 0.0136 - acc: 1.0000 - val_loss: 3.0619 - val_acc: 0.5294\n",
      "Epoch 138/150\n",
      "3/3 [==============================] - 0s - loss: 0.0060 - acc: 1.0000 - val_loss: 0.6654 - val_acc: 0.7647\n",
      "Epoch 139/150\n",
      "3/3 [==============================] - 0s - loss: 0.0052 - acc: 1.0000 - val_loss: 2.3985 - val_acc: 0.5882\n",
      "Epoch 140/150\n",
      "3/3 [==============================] - 0s - loss: 0.0018 - acc: 1.0000 - val_loss: 2.0152 - val_acc: 0.6765\n",
      "Epoch 141/150\n",
      "3/3 [==============================] - 0s - loss: 0.0031 - acc: 1.0000 - val_loss: 2.3424 - val_acc: 0.6176\n",
      "Epoch 142/150\n",
      "3/3 [==============================] - 0s - loss: 0.0086 - acc: 1.0000 - val_loss: 1.8040 - val_acc: 0.6471\n",
      "Epoch 143/150\n",
      "3/3 [==============================] - 0s - loss: 0.0125 - acc: 1.0000 - val_loss: 1.4037 - val_acc: 0.8235\n",
      "Epoch 144/150\n",
      "3/3 [==============================] - 0s - loss: 0.0047 - acc: 1.0000 - val_loss: 2.0067 - val_acc: 0.6765\n",
      "Epoch 145/150\n",
      "3/3 [==============================] - 0s - loss: 0.0069 - acc: 1.0000 - val_loss: 2.5480 - val_acc: 0.6765\n",
      "Epoch 146/150\n",
      "3/3 [==============================] - 0s - loss: 0.0161 - acc: 1.0000 - val_loss: 2.0020 - val_acc: 0.7353\n",
      "Epoch 147/150\n",
      "3/3 [==============================] - 0s - loss: 0.0017 - acc: 1.0000 - val_loss: 1.6852 - val_acc: 0.6765\n",
      "Epoch 148/150\n",
      "3/3 [==============================] - 0s - loss: 0.0216 - acc: 1.0000 - val_loss: 2.9308 - val_acc: 0.6176\n",
      "Epoch 149/150\n",
      "3/3 [==============================] - 0s - loss: 0.0018 - acc: 1.0000 - val_loss: 2.2070 - val_acc: 0.7059\n",
      "Epoch 150/150\n",
      "3/3 [==============================] - 0s - loss: 0.0035 - acc: 1.0000 - val_loss: 2.9091 - val_acc: 0.6176\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1bbd8c18748>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nFJRSSjloUlBKKeXw/wFvBAvWVbTS2QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1bbd85712e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# preprocess\n",
    "rescale_factor = 255\n",
    "train_datagen = ImageDataGenerator(featurewise_center=True, featurewise_std_normalization=True\n",
    "                                   , horizontal_flip=True)\n",
    "train_datagen.fit(x_train)\n",
    "\n",
    "test_datagen = ImageDataGenerator(featurewise_center=True, featurewise_std_normalization=True)\n",
    "test_datagen.fit(x_train)\n",
    "\n",
    "# build model\n",
    "model_name = '1st_place'\n",
    "model = Sequential(name=model_name)\n",
    "model.add(Cropping2D(cropping=3, input_shape=image_shape))\n",
    "model.add(Conv2D(32, (5,5), padding='same', activation='relu', kernel_initializer='TruncatedNormal'))\n",
    "model.add(MaxPooling2D((2,2), strides=(2,2), padding='valid'))\n",
    "\n",
    "model.add(ZeroPadding2D(padding=1))\n",
    "model.add(Conv2D(32, (4,4), padding='valid', activation='relu', kernel_initializer='TruncatedNormal'))\n",
    "model.add(AveragePooling2D((2,2), strides=(2,2), padding='valid'))\n",
    "\n",
    "model.add(Conv2D(64, (5,5), padding='same', activation='relu', kernel_initializer='TruncatedNormal'))\n",
    "model.add(AveragePooling2D((2,2), strides=(2,2), padding='valid'))\n",
    "\n",
    "model.add(Flatten())\n",
    "# TODO is this where dropout should be?\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(3072, activation='relu', kernel_initializer='TruncatedNormal'))\n",
    "# TODO svm activation function\n",
    "model.add(Dense(n_classes, activation='softmax', kernel_initializer='TruncatedNormal'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# compile model\n",
    "sgd = optimizers.SGD(momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "# fit model\n",
    "epoch_sets = 1\n",
    "epochs = 150\n",
    "batch_size = 16\n",
    "for es in range(epoch_sets):\n",
    "    # get current lr\n",
    "    lr = k.get_value(sgd.lr)\n",
    "    print('epoch set:', es, 'lr value:', lr)\n",
    "    \n",
    "    # fit model\n",
    "    history = model.fit_generator(train_datagen.flow(x_train, y_train_onehot, batch_size=batch_size)\n",
    "                        , steps_per_epoch=n_train//batch_size\n",
    "                        , epochs=epochs\n",
    "                        , validation_data=test_datagen.flow(x_test, y_test_onehot, batch_size=batch_size)\n",
    "                        , validation_steps=n_test//batch_size)\n",
    "    # decay lr by half\n",
    "    lr = lr / 2\n",
    "    k.set_value(sgd.lr, lr)\n",
    "    \n",
    "# visualize loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocess: manual center & normalize + sgd_decay on tiny subset + batchsize=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "cropping2d_6 (Cropping2D)    (None, 42, 42, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_16 (Conv2D)           (None, 42, 42, 32)        832       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 21, 21, 32)        0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_6 (ZeroPaddin (None, 23, 23, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 20, 20, 32)        16416     \n",
      "_________________________________________________________________\n",
      "average_pooling2d_11 (Averag (None, 10, 10, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_18 (Conv2D)           (None, 10, 10, 64)        51264     \n",
      "_________________________________________________________________\n",
      "average_pooling2d_12 (Averag (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 3072)              4918272   \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 7)                 21511     \n",
      "=================================================================\n",
      "Total params: 5,008,295\n",
      "Trainable params: 5,008,295\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "epoch set: 0 lr value: 0.01\n",
      "Train on 50 samples, validate on 50 samples\n",
      "Epoch 1/150\n",
      "50/50 [==============================] - 0s - loss: 2.0538 - acc: 0.0600 - val_loss: 4.2843 - val_acc: 0.6400\n",
      "Epoch 2/150\n",
      "50/50 [==============================] - 0s - loss: 1.5651 - acc: 0.5600 - val_loss: 5.7413 - val_acc: 0.6400\n",
      "Epoch 3/150\n",
      "50/50 [==============================] - 0s - loss: 1.1133 - acc: 0.6000 - val_loss: 5.8025 - val_acc: 0.6400\n",
      "Epoch 4/150\n",
      "50/50 [==============================] - 0s - loss: 0.7975 - acc: 0.6000 - val_loss: 4.5126 - val_acc: 0.6400\n",
      "Epoch 5/150\n",
      "50/50 [==============================] - 0s - loss: 0.5976 - acc: 0.7400 - val_loss: 4.3757 - val_acc: 0.7000\n",
      "Epoch 6/150\n",
      "50/50 [==============================] - 0s - loss: 0.6646 - acc: 0.5600 - val_loss: 10.0318 - val_acc: 0.3400\n",
      "Epoch 7/150\n",
      "50/50 [==============================] - 0s - loss: 0.8835 - acc: 0.4400 - val_loss: 5.8025 - val_acc: 0.6400\n",
      "Epoch 8/150\n",
      "50/50 [==============================] - 0s - loss: 3.4031 - acc: 0.6000 - val_loss: 2.7473 - val_acc: 0.6400\n",
      "Epoch 9/150\n",
      "50/50 [==============================] - 0s - loss: 0.6232 - acc: 0.7400 - val_loss: 6.1798 - val_acc: 0.3400\n",
      "Epoch 10/150\n",
      "50/50 [==============================] - 0s - loss: 0.7051 - acc: 0.7000 - val_loss: 6.3409 - val_acc: 0.4000\n",
      "Epoch 11/150\n",
      "50/50 [==============================] - 0s - loss: 0.7246 - acc: 0.6400 - val_loss: 5.7549 - val_acc: 0.5200\n",
      "Epoch 12/150\n",
      "50/50 [==============================] - 0s - loss: 0.6880 - acc: 0.7400 - val_loss: 5.4932 - val_acc: 0.5800\n",
      "Epoch 13/150\n",
      "50/50 [==============================] - 0s - loss: 0.5853 - acc: 0.7800 - val_loss: 5.0375 - val_acc: 0.5800\n",
      "Epoch 14/150\n",
      "50/50 [==============================] - 0s - loss: 0.5293 - acc: 0.7800 - val_loss: 4.9924 - val_acc: 0.6200\n",
      "Epoch 15/150\n",
      "50/50 [==============================] - 0s - loss: 0.4944 - acc: 0.7600 - val_loss: 5.0329 - val_acc: 0.6200\n",
      "Epoch 16/150\n",
      "50/50 [==============================] - 0s - loss: 0.4318 - acc: 0.8400 - val_loss: 5.0477 - val_acc: 0.6200\n",
      "Epoch 17/150\n",
      "50/50 [==============================] - 0s - loss: 0.4397 - acc: 0.8200 - val_loss: 4.9800 - val_acc: 0.6600\n",
      "Epoch 18/150\n",
      "50/50 [==============================] - 0s - loss: 0.4153 - acc: 0.7800 - val_loss: 4.2636 - val_acc: 0.6800\n",
      "Epoch 19/150\n",
      "50/50 [==============================] - 0s - loss: 0.4317 - acc: 0.8000 - val_loss: 6.0219 - val_acc: 0.6000\n",
      "Epoch 20/150\n",
      "50/50 [==============================] - 0s - loss: 0.3524 - acc: 0.8600 - val_loss: 5.1624 - val_acc: 0.6800\n",
      "Epoch 21/150\n",
      "50/50 [==============================] - 0s - loss: 0.3883 - acc: 0.8000 - val_loss: 7.6295 - val_acc: 0.5000\n",
      "Epoch 22/150\n",
      "50/50 [==============================] - 0s - loss: 0.5723 - acc: 0.7200 - val_loss: 5.6981 - val_acc: 0.6400\n",
      "Epoch 23/150\n",
      "50/50 [==============================] - 0s - loss: 1.1182 - acc: 0.6400 - val_loss: 7.4310 - val_acc: 0.5000\n",
      "Epoch 24/150\n",
      "50/50 [==============================] - 0s - loss: 0.4189 - acc: 0.7800 - val_loss: 4.8099 - val_acc: 0.6400\n",
      "Epoch 25/150\n",
      "50/50 [==============================] - 0s - loss: 0.3015 - acc: 0.8800 - val_loss: 4.9956 - val_acc: 0.6600\n",
      "Epoch 26/150\n",
      "50/50 [==============================] - 0s - loss: 0.2849 - acc: 0.9000 - val_loss: 5.2130 - val_acc: 0.6000\n",
      "Epoch 27/150\n",
      "50/50 [==============================] - 0s - loss: 0.2939 - acc: 0.9000 - val_loss: 5.4955 - val_acc: 0.6200\n",
      "Epoch 28/150\n",
      "50/50 [==============================] - 0s - loss: 0.2340 - acc: 0.9200 - val_loss: 5.4856 - val_acc: 0.6400\n",
      "Epoch 29/150\n",
      "50/50 [==============================] - 0s - loss: 0.2256 - acc: 0.9000 - val_loss: 5.6751 - val_acc: 0.5800\n",
      "Epoch 30/150\n",
      "50/50 [==============================] - 0s - loss: 0.2048 - acc: 0.9400 - val_loss: 5.7672 - val_acc: 0.6000\n",
      "Epoch 31/150\n",
      "50/50 [==============================] - 0s - loss: 0.1888 - acc: 0.9200 - val_loss: 5.4156 - val_acc: 0.6200\n",
      "Epoch 32/150\n",
      "50/50 [==============================] - 0s - loss: 0.1948 - acc: 0.9600 - val_loss: 6.3767 - val_acc: 0.6000\n",
      "Epoch 33/150\n",
      "50/50 [==============================] - 0s - loss: 0.1700 - acc: 0.9600 - val_loss: 6.0993 - val_acc: 0.6000\n",
      "Epoch 34/150\n",
      "50/50 [==============================] - 0s - loss: 0.1538 - acc: 0.9600 - val_loss: 6.2151 - val_acc: 0.6000\n",
      "Epoch 35/150\n",
      "50/50 [==============================] - 0s - loss: 0.1412 - acc: 0.9600 - val_loss: 6.0902 - val_acc: 0.6000\n",
      "Epoch 36/150\n",
      "50/50 [==============================] - 0s - loss: 0.1184 - acc: 0.9800 - val_loss: 6.2788 - val_acc: 0.6000\n",
      "Epoch 37/150\n",
      "50/50 [==============================] - 0s - loss: 0.1000 - acc: 1.0000 - val_loss: 5.8462 - val_acc: 0.6000\n",
      "Epoch 38/150\n",
      "50/50 [==============================] - 0s - loss: 0.1024 - acc: 0.9800 - val_loss: 5.9281 - val_acc: 0.6200\n",
      "Epoch 39/150\n",
      "50/50 [==============================] - 0s - loss: 0.1161 - acc: 0.9600 - val_loss: 5.6443 - val_acc: 0.6200\n",
      "Epoch 40/150\n",
      "50/50 [==============================] - 0s - loss: 0.0871 - acc: 0.9800 - val_loss: 5.7283 - val_acc: 0.6200\n",
      "Epoch 41/150\n",
      "50/50 [==============================] - 0s - loss: 0.0903 - acc: 0.9800 - val_loss: 5.5527 - val_acc: 0.6400\n",
      "Epoch 42/150\n",
      "50/50 [==============================] - 0s - loss: 0.0625 - acc: 0.9800 - val_loss: 5.5634 - val_acc: 0.6200\n",
      "Epoch 43/150\n",
      "50/50 [==============================] - 0s - loss: 0.0782 - acc: 0.9600 - val_loss: 5.8827 - val_acc: 0.6000\n",
      "Epoch 44/150\n",
      "50/50 [==============================] - 0s - loss: 0.0647 - acc: 0.9600 - val_loss: 5.2752 - val_acc: 0.6600\n",
      "Epoch 45/150\n",
      "50/50 [==============================] - 0s - loss: 0.0509 - acc: 1.0000 - val_loss: 5.6872 - val_acc: 0.6200\n",
      "Epoch 46/150\n",
      "50/50 [==============================] - 0s - loss: 0.0671 - acc: 0.9600 - val_loss: 6.0245 - val_acc: 0.6000\n",
      "Epoch 47/150\n",
      "50/50 [==============================] - 0s - loss: 0.0393 - acc: 0.9800 - val_loss: 6.4738 - val_acc: 0.5800\n",
      "Epoch 48/150\n",
      "50/50 [==============================] - 0s - loss: 0.0475 - acc: 1.0000 - val_loss: 6.1253 - val_acc: 0.6200\n",
      "Epoch 49/150\n",
      "50/50 [==============================] - 0s - loss: 0.0297 - acc: 1.0000 - val_loss: 5.7440 - val_acc: 0.6400\n",
      "Epoch 50/150\n",
      "50/50 [==============================] - 0s - loss: 0.0137 - acc: 1.0000 - val_loss: 5.7002 - val_acc: 0.6400\n",
      "Epoch 51/150\n",
      "50/50 [==============================] - 0s - loss: 0.0341 - acc: 0.9800 - val_loss: 5.7332 - val_acc: 0.6400\n",
      "Epoch 52/150\n",
      "50/50 [==============================] - 0s - loss: 0.0155 - acc: 1.0000 - val_loss: 5.4804 - val_acc: 0.6600\n",
      "Epoch 53/150\n",
      "50/50 [==============================] - 0s - loss: 0.0478 - acc: 0.9800 - val_loss: 5.4802 - val_acc: 0.6600\n",
      "Epoch 54/150\n",
      "50/50 [==============================] - 0s - loss: 0.0349 - acc: 0.9800 - val_loss: 6.1249 - val_acc: 0.6200\n",
      "Epoch 55/150\n",
      "50/50 [==============================] - 0s - loss: 0.0222 - acc: 1.0000 - val_loss: 5.6624 - val_acc: 0.6200\n",
      "Epoch 56/150\n",
      "50/50 [==============================] - 0s - loss: 0.0187 - acc: 1.0000 - val_loss: 5.9752 - val_acc: 0.6200\n",
      "Epoch 57/150\n",
      "50/50 [==============================] - 0s - loss: 0.0292 - acc: 0.9800 - val_loss: 5.6697 - val_acc: 0.6200\n",
      "Epoch 58/150\n",
      "50/50 [==============================] - 0s - loss: 0.0111 - acc: 1.0000 - val_loss: 6.0495 - val_acc: 0.6000\n",
      "Epoch 59/150\n",
      "50/50 [==============================] - 0s - loss: 0.0313 - acc: 0.9800 - val_loss: 5.1633 - val_acc: 0.6800\n",
      "Epoch 60/150\n",
      "50/50 [==============================] - 0s - loss: 0.1062 - acc: 0.9600 - val_loss: 6.0477 - val_acc: 0.6200\n",
      "Epoch 61/150\n",
      "50/50 [==============================] - 0s - loss: 0.0236 - acc: 1.0000 - val_loss: 5.3770 - val_acc: 0.6400\n",
      "Epoch 62/150\n",
      "50/50 [==============================] - 0s - loss: 0.0105 - acc: 1.0000 - val_loss: 5.1193 - val_acc: 0.6600\n",
      "Epoch 63/150\n",
      "50/50 [==============================] - 0s - loss: 0.0229 - acc: 0.9800 - val_loss: 5.6697 - val_acc: 0.6200\n",
      "Epoch 64/150\n",
      "50/50 [==============================] - 0s - loss: 0.0244 - acc: 1.0000 - val_loss: 5.4542 - val_acc: 0.6600\n",
      "Epoch 65/150\n",
      "50/50 [==============================] - 0s - loss: 0.0060 - acc: 1.0000 - val_loss: 5.4802 - val_acc: 0.6600\n",
      "Epoch 66/150\n",
      "50/50 [==============================] - 0s - loss: 0.0111 - acc: 1.0000 - val_loss: 5.6518 - val_acc: 0.6400\n",
      "Epoch 67/150\n",
      "50/50 [==============================] - 0s - loss: 0.0120 - acc: 1.0000 - val_loss: 5.4518 - val_acc: 0.6600\n",
      "Epoch 68/150\n",
      "50/50 [==============================] - 0s - loss: 0.0131 - acc: 1.0000 - val_loss: 5.5502 - val_acc: 0.6400\n",
      "Epoch 69/150\n",
      "50/50 [==============================] - 0s - loss: 0.0124 - acc: 1.0000 - val_loss: 6.2732 - val_acc: 0.6000\n",
      "Epoch 70/150\n",
      "50/50 [==============================] - 0s - loss: 0.0404 - acc: 0.9800 - val_loss: 5.4612 - val_acc: 0.6600\n",
      "Epoch 71/150\n",
      "50/50 [==============================] - 0s - loss: 0.0097 - acc: 1.0000 - val_loss: 5.5306 - val_acc: 0.6200\n",
      "Epoch 72/150\n",
      "50/50 [==============================] - 0s - loss: 0.0030 - acc: 1.0000 - val_loss: 5.9667 - val_acc: 0.6200\n",
      "Epoch 73/150\n",
      "50/50 [==============================] - 0s - loss: 0.0098 - acc: 1.0000 - val_loss: 5.3993 - val_acc: 0.6200\n",
      "Epoch 74/150\n",
      "50/50 [==============================] - 0s - loss: 0.0080 - acc: 1.0000 - val_loss: 5.1085 - val_acc: 0.6800\n",
      "Epoch 75/150\n",
      "50/50 [==============================] - 0s - loss: 0.0393 - acc: 0.9800 - val_loss: 5.9550 - val_acc: 0.6200\n",
      "Epoch 76/150\n",
      "50/50 [==============================] - 0s - loss: 0.0441 - acc: 0.9800 - val_loss: 6.1249 - val_acc: 0.6200\n",
      "Epoch 77/150\n",
      "50/50 [==============================] - 0s - loss: 0.0069 - acc: 1.0000 - val_loss: 6.1249 - val_acc: 0.6200\n",
      "Epoch 78/150\n",
      "50/50 [==============================] - 0s - loss: 0.0661 - acc: 0.9800 - val_loss: 5.4848 - val_acc: 0.6400\n",
      "Epoch 79/150\n",
      "50/50 [==============================] - 0s - loss: 0.0039 - acc: 1.0000 - val_loss: 5.3887 - val_acc: 0.6400\n",
      "Epoch 80/150\n",
      "50/50 [==============================] - 0s - loss: 0.0063 - acc: 1.0000 - val_loss: 5.3328 - val_acc: 0.6400\n",
      "Epoch 81/150\n",
      "50/50 [==============================] - 0s - loss: 0.0162 - acc: 0.9800 - val_loss: 5.2629 - val_acc: 0.6600\n",
      "Epoch 82/150\n",
      "50/50 [==============================] - 0s - loss: 0.0020 - acc: 1.0000 - val_loss: 5.4658 - val_acc: 0.6600\n",
      "Epoch 83/150\n",
      "50/50 [==============================] - 0s - loss: 0.0065 - acc: 1.0000 - val_loss: 5.2768 - val_acc: 0.6600\n",
      "Epoch 84/150\n",
      "50/50 [==============================] - 0s - loss: 0.0057 - acc: 1.0000 - val_loss: 5.4764 - val_acc: 0.6600\n",
      "Epoch 85/150\n",
      "50/50 [==============================] - 0s - loss: 0.0065 - acc: 1.0000 - val_loss: 5.2885 - val_acc: 0.6600\n",
      "Epoch 86/150\n",
      "50/50 [==============================] - 0s - loss: 0.0017 - acc: 1.0000 - val_loss: 5.4146 - val_acc: 0.6600\n",
      "Epoch 87/150\n",
      "50/50 [==============================] - 0s - loss: 0.0016 - acc: 1.0000 - val_loss: 5.4802 - val_acc: 0.6600\n",
      "Epoch 88/150\n",
      "50/50 [==============================] - 0s - loss: 0.0037 - acc: 1.0000 - val_loss: 5.5057 - val_acc: 0.6400\n",
      "Epoch 89/150\n",
      "50/50 [==============================] - 0s - loss: 0.0038 - acc: 1.0000 - val_loss: 5.4803 - val_acc: 0.6600\n",
      "Epoch 90/150\n",
      "50/50 [==============================] - 0s - loss: 0.0181 - acc: 1.0000 - val_loss: 5.8037 - val_acc: 0.6400\n",
      "Epoch 91/150\n",
      "50/50 [==============================] - 0s - loss: 0.0202 - acc: 1.0000 - val_loss: 6.1249 - val_acc: 0.6200\n",
      "Epoch 92/150\n",
      "50/50 [==============================] - 0s - loss: 0.0125 - acc: 1.0000 - val_loss: 6.2248 - val_acc: 0.6000\n",
      "Epoch 93/150\n",
      "50/50 [==============================] - 0s - loss: 0.0172 - acc: 1.0000 - val_loss: 6.1249 - val_acc: 0.6200\n",
      "Epoch 94/150\n",
      "50/50 [==============================] - 0s - loss: 0.0074 - acc: 1.0000 - val_loss: 6.1249 - val_acc: 0.6200\n",
      "Epoch 95/150\n",
      "50/50 [==============================] - 0s - loss: 0.0058 - acc: 1.0000 - val_loss: 6.1256 - val_acc: 0.6200\n",
      "Epoch 96/150\n",
      "50/50 [==============================] - 0s - loss: 0.0032 - acc: 1.0000 - val_loss: 5.9584 - val_acc: 0.6000\n",
      "Epoch 97/150\n",
      "50/50 [==============================] - 0s - loss: 0.0033 - acc: 1.0000 - val_loss: 5.5018 - val_acc: 0.6400\n",
      "Epoch 98/150\n",
      "50/50 [==============================] - 0s - loss: 0.0077 - acc: 1.0000 - val_loss: 5.4917 - val_acc: 0.6600\n",
      "Epoch 99/150\n",
      "50/50 [==============================] - 0s - loss: 0.0055 - acc: 1.0000 - val_loss: 5.4802 - val_acc: 0.6600\n",
      "Epoch 100/150\n",
      "50/50 [==============================] - 0s - loss: 0.0742 - acc: 0.9800 - val_loss: 5.8025 - val_acc: 0.6400\n",
      "Epoch 101/150\n",
      "50/50 [==============================] - 0s - loss: 0.0218 - acc: 0.9800 - val_loss: 5.8025 - val_acc: 0.6400\n",
      "Epoch 102/150\n",
      "50/50 [==============================] - 0s - loss: 0.0288 - acc: 0.9800 - val_loss: 5.4762 - val_acc: 0.6600\n",
      "Epoch 103/150\n",
      "50/50 [==============================] - 0s - loss: 0.0016 - acc: 1.0000 - val_loss: 5.1972 - val_acc: 0.6600\n",
      "Epoch 104/150\n",
      "50/50 [==============================] - 0s - loss: 0.0037 - acc: 1.0000 - val_loss: 5.1581 - val_acc: 0.6800\n",
      "Epoch 105/150\n",
      "50/50 [==============================] - 0s - loss: 0.0056 - acc: 1.0000 - val_loss: 5.3819 - val_acc: 0.6600\n",
      "Epoch 106/150\n",
      "50/50 [==============================] - 0s - loss: 0.0029 - acc: 1.0000 - val_loss: 5.5225 - val_acc: 0.6400\n",
      "Epoch 107/150\n",
      "50/50 [==============================] - 0s - loss: 0.0053 - acc: 1.0000 - val_loss: 5.5922 - val_acc: 0.6400\n",
      "Epoch 108/150\n",
      "50/50 [==============================] - 0s - loss: 0.0038 - acc: 1.0000 - val_loss: 5.8025 - val_acc: 0.6400\n",
      "Epoch 109/150\n",
      "50/50 [==============================] - 0s - loss: 0.0025 - acc: 1.0000 - val_loss: 5.8025 - val_acc: 0.6400\n",
      "Epoch 110/150\n",
      "50/50 [==============================] - 0s - loss: 0.0050 - acc: 1.0000 - val_loss: 5.8025 - val_acc: 0.6400\n",
      "Epoch 111/150\n",
      "50/50 [==============================] - 0s - loss: 0.0039 - acc: 1.0000 - val_loss: 5.6248 - val_acc: 0.6400\n",
      "Epoch 112/150\n",
      "50/50 [==============================] - 0s - loss: 0.0028 - acc: 1.0000 - val_loss: 5.2460 - val_acc: 0.6400\n",
      "Epoch 113/150\n",
      "50/50 [==============================] - 0s - loss: 0.0019 - acc: 1.0000 - val_loss: 5.2239 - val_acc: 0.6600\n",
      "Epoch 114/150\n",
      "50/50 [==============================] - 0s - loss: 0.0023 - acc: 1.0000 - val_loss: 5.4802 - val_acc: 0.6600\n",
      "Epoch 115/150\n",
      "50/50 [==============================] - 0s - loss: 0.0279 - acc: 0.9800 - val_loss: 5.8028 - val_acc: 0.6400\n",
      "Epoch 116/150\n",
      "50/50 [==============================] - 0s - loss: 0.0078 - acc: 1.0000 - val_loss: 5.5262 - val_acc: 0.6400\n",
      "Epoch 117/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50/50 [==============================] - 0s - loss: 0.0182 - acc: 1.0000 - val_loss: 5.3973 - val_acc: 0.6600\n",
      "Epoch 118/150\n",
      "50/50 [==============================] - 0s - loss: 0.0018 - acc: 1.0000 - val_loss: 5.1620 - val_acc: 0.6800\n",
      "Epoch 119/150\n",
      "50/50 [==============================] - 0s - loss: 0.0055 - acc: 1.0000 - val_loss: 5.1578 - val_acc: 0.6800\n",
      "Epoch 120/150\n",
      "50/50 [==============================] - 0s - loss: 0.0351 - acc: 0.9800 - val_loss: 6.3105 - val_acc: 0.6000\n",
      "Epoch 121/150\n",
      "50/50 [==============================] - 0s - loss: 0.0078 - acc: 1.0000 - val_loss: 6.3962 - val_acc: 0.6000\n",
      "Epoch 122/150\n",
      "50/50 [==============================] - 0s - loss: 0.1504 - acc: 0.9600 - val_loss: 6.7696 - val_acc: 0.5800\n",
      "Epoch 123/150\n",
      "50/50 [==============================] - 0s - loss: 0.0419 - acc: 0.9800 - val_loss: 6.7709 - val_acc: 0.5800\n",
      "Epoch 124/150\n",
      "50/50 [==============================] - 0s - loss: 0.0083 - acc: 1.0000 - val_loss: 6.4472 - val_acc: 0.6000\n",
      "Epoch 125/150\n",
      "50/50 [==============================] - 0s - loss: 0.0586 - acc: 0.9600 - val_loss: 4.8355 - val_acc: 0.7000\n",
      "Epoch 126/150\n",
      "50/50 [==============================] - 0s - loss: 0.0156 - acc: 1.0000 - val_loss: 4.5147 - val_acc: 0.7200\n",
      "Epoch 127/150\n",
      "50/50 [==============================] - 0s - loss: 0.0024 - acc: 1.0000 - val_loss: 4.2262 - val_acc: 0.7200\n",
      "Epoch 128/150\n",
      "50/50 [==============================] - 0s - loss: 0.0066 - acc: 1.0000 - val_loss: 4.1907 - val_acc: 0.7400\n",
      "Epoch 129/150\n",
      "50/50 [==============================] - 0s - loss: 0.0322 - acc: 0.9800 - val_loss: 5.1578 - val_acc: 0.6800\n",
      "Epoch 130/150\n",
      "50/50 [==============================] - 0s - loss: 0.0032 - acc: 1.0000 - val_loss: 5.1578 - val_acc: 0.6800\n",
      "Epoch 131/150\n",
      "50/50 [==============================] - 0s - loss: 0.0062 - acc: 1.0000 - val_loss: 5.4880 - val_acc: 0.6400\n",
      "Epoch 132/150\n",
      "50/50 [==============================] - 0s - loss: 0.0046 - acc: 1.0000 - val_loss: 5.9929 - val_acc: 0.6200\n",
      "Epoch 133/150\n",
      "50/50 [==============================] - 0s - loss: 0.0024 - acc: 1.0000 - val_loss: 5.8033 - val_acc: 0.6400\n",
      "Epoch 134/150\n",
      "50/50 [==============================] - 0s - loss: 0.0044 - acc: 1.0000 - val_loss: 5.8025 - val_acc: 0.6400\n",
      "Epoch 135/150\n",
      "50/50 [==============================] - 0s - loss: 0.0339 - acc: 0.9800 - val_loss: 5.7981 - val_acc: 0.6400\n",
      "Epoch 136/150\n",
      "50/50 [==============================] - 0s - loss: 0.0503 - acc: 0.9800 - val_loss: 4.8431 - val_acc: 0.7000\n",
      "Epoch 137/150\n",
      "50/50 [==============================] - 0s - loss: 0.0026 - acc: 1.0000 - val_loss: 4.6215 - val_acc: 0.7000\n",
      "Epoch 138/150\n",
      "50/50 [==============================] - 0s - loss: 0.0027 - acc: 1.0000 - val_loss: 4.1907 - val_acc: 0.7400\n",
      "Epoch 139/150\n",
      "50/50 [==============================] - 0s - loss: 0.0063 - acc: 1.0000 - val_loss: 4.8529 - val_acc: 0.6600\n",
      "Epoch 140/150\n",
      "50/50 [==============================] - 0s - loss: 0.0030 - acc: 1.0000 - val_loss: 5.6132 - val_acc: 0.6400\n",
      "Epoch 141/150\n",
      "50/50 [==============================] - 0s - loss: 0.0044 - acc: 1.0000 - val_loss: 5.8025 - val_acc: 0.6400\n",
      "Epoch 142/150\n",
      "50/50 [==============================] - 0s - loss: 0.0073 - acc: 1.0000 - val_loss: 5.8025 - val_acc: 0.6400\n",
      "Epoch 143/150\n",
      "50/50 [==============================] - 0s - loss: 0.0038 - acc: 1.0000 - val_loss: 5.8025 - val_acc: 0.6400\n",
      "Epoch 144/150\n",
      "50/50 [==============================] - 0s - loss: 0.0016 - acc: 1.0000 - val_loss: 5.8025 - val_acc: 0.6400\n",
      "Epoch 145/150\n",
      "50/50 [==============================] - 0s - loss: 0.0054 - acc: 1.0000 - val_loss: 5.8025 - val_acc: 0.6400\n",
      "Epoch 146/150\n",
      "50/50 [==============================] - 0s - loss: 0.0530 - acc: 0.9800 - val_loss: 6.0073 - val_acc: 0.6200\n",
      "Epoch 147/150\n",
      "50/50 [==============================] - 0s - loss: 0.0049 - acc: 1.0000 - val_loss: 6.4648 - val_acc: 0.5800\n",
      "Epoch 148/150\n",
      "50/50 [==============================] - 0s - loss: 0.0030 - acc: 1.0000 - val_loss: 6.9452 - val_acc: 0.5600\n",
      "Epoch 149/150\n",
      "50/50 [==============================] - 0s - loss: 0.2341 - acc: 0.9800 - val_loss: 5.8495 - val_acc: 0.6200\n",
      "Epoch 150/150\n",
      "50/50 [==============================] - 0s - loss: 0.0029 - acc: 1.0000 - val_loss: 6.1249 - val_acc: 0.6200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1bb80594fd0>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEKCAYAAAAfGVI8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n8/8BQ9eCIamZabUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1bb80130278>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# preprocess\n",
    "x_train_pp = x_train - np.mean(x_train)\n",
    "x_train_pp /= np.std(x_train_pp, axis=0)\n",
    "\n",
    "x_test_pp  = x_test - np.mean(x_train) \n",
    "x_test_pp /= np.std(x_train_pp, axis=0)\n",
    "\n",
    "# build model\n",
    "model_name = '1st_place'\n",
    "model = Sequential(name=model_name)\n",
    "model.add(Cropping2D(cropping=3, input_shape=image_shape))\n",
    "model.add(Conv2D(32, (5,5), padding='same', activation='relu', kernel_initializer='TruncatedNormal'))\n",
    "model.add(MaxPooling2D((2,2), strides=(2,2), padding='valid'))\n",
    "\n",
    "model.add(ZeroPadding2D(padding=1))\n",
    "model.add(Conv2D(32, (4,4), padding='valid', activation='relu', kernel_initializer='TruncatedNormal'))\n",
    "model.add(AveragePooling2D((2,2), strides=(2,2), padding='valid'))\n",
    "\n",
    "model.add(Conv2D(64, (5,5), padding='same', activation='relu', kernel_initializer='TruncatedNormal'))\n",
    "model.add(AveragePooling2D((2,2), strides=(2,2), padding='valid'))\n",
    "\n",
    "model.add(Flatten())\n",
    "# TODO is this where dropout should be?\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(3072, activation='relu', kernel_initializer='TruncatedNormal'))\n",
    "# TODO svm activation function\n",
    "model.add(Dense(n_classes, activation='softmax', kernel_initializer='TruncatedNormal'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# compile model\n",
    "sgd = optimizers.SGD(momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "# fit model\n",
    "epoch_sets = 1\n",
    "epochs = 150\n",
    "batch_size = 50\n",
    "for es in range(epoch_sets):\n",
    "    # get current lr\n",
    "    lr = k.get_value(sgd.lr)\n",
    "    print('epoch set:', es, 'lr value:', lr)\n",
    "    \n",
    "    # fit model\n",
    "    history = model.fit(x_train_pp, y_train_onehot\n",
    "                        , batch_size = batch_size\n",
    "                        , epochs=epochs\n",
    "                        , validation_data=(x_test_pp, y_test_onehot))\n",
    "    # decay lr by half\n",
    "    lr = lr / 2\n",
    "    k.set_value(sgd.lr, lr)\n",
    "    \n",
    "# visualize loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocess: manual center & normalize + adam on tiny subset + batchsize=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "cropping2d_7 (Cropping2D)    (None, 42, 42, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_19 (Conv2D)           (None, 42, 42, 32)        832       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 21, 21, 32)        0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_7 (ZeroPaddin (None, 23, 23, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_20 (Conv2D)           (None, 20, 20, 32)        16416     \n",
      "_________________________________________________________________\n",
      "average_pooling2d_13 (Averag (None, 10, 10, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_21 (Conv2D)           (None, 10, 10, 64)        51264     \n",
      "_________________________________________________________________\n",
      "average_pooling2d_14 (Averag (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 3072)              4918272   \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 7)                 21511     \n",
      "=================================================================\n",
      "Total params: 5,008,295\n",
      "Trainable params: 5,008,295\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "epoch set: 0 lr value: 0.005\n",
      "Train on 50 samples, validate on 50 samples\n",
      "Epoch 1/150\n",
      "50/50 [==============================] - 0s - loss: 1.7612 - acc: 0.2600 - val_loss: 5.8025 - val_acc: 0.6400\n",
      "Epoch 2/150\n",
      "50/50 [==============================] - 0s - loss: 0.9852 - acc: 0.6000 - val_loss: 10.3156 - val_acc: 0.3600\n",
      "Epoch 3/150\n",
      "50/50 [==============================] - 0s - loss: 2.2444 - acc: 0.4000 - val_loss: 9.5984 - val_acc: 0.3600\n",
      "Epoch 4/150\n",
      "50/50 [==============================] - 0s - loss: 0.7429 - acc: 0.5000 - val_loss: 5.8025 - val_acc: 0.6400\n",
      "Epoch 5/150\n",
      "50/50 [==============================] - 0s - loss: 0.6601 - acc: 0.6000 - val_loss: 5.8025 - val_acc: 0.6400\n",
      "Epoch 6/150\n",
      "50/50 [==============================] - 0s - loss: 0.8023 - acc: 0.6000 - val_loss: 5.8025 - val_acc: 0.6400\n",
      "Epoch 7/150\n",
      "50/50 [==============================] - 0s - loss: 0.7649 - acc: 0.6000 - val_loss: 5.8025 - val_acc: 0.6400\n",
      "Epoch 8/150\n",
      "50/50 [==============================] - 0s - loss: 0.6999 - acc: 0.6000 - val_loss: 5.8025 - val_acc: 0.6400\n",
      "Epoch 9/150\n",
      "50/50 [==============================] - 0s - loss: 0.6786 - acc: 0.6400 - val_loss: 4.3070 - val_acc: 0.6000\n",
      "Epoch 10/150\n",
      "50/50 [==============================] - 0s - loss: 0.6345 - acc: 0.7600 - val_loss: 4.0243 - val_acc: 0.6200\n",
      "Epoch 11/150\n",
      "50/50 [==============================] - 0s - loss: 0.5860 - acc: 0.6800 - val_loss: 5.0383 - val_acc: 0.6000\n",
      "Epoch 12/150\n",
      "50/50 [==============================] - 0s - loss: 0.5170 - acc: 0.7600 - val_loss: 5.1011 - val_acc: 0.5400\n",
      "Epoch 13/150\n",
      "50/50 [==============================] - 0s - loss: 0.4668 - acc: 0.8200 - val_loss: 5.3962 - val_acc: 0.6000\n",
      "Epoch 14/150\n",
      "50/50 [==============================] - 0s - loss: 0.4022 - acc: 0.8200 - val_loss: 5.9586 - val_acc: 0.5600\n",
      "Epoch 15/150\n",
      "50/50 [==============================] - 0s - loss: 0.3709 - acc: 0.8400 - val_loss: 5.8784 - val_acc: 0.5800\n",
      "Epoch 16/150\n",
      "50/50 [==============================] - 0s - loss: 0.3017 - acc: 0.8800 - val_loss: 6.3051 - val_acc: 0.5400\n",
      "Epoch 17/150\n",
      "50/50 [==============================] - 0s - loss: 0.2839 - acc: 0.8600 - val_loss: 5.7949 - val_acc: 0.6000\n",
      "Epoch 18/150\n",
      "50/50 [==============================] - 0s - loss: 0.2684 - acc: 0.9200 - val_loss: 5.4814 - val_acc: 0.6600\n",
      "Epoch 19/150\n",
      "50/50 [==============================] - 0s - loss: 0.2646 - acc: 0.8600 - val_loss: 5.3324 - val_acc: 0.6600\n",
      "Epoch 20/150\n",
      "50/50 [==============================] - 0s - loss: 0.2245 - acc: 0.9200 - val_loss: 7.0050 - val_acc: 0.5600\n",
      "Epoch 21/150\n",
      "50/50 [==============================] - 0s - loss: 0.1870 - acc: 0.9400 - val_loss: 7.0066 - val_acc: 0.5600\n",
      "Epoch 22/150\n",
      "50/50 [==============================] - 0s - loss: 0.1466 - acc: 0.9600 - val_loss: 5.3408 - val_acc: 0.6000\n",
      "Epoch 23/150\n",
      "50/50 [==============================] - 0s - loss: 0.1023 - acc: 0.9600 - val_loss: 5.5772 - val_acc: 0.6200\n",
      "Epoch 24/150\n",
      "50/50 [==============================] - 0s - loss: 0.1168 - acc: 0.9800 - val_loss: 5.4952 - val_acc: 0.6400\n",
      "Epoch 25/150\n",
      "50/50 [==============================] - 0s - loss: 0.1009 - acc: 0.9800 - val_loss: 5.8259 - val_acc: 0.6200\n",
      "Epoch 26/150\n",
      "50/50 [==============================] - 0s - loss: 0.0864 - acc: 0.9800 - val_loss: 6.9144 - val_acc: 0.5600\n",
      "Epoch 27/150\n",
      "50/50 [==============================] - 0s - loss: 0.1216 - acc: 0.9400 - val_loss: 6.3919 - val_acc: 0.5800\n",
      "Epoch 28/150\n",
      "50/50 [==============================] - 0s - loss: 0.0670 - acc: 1.0000 - val_loss: 5.4802 - val_acc: 0.6600\n",
      "Epoch 29/150\n",
      "50/50 [==============================] - 0s - loss: 0.0758 - acc: 0.9800 - val_loss: 4.8437 - val_acc: 0.7000\n",
      "Epoch 30/150\n",
      "50/50 [==============================] - 0s - loss: 0.1054 - acc: 0.9400 - val_loss: 5.4806 - val_acc: 0.6600\n",
      "Epoch 31/150\n",
      "50/50 [==============================] - 0s - loss: 0.0436 - acc: 0.9800 - val_loss: 6.4708 - val_acc: 0.5800\n",
      "Epoch 32/150\n",
      "50/50 [==============================] - 0s - loss: 0.0409 - acc: 0.9800 - val_loss: 7.0657 - val_acc: 0.5600\n",
      "Epoch 33/150\n",
      "50/50 [==============================] - 0s - loss: 0.0330 - acc: 0.9800 - val_loss: 7.1391 - val_acc: 0.5400\n",
      "Epoch 34/150\n",
      "50/50 [==============================] - 0s - loss: 0.0508 - acc: 0.9800 - val_loss: 7.0131 - val_acc: 0.5600\n",
      "Epoch 35/150\n",
      "50/50 [==============================] - 0s - loss: 0.0334 - acc: 0.9800 - val_loss: 6.4053 - val_acc: 0.6000\n",
      "Epoch 36/150\n",
      "50/50 [==============================] - 0s - loss: 0.0236 - acc: 1.0000 - val_loss: 5.4862 - val_acc: 0.6600\n",
      "Epoch 37/150\n",
      "50/50 [==============================] - 0s - loss: 0.0313 - acc: 0.9800 - val_loss: 5.5054 - val_acc: 0.6200\n",
      "Epoch 38/150\n",
      "50/50 [==============================] - 0s - loss: 0.0493 - acc: 0.9800 - val_loss: 6.1033 - val_acc: 0.6200\n",
      "Epoch 39/150\n",
      "50/50 [==============================] - 0s - loss: 0.0181 - acc: 1.0000 - val_loss: 6.4472 - val_acc: 0.6000\n",
      "Epoch 40/150\n",
      "50/50 [==============================] - 0s - loss: 0.0134 - acc: 1.0000 - val_loss: 6.1684 - val_acc: 0.5800\n",
      "Epoch 41/150\n",
      "50/50 [==============================] - 0s - loss: 0.0085 - acc: 1.0000 - val_loss: 6.6151 - val_acc: 0.5800\n",
      "Epoch 42/150\n",
      "50/50 [==============================] - 0s - loss: 0.0147 - acc: 1.0000 - val_loss: 6.1265 - val_acc: 0.6200\n",
      "Epoch 43/150\n",
      "50/50 [==============================] - 0s - loss: 0.0186 - acc: 1.0000 - val_loss: 5.7156 - val_acc: 0.6200\n",
      "Epoch 44/150\n",
      "50/50 [==============================] - 0s - loss: 0.0183 - acc: 1.0000 - val_loss: 5.9810 - val_acc: 0.6200\n",
      "Epoch 45/150\n",
      "50/50 [==============================] - 0s - loss: 0.0098 - acc: 1.0000 - val_loss: 6.3166 - val_acc: 0.5800\n",
      "Epoch 46/150\n",
      "50/50 [==============================] - 0s - loss: 0.0086 - acc: 1.0000 - val_loss: 5.8708 - val_acc: 0.6200\n",
      "Epoch 47/150\n",
      "50/50 [==============================] - 0s - loss: 0.0044 - acc: 1.0000 - val_loss: 5.8025 - val_acc: 0.6400\n",
      "Epoch 48/150\n",
      "50/50 [==============================] - 0s - loss: 0.0076 - acc: 1.0000 - val_loss: 5.9056 - val_acc: 0.6200\n",
      "Epoch 49/150\n",
      "50/50 [==============================] - 0s - loss: 0.0221 - acc: 1.0000 - val_loss: 6.1249 - val_acc: 0.6200\n",
      "Epoch 50/150\n",
      "50/50 [==============================] - 0s - loss: 0.0162 - acc: 1.0000 - val_loss: 5.9908 - val_acc: 0.6200\n",
      "Epoch 51/150\n",
      "50/50 [==============================] - 0s - loss: 0.0115 - acc: 1.0000 - val_loss: 6.4472 - val_acc: 0.6000\n",
      "Epoch 52/150\n",
      "50/50 [==============================] - 0s - loss: 0.0020 - acc: 1.0000 - val_loss: 7.1393 - val_acc: 0.5400\n",
      "Epoch 53/150\n",
      "50/50 [==============================] - 0s - loss: 0.0043 - acc: 1.0000 - val_loss: 7.0612 - val_acc: 0.5600\n",
      "Epoch 54/150\n",
      "50/50 [==============================] - 0s - loss: 0.0074 - acc: 1.0000 - val_loss: 7.0920 - val_acc: 0.5600\n",
      "Epoch 55/150\n",
      "50/50 [==============================] - 0s - loss: 0.0041 - acc: 1.0000 - val_loss: 6.2929 - val_acc: 0.5800\n",
      "Epoch 56/150\n",
      "50/50 [==============================] - 0s - loss: 0.0024 - acc: 1.0000 - val_loss: 6.1666 - val_acc: 0.6000\n",
      "Epoch 57/150\n",
      "50/50 [==============================] - 0s - loss: 0.0023 - acc: 1.0000 - val_loss: 6.2077 - val_acc: 0.6000\n",
      "Epoch 58/150\n",
      "50/50 [==============================] - 0s - loss: 0.0021 - acc: 1.0000 - val_loss: 6.3215 - val_acc: 0.6000\n",
      "Epoch 59/150\n",
      "50/50 [==============================] - 0s - loss: 0.0040 - acc: 1.0000 - val_loss: 6.0652 - val_acc: 0.6000\n",
      "Epoch 60/150\n",
      "50/50 [==============================] - 0s - loss: 0.0019 - acc: 1.0000 - val_loss: 6.1249 - val_acc: 0.6200\n",
      "Epoch 61/150\n",
      "50/50 [==============================] - 0s - loss: 0.0017 - acc: 1.0000 - val_loss: 6.1249 - val_acc: 0.6200\n",
      "Epoch 62/150\n",
      "50/50 [==============================] - 0s - loss: 0.0016 - acc: 1.0000 - val_loss: 5.8026 - val_acc: 0.6400\n",
      "Epoch 63/150\n",
      "50/50 [==============================] - 0s - loss: 3.4248e-04 - acc: 1.0000 - val_loss: 5.9105 - val_acc: 0.6200\n",
      "Epoch 64/150\n",
      "50/50 [==============================] - 0s - loss: 0.0023 - acc: 1.0000 - val_loss: 6.0556 - val_acc: 0.6200\n",
      "Epoch 65/150\n",
      "50/50 [==============================] - 0s - loss: 0.0034 - acc: 1.0000 - val_loss: 6.1249 - val_acc: 0.6200\n",
      "Epoch 66/150\n",
      "50/50 [==============================] - 0s - loss: 8.1600e-04 - acc: 1.0000 - val_loss: 6.1249 - val_acc: 0.6200\n",
      "Epoch 67/150\n",
      "50/50 [==============================] - 0s - loss: 9.8930e-04 - acc: 1.0000 - val_loss: 6.1249 - val_acc: 0.6200\n",
      "Epoch 68/150\n",
      "50/50 [==============================] - 0s - loss: 7.0134e-04 - acc: 1.0000 - val_loss: 6.1249 - val_acc: 0.6200\n",
      "Epoch 69/150\n",
      "50/50 [==============================] - 0s - loss: 0.0014 - acc: 1.0000 - val_loss: 6.1376 - val_acc: 0.6200\n",
      "Epoch 70/150\n",
      "50/50 [==============================] - 0s - loss: 0.0059 - acc: 1.0000 - val_loss: 6.1258 - val_acc: 0.6200\n",
      "Epoch 71/150\n",
      "50/50 [==============================] - 0s - loss: 0.0025 - acc: 1.0000 - val_loss: 6.2659 - val_acc: 0.6000\n",
      "Epoch 72/150\n",
      "50/50 [==============================] - 0s - loss: 0.0018 - acc: 1.0000 - val_loss: 6.4860 - val_acc: 0.5800\n",
      "Epoch 73/150\n",
      "50/50 [==============================] - 0s - loss: 1.7917e-04 - acc: 1.0000 - val_loss: 6.8478 - val_acc: 0.5600\n",
      "Epoch 74/150\n",
      "50/50 [==============================] - 0s - loss: 4.6717e-04 - acc: 1.0000 - val_loss: 7.1622 - val_acc: 0.5400\n",
      "Epoch 75/150\n",
      "50/50 [==============================] - 0s - loss: 4.3620e-04 - acc: 1.0000 - val_loss: 7.1831 - val_acc: 0.5400\n",
      "Epoch 76/150\n",
      "50/50 [==============================] - 0s - loss: 0.0029 - acc: 1.0000 - val_loss: 7.1330 - val_acc: 0.5400\n",
      "Epoch 77/150\n",
      "50/50 [==============================] - 0s - loss: 2.4459e-04 - acc: 1.0000 - val_loss: 7.0900 - val_acc: 0.5400\n",
      "Epoch 78/150\n",
      "50/50 [==============================] - 0s - loss: 8.7012e-04 - acc: 1.0000 - val_loss: 7.0700 - val_acc: 0.5600\n",
      "Epoch 79/150\n",
      "50/50 [==============================] - 0s - loss: 7.2355e-04 - acc: 1.0000 - val_loss: 7.0920 - val_acc: 0.5600\n",
      "Epoch 80/150\n",
      "50/50 [==============================] - 0s - loss: 6.6132e-04 - acc: 1.0000 - val_loss: 7.1294 - val_acc: 0.5400\n",
      "Epoch 81/150\n",
      "50/50 [==============================] - 0s - loss: 8.7155e-04 - acc: 1.0000 - val_loss: 7.1051 - val_acc: 0.5600\n",
      "Epoch 82/150\n",
      "50/50 [==============================] - 0s - loss: 0.0014 - acc: 1.0000 - val_loss: 7.0920 - val_acc: 0.5600\n",
      "Epoch 83/150\n",
      "50/50 [==============================] - 0s - loss: 2.0814e-04 - acc: 1.0000 - val_loss: 7.0920 - val_acc: 0.5600\n",
      "Epoch 84/150\n",
      "50/50 [==============================] - 0s - loss: 3.3221e-04 - acc: 1.0000 - val_loss: 7.0920 - val_acc: 0.5600\n",
      "Epoch 85/150\n",
      "50/50 [==============================] - 0s - loss: 1.2973e-04 - acc: 1.0000 - val_loss: 7.0920 - val_acc: 0.5600\n",
      "Epoch 86/150\n",
      "50/50 [==============================] - 0s - loss: 1.5223e-04 - acc: 1.0000 - val_loss: 7.0920 - val_acc: 0.5600\n",
      "Epoch 87/150\n",
      "50/50 [==============================] - 0s - loss: 1.5324e-04 - acc: 1.0000 - val_loss: 7.0920 - val_acc: 0.5600\n",
      "Epoch 88/150\n",
      "50/50 [==============================] - 0s - loss: 2.0904e-04 - acc: 1.0000 - val_loss: 7.0920 - val_acc: 0.5600\n",
      "Epoch 89/150\n",
      "50/50 [==============================] - 0s - loss: 1.7734e-04 - acc: 1.0000 - val_loss: 7.0920 - val_acc: 0.5600\n",
      "Epoch 90/150\n",
      "50/50 [==============================] - 0s - loss: 1.5684e-04 - acc: 1.0000 - val_loss: 7.0920 - val_acc: 0.5600\n",
      "Epoch 91/150\n",
      "50/50 [==============================] - 0s - loss: 3.2626e-04 - acc: 1.0000 - val_loss: 7.0920 - val_acc: 0.5600\n",
      "Epoch 92/150\n",
      "50/50 [==============================] - 0s - loss: 1.6634e-04 - acc: 1.0000 - val_loss: 7.0920 - val_acc: 0.5600\n",
      "Epoch 93/150\n",
      "50/50 [==============================] - 0s - loss: 7.5580e-05 - acc: 1.0000 - val_loss: 7.0920 - val_acc: 0.5600\n",
      "Epoch 94/150\n",
      "50/50 [==============================] - 0s - loss: 1.6691e-04 - acc: 1.0000 - val_loss: 7.0920 - val_acc: 0.5600\n",
      "Epoch 95/150\n",
      "50/50 [==============================] - 0s - loss: 7.1424e-04 - acc: 1.0000 - val_loss: 7.0527 - val_acc: 0.5600\n",
      "Epoch 96/150\n",
      "50/50 [==============================] - 0s - loss: 4.6426e-05 - acc: 1.0000 - val_loss: 6.9966 - val_acc: 0.5600\n",
      "Epoch 97/150\n",
      "50/50 [==============================] - 0s - loss: 2.5661e-04 - acc: 1.0000 - val_loss: 6.9504 - val_acc: 0.5600\n",
      "Epoch 98/150\n",
      "50/50 [==============================] - 0s - loss: 8.3115e-05 - acc: 1.0000 - val_loss: 6.9095 - val_acc: 0.5600\n",
      "Epoch 99/150\n",
      "50/50 [==============================] - 0s - loss: 1.2456e-04 - acc: 1.0000 - val_loss: 6.8696 - val_acc: 0.5600\n",
      "Epoch 100/150\n",
      "50/50 [==============================] - 0s - loss: 2.9394e-05 - acc: 1.0000 - val_loss: 6.8340 - val_acc: 0.5600\n",
      "Epoch 101/150\n",
      "50/50 [==============================] - 0s - loss: 1.3947e-04 - acc: 1.0000 - val_loss: 6.7959 - val_acc: 0.5600\n",
      "Epoch 102/150\n",
      "50/50 [==============================] - 0s - loss: 2.5248e-04 - acc: 1.0000 - val_loss: 6.7741 - val_acc: 0.5800\n",
      "Epoch 103/150\n",
      "50/50 [==============================] - 0s - loss: 2.7091e-04 - acc: 1.0000 - val_loss: 6.7700 - val_acc: 0.5800\n",
      "Epoch 104/150\n",
      "50/50 [==============================] - 0s - loss: 1.0000e-04 - acc: 1.0000 - val_loss: 6.7696 - val_acc: 0.5800\n",
      "Epoch 105/150\n",
      "50/50 [==============================] - 0s - loss: 1.6085e-04 - acc: 1.0000 - val_loss: 6.7696 - val_acc: 0.5800\n",
      "Epoch 106/150\n",
      "50/50 [==============================] - 0s - loss: 9.5889e-05 - acc: 1.0000 - val_loss: 6.7696 - val_acc: 0.5800\n",
      "Epoch 107/150\n",
      "50/50 [==============================] - 0s - loss: 1.5225e-04 - acc: 1.0000 - val_loss: 6.7696 - val_acc: 0.5800\n",
      "Epoch 108/150\n",
      "50/50 [==============================] - 0s - loss: 4.8450e-05 - acc: 1.0000 - val_loss: 6.7430 - val_acc: 0.5800\n",
      "Epoch 109/150\n",
      "50/50 [==============================] - 0s - loss: 2.8288e-05 - acc: 1.0000 - val_loss: 6.6882 - val_acc: 0.5800\n",
      "Epoch 110/150\n",
      "50/50 [==============================] - 0s - loss: 4.3559e-05 - acc: 1.0000 - val_loss: 6.6370 - val_acc: 0.5800\n",
      "Epoch 111/150\n",
      "50/50 [==============================] - 0s - loss: 8.4207e-05 - acc: 1.0000 - val_loss: 6.5944 - val_acc: 0.5800\n",
      "Epoch 112/150\n",
      "50/50 [==============================] - 0s - loss: 7.4458e-05 - acc: 1.0000 - val_loss: 6.5572 - val_acc: 0.5800\n",
      "Epoch 113/150\n",
      "50/50 [==============================] - 0s - loss: 8.9973e-05 - acc: 1.0000 - val_loss: 6.5285 - val_acc: 0.5800\n",
      "Epoch 114/150\n",
      "50/50 [==============================] - 0s - loss: 3.2048e-05 - acc: 1.0000 - val_loss: 6.5034 - val_acc: 0.5800\n",
      "Epoch 115/150\n",
      "50/50 [==============================] - 0s - loss: 7.7400e-05 - acc: 1.0000 - val_loss: 6.4812 - val_acc: 0.5800\n",
      "Epoch 116/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50/50 [==============================] - 0s - loss: 3.0129e-04 - acc: 1.0000 - val_loss: 6.4722 - val_acc: 0.5800\n",
      "Epoch 117/150\n",
      "50/50 [==============================] - 0s - loss: 6.0800e-05 - acc: 1.0000 - val_loss: 6.4635 - val_acc: 0.5800\n",
      "Epoch 118/150\n",
      "50/50 [==============================] - 0s - loss: 0.0012 - acc: 1.0000 - val_loss: 6.5374 - val_acc: 0.5800\n",
      "Epoch 119/150\n",
      "50/50 [==============================] - 0s - loss: 5.7916e-05 - acc: 1.0000 - val_loss: 6.4237 - val_acc: 0.5800\n",
      "Epoch 120/150\n",
      "50/50 [==============================] - 0s - loss: 2.7179e-04 - acc: 1.0000 - val_loss: 6.3623 - val_acc: 0.6000\n",
      "Epoch 121/150\n",
      "50/50 [==============================] - 0s - loss: 6.9952e-05 - acc: 1.0000 - val_loss: 6.4810 - val_acc: 0.5800\n",
      "Epoch 122/150\n",
      "50/50 [==============================] - 0s - loss: 6.0574e-05 - acc: 1.0000 - val_loss: 6.4464 - val_acc: 0.5800\n",
      "Epoch 123/150\n",
      "50/50 [==============================] - 0s - loss: 1.2121e-04 - acc: 1.0000 - val_loss: 6.3608 - val_acc: 0.6000\n",
      "Epoch 124/150\n",
      "50/50 [==============================] - 0s - loss: 1.4554e-04 - acc: 1.0000 - val_loss: 6.4142 - val_acc: 0.6000\n",
      "Epoch 125/150\n",
      "50/50 [==============================] - 0s - loss: 3.3949e-04 - acc: 1.0000 - val_loss: 6.4309 - val_acc: 0.6000\n",
      "Epoch 126/150\n",
      "50/50 [==============================] - 0s - loss: 8.9174e-05 - acc: 1.0000 - val_loss: 6.4403 - val_acc: 0.6000\n",
      "Epoch 127/150\n",
      "50/50 [==============================] - 0s - loss: 1.3828e-04 - acc: 1.0000 - val_loss: 6.4173 - val_acc: 0.6000\n",
      "Epoch 128/150\n",
      "50/50 [==============================] - 0s - loss: 3.3812e-05 - acc: 1.0000 - val_loss: 6.3906 - val_acc: 0.6000\n",
      "Epoch 129/150\n",
      "50/50 [==============================] - 0s - loss: 3.1859e-04 - acc: 1.0000 - val_loss: 6.3414 - val_acc: 0.6000\n",
      "Epoch 130/150\n",
      "50/50 [==============================] - 0s - loss: 6.9035e-05 - acc: 1.0000 - val_loss: 6.3000 - val_acc: 0.6000\n",
      "Epoch 131/150\n",
      "50/50 [==============================] - 0s - loss: 6.3353e-05 - acc: 1.0000 - val_loss: 6.2596 - val_acc: 0.6000\n",
      "Epoch 132/150\n",
      "50/50 [==============================] - 0s - loss: 3.2585e-04 - acc: 1.0000 - val_loss: 6.2098 - val_acc: 0.6000\n",
      "Epoch 133/150\n",
      "50/50 [==============================] - 0s - loss: 2.1398e-05 - acc: 1.0000 - val_loss: 6.1628 - val_acc: 0.6000\n",
      "Epoch 134/150\n",
      "50/50 [==============================] - 0s - loss: 4.4573e-05 - acc: 1.0000 - val_loss: 6.1195 - val_acc: 0.6000\n",
      "Epoch 135/150\n",
      "50/50 [==============================] - 0s - loss: 3.2374e-05 - acc: 1.0000 - val_loss: 6.0854 - val_acc: 0.6000\n",
      "Epoch 136/150\n",
      "50/50 [==============================] - 0s - loss: 4.1951e-05 - acc: 1.0000 - val_loss: 6.0619 - val_acc: 0.6200\n",
      "Epoch 137/150\n",
      "50/50 [==============================] - 0s - loss: 6.7253e-05 - acc: 1.0000 - val_loss: 6.0446 - val_acc: 0.6200\n",
      "Epoch 138/150\n",
      "50/50 [==============================] - 0s - loss: 6.0491e-04 - acc: 1.0000 - val_loss: 6.0200 - val_acc: 0.6200\n",
      "Epoch 139/150\n",
      "50/50 [==============================] - 0s - loss: 3.6529e-05 - acc: 1.0000 - val_loss: 5.9995 - val_acc: 0.6200\n",
      "Epoch 140/150\n",
      "50/50 [==============================] - 0s - loss: 6.2972e-05 - acc: 1.0000 - val_loss: 5.9763 - val_acc: 0.6200\n",
      "Epoch 141/150\n",
      "50/50 [==============================] - 0s - loss: 3.5839e-05 - acc: 1.0000 - val_loss: 5.9549 - val_acc: 0.6200\n",
      "Epoch 142/150\n",
      "50/50 [==============================] - 0s - loss: 4.9012e-05 - acc: 1.0000 - val_loss: 5.9337 - val_acc: 0.6200\n",
      "Epoch 143/150\n",
      "50/50 [==============================] - 0s - loss: 1.0508e-04 - acc: 1.0000 - val_loss: 5.9134 - val_acc: 0.6200\n",
      "Epoch 144/150\n",
      "50/50 [==============================] - 0s - loss: 3.2251e-04 - acc: 1.0000 - val_loss: 5.9146 - val_acc: 0.6200\n",
      "Epoch 145/150\n",
      "50/50 [==============================] - 0s - loss: 5.2834e-05 - acc: 1.0000 - val_loss: 5.9150 - val_acc: 0.6200\n",
      "Epoch 146/150\n",
      "50/50 [==============================] - 0s - loss: 5.8105e-05 - acc: 1.0000 - val_loss: 5.9182 - val_acc: 0.6200\n",
      "Epoch 147/150\n",
      "50/50 [==============================] - 0s - loss: 6.7951e-05 - acc: 1.0000 - val_loss: 5.9271 - val_acc: 0.6200\n",
      "Epoch 148/150\n",
      "50/50 [==============================] - 0s - loss: 7.0166e-05 - acc: 1.0000 - val_loss: 5.9327 - val_acc: 0.6200\n",
      "Epoch 149/150\n",
      "50/50 [==============================] - 0s - loss: 6.9302e-05 - acc: 1.0000 - val_loss: 5.9387 - val_acc: 0.6200\n",
      "Epoch 150/150\n",
      "50/50 [==============================] - 0s - loss: 7.8969e-05 - acc: 1.0000 - val_loss: 5.9490 - val_acc: 0.6200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1bb80de1cf8>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEKCAYAAAAfGVI8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJs1ZIDDGmDRngcAYY9KcBQJjjElz/x9K9WLg4ciBAAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1bb807f3d68>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# preprocess\n",
    "x_train_pp = x_train - np.mean(x_train)\n",
    "x_train_pp /= np.std(x_train_pp, axis=0)\n",
    "\n",
    "x_test_pp  = x_test - np.mean(x_train) \n",
    "x_test_pp /= np.std(x_train_pp, axis=0)\n",
    "\n",
    "# build model\n",
    "model_name = '1st_place'\n",
    "model = Sequential(name=model_name)\n",
    "model.add(Cropping2D(cropping=3, input_shape=image_shape))\n",
    "model.add(Conv2D(32, (5,5), padding='same', activation='relu', kernel_initializer='TruncatedNormal'))\n",
    "model.add(MaxPooling2D((2,2), strides=(2,2), padding='valid'))\n",
    "\n",
    "model.add(ZeroPadding2D(padding=1))\n",
    "model.add(Conv2D(32, (4,4), padding='valid', activation='relu', kernel_initializer='TruncatedNormal'))\n",
    "model.add(AveragePooling2D((2,2), strides=(2,2), padding='valid'))\n",
    "\n",
    "model.add(Conv2D(64, (5,5), padding='same', activation='relu', kernel_initializer='TruncatedNormal'))\n",
    "model.add(AveragePooling2D((2,2), strides=(2,2), padding='valid'))\n",
    "\n",
    "model.add(Flatten())\n",
    "# TODO is this where dropout should be?\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(3072, activation='relu', kernel_initializer='TruncatedNormal'))\n",
    "# TODO svm activation function\n",
    "model.add(Dense(n_classes, activation='softmax', kernel_initializer='TruncatedNormal'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# compile model\n",
    "adam = optimizers.Adam()\n",
    "model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "\n",
    "# fit model\n",
    "epoch_sets = 1\n",
    "epochs = 150\n",
    "batch_size = 50\n",
    "for es in range(epoch_sets):\n",
    "    # get current lr\n",
    "    lr = k.get_value(sgd.lr)\n",
    "    print('epoch set:', es, 'lr value:', lr)\n",
    "    \n",
    "    # fit model\n",
    "    history = model.fit(x_train_pp, y_train_onehot\n",
    "                        , batch_size = batch_size\n",
    "                        , epochs=epochs\n",
    "                        , validation_data=(x_test_pp, y_test_onehot))\n",
    "    # decay lr by half\n",
    "    lr = lr / 2\n",
    "    k.set_value(sgd.lr, lr)\n",
    "    \n",
    "# visualize loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocess: manual center & normalize + adam on tiny subset + batchsize=60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "cropping2d_8 (Cropping2D)    (None, 42, 42, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_22 (Conv2D)           (None, 42, 42, 32)        832       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 21, 21, 32)        0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_8 (ZeroPaddin (None, 23, 23, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_23 (Conv2D)           (None, 20, 20, 32)        16416     \n",
      "_________________________________________________________________\n",
      "average_pooling2d_15 (Averag (None, 10, 10, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_24 (Conv2D)           (None, 10, 10, 64)        51264     \n",
      "_________________________________________________________________\n",
      "average_pooling2d_16 (Averag (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 3072)              4918272   \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 7)                 21511     \n",
      "=================================================================\n",
      "Total params: 5,008,295\n",
      "Trainable params: 5,008,295\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "epoch set: 0 lr value: 0.0025\n",
      "Train on 50 samples, validate on 50 samples\n",
      "Epoch 1/100\n",
      "50/50 [==============================] - 0s - loss: 2.2545 - acc: 0.0000e+00 - val_loss: 5.8025 - val_acc: 0.6400\n",
      "Epoch 2/100\n",
      "50/50 [==============================] - 0s - loss: 0.8000 - acc: 0.6000 - val_loss: 8.4693 - val_acc: 0.3400\n",
      "Epoch 3/100\n",
      "50/50 [==============================] - 0s - loss: 0.6581 - acc: 0.5800 - val_loss: 5.2580 - val_acc: 0.6600\n",
      "Epoch 4/100\n",
      "50/50 [==============================] - 0s - loss: 0.5407 - acc: 0.6600 - val_loss: 4.9064 - val_acc: 0.6600\n",
      "Epoch 5/100\n",
      "50/50 [==============================] - 0s - loss: 0.4718 - acc: 0.8600 - val_loss: 6.5538 - val_acc: 0.5400\n",
      "Epoch 6/100\n",
      "50/50 [==============================] - 0s - loss: 0.5035 - acc: 0.7600 - val_loss: 4.9697 - val_acc: 0.6600\n",
      "Epoch 7/100\n",
      "50/50 [==============================] - 0s - loss: 0.4086 - acc: 0.8400 - val_loss: 5.0436 - val_acc: 0.6600\n",
      "Epoch 8/100\n",
      "50/50 [==============================] - 0s - loss: 0.3683 - acc: 0.8600 - val_loss: 5.3525 - val_acc: 0.6400\n",
      "Epoch 9/100\n",
      "50/50 [==============================] - 0s - loss: 0.3225 - acc: 0.8800 - val_loss: 5.8476 - val_acc: 0.5800\n",
      "Epoch 10/100\n",
      "50/50 [==============================] - 0s - loss: 0.2970 - acc: 0.9000 - val_loss: 5.7057 - val_acc: 0.6400\n",
      "Epoch 11/100\n",
      "50/50 [==============================] - 0s - loss: 0.2783 - acc: 0.8800 - val_loss: 5.6703 - val_acc: 0.6400\n",
      "Epoch 12/100\n",
      "50/50 [==============================] - 0s - loss: 0.2549 - acc: 0.8800 - val_loss: 6.0964 - val_acc: 0.6200\n",
      "Epoch 13/100\n",
      "50/50 [==============================] - 0s - loss: 0.1943 - acc: 0.9200 - val_loss: 5.2201 - val_acc: 0.6400\n",
      "Epoch 14/100\n",
      "50/50 [==============================] - 0s - loss: 0.2028 - acc: 0.9200 - val_loss: 6.1866 - val_acc: 0.6000\n",
      "Epoch 15/100\n",
      "50/50 [==============================] - 0s - loss: 0.1038 - acc: 0.9800 - val_loss: 7.0526 - val_acc: 0.5600\n",
      "Epoch 16/100\n",
      "50/50 [==============================] - 0s - loss: 0.1774 - acc: 0.9400 - val_loss: 7.0321 - val_acc: 0.5400\n",
      "Epoch 17/100\n",
      "50/50 [==============================] - 0s - loss: 0.0912 - acc: 0.9800 - val_loss: 6.2349 - val_acc: 0.5800\n",
      "Epoch 18/100\n",
      "50/50 [==============================] - 0s - loss: 0.1389 - acc: 0.9400 - val_loss: 5.6939 - val_acc: 0.6200\n",
      "Epoch 19/100\n",
      "50/50 [==============================] - 0s - loss: 0.0925 - acc: 0.9600 - val_loss: 6.7215 - val_acc: 0.5800\n",
      "Epoch 20/100\n",
      "50/50 [==============================] - 0s - loss: 0.0931 - acc: 0.9800 - val_loss: 6.3222 - val_acc: 0.6000\n",
      "Epoch 21/100\n",
      "50/50 [==============================] - 0s - loss: 0.0746 - acc: 0.9800 - val_loss: 5.1261 - val_acc: 0.6400\n",
      "Epoch 22/100\n",
      "50/50 [==============================] - 0s - loss: 0.0568 - acc: 0.9800 - val_loss: 5.8072 - val_acc: 0.6400\n",
      "Epoch 23/100\n",
      "50/50 [==============================] - 0s - loss: 0.0535 - acc: 0.9800 - val_loss: 5.8565 - val_acc: 0.6000\n",
      "Epoch 24/100\n",
      "50/50 [==============================] - 0s - loss: 0.0462 - acc: 0.9800 - val_loss: 5.6561 - val_acc: 0.6200\n",
      "Epoch 25/100\n",
      "50/50 [==============================] - 0s - loss: 0.0346 - acc: 1.0000 - val_loss: 5.8025 - val_acc: 0.6400\n",
      "Epoch 26/100\n",
      "50/50 [==============================] - 0s - loss: 0.0262 - acc: 1.0000 - val_loss: 5.8025 - val_acc: 0.6400\n",
      "Epoch 27/100\n",
      "50/50 [==============================] - 0s - loss: 0.0237 - acc: 1.0000 - val_loss: 5.8025 - val_acc: 0.6400\n",
      "Epoch 28/100\n",
      "50/50 [==============================] - 0s - loss: 0.0402 - acc: 1.0000 - val_loss: 5.3118 - val_acc: 0.6600\n",
      "Epoch 29/100\n",
      "50/50 [==============================] - 0s - loss: 0.0098 - acc: 1.0000 - val_loss: 6.4472 - val_acc: 0.6000\n",
      "Epoch 30/100\n",
      "50/50 [==============================] - 0s - loss: 0.0155 - acc: 1.0000 - val_loss: 6.2157 - val_acc: 0.6000\n",
      "Epoch 31/100\n",
      "50/50 [==============================] - 0s - loss: 0.0165 - acc: 1.0000 - val_loss: 6.1576 - val_acc: 0.6000\n",
      "Epoch 32/100\n",
      "50/50 [==============================] - 0s - loss: 0.0116 - acc: 1.0000 - val_loss: 5.8853 - val_acc: 0.6200\n",
      "Epoch 33/100\n",
      "50/50 [==============================] - 0s - loss: 0.0167 - acc: 1.0000 - val_loss: 6.7696 - val_acc: 0.5800\n",
      "Epoch 34/100\n",
      "50/50 [==============================] - 0s - loss: 0.0088 - acc: 1.0000 - val_loss: 6.3145 - val_acc: 0.5800\n",
      "Epoch 35/100\n",
      "50/50 [==============================] - 0s - loss: 0.0040 - acc: 1.0000 - val_loss: 6.4472 - val_acc: 0.6000\n",
      "Epoch 36/100\n",
      "50/50 [==============================] - 0s - loss: 0.0045 - acc: 1.0000 - val_loss: 6.7696 - val_acc: 0.5800\n",
      "Epoch 37/100\n",
      "50/50 [==============================] - 0s - loss: 0.0107 - acc: 1.0000 - val_loss: 6.7696 - val_acc: 0.5800\n",
      "Epoch 38/100\n",
      "50/50 [==============================] - 0s - loss: 0.0506 - acc: 0.9800 - val_loss: 6.0550 - val_acc: 0.6200\n",
      "Epoch 39/100\n",
      "50/50 [==============================] - 0s - loss: 0.0016 - acc: 1.0000 - val_loss: 6.2218 - val_acc: 0.6000\n",
      "Epoch 40/100\n",
      "50/50 [==============================] - 0s - loss: 0.0015 - acc: 1.0000 - val_loss: 5.3941 - val_acc: 0.6200\n",
      "Epoch 41/100\n",
      "50/50 [==============================] - 0s - loss: 0.0050 - acc: 1.0000 - val_loss: 5.8025 - val_acc: 0.6400\n",
      "Epoch 42/100\n",
      "50/50 [==============================] - 0s - loss: 0.0827 - acc: 0.9600 - val_loss: 5.4802 - val_acc: 0.6600\n",
      "Epoch 43/100\n",
      "50/50 [==============================] - 0s - loss: 0.0011 - acc: 1.0000 - val_loss: 4.5154 - val_acc: 0.7200\n",
      "Epoch 44/100\n",
      "50/50 [==============================] - 0s - loss: 6.6650e-04 - acc: 1.0000 - val_loss: 5.4802 - val_acc: 0.6600\n",
      "Epoch 45/100\n",
      "50/50 [==============================] - 0s - loss: 0.0038 - acc: 1.0000 - val_loss: 6.1249 - val_acc: 0.6200\n",
      "Epoch 46/100\n",
      "50/50 [==============================] - 0s - loss: 0.0593 - acc: 0.9800 - val_loss: 5.8027 - val_acc: 0.6400\n",
      "Epoch 47/100\n",
      "50/50 [==============================] - 0s - loss: 0.0075 - acc: 1.0000 - val_loss: 5.1578 - val_acc: 0.6800\n",
      "Epoch 48/100\n",
      "50/50 [==============================] - 0s - loss: 0.0019 - acc: 1.0000 - val_loss: 5.1945 - val_acc: 0.6600\n",
      "Epoch 49/100\n",
      "50/50 [==============================] - 0s - loss: 0.0011 - acc: 1.0000 - val_loss: 5.4817 - val_acc: 0.6600\n",
      "Epoch 50/100\n",
      "50/50 [==============================] - 0s - loss: 0.0025 - acc: 1.0000 - val_loss: 5.7654 - val_acc: 0.6400\n",
      "Epoch 51/100\n",
      "50/50 [==============================] - 0s - loss: 0.0035 - acc: 1.0000 - val_loss: 5.5758 - val_acc: 0.6200\n",
      "Epoch 52/100\n",
      "50/50 [==============================] - 0s - loss: 0.0058 - acc: 1.0000 - val_loss: 5.6253 - val_acc: 0.6400\n",
      "Epoch 53/100\n",
      "50/50 [==============================] - 0s - loss: 0.0044 - acc: 1.0000 - val_loss: 5.8025 - val_acc: 0.6400\n",
      "Epoch 54/100\n",
      "50/50 [==============================] - 0s - loss: 0.0053 - acc: 1.0000 - val_loss: 5.7124 - val_acc: 0.6400\n",
      "Epoch 55/100\n",
      "50/50 [==============================] - 0s - loss: 0.0027 - acc: 1.0000 - val_loss: 5.3241 - val_acc: 0.6600\n",
      "Epoch 56/100\n",
      "50/50 [==============================] - 0s - loss: 0.0013 - acc: 1.0000 - val_loss: 5.1578 - val_acc: 0.6800\n",
      "Epoch 57/100\n",
      "50/50 [==============================] - 0s - loss: 0.0048 - acc: 1.0000 - val_loss: 5.1578 - val_acc: 0.6800\n",
      "Epoch 58/100\n",
      "50/50 [==============================] - 0s - loss: 0.0095 - acc: 1.0000 - val_loss: 5.1578 - val_acc: 0.6800\n",
      "Epoch 59/100\n",
      "50/50 [==============================] - 0s - loss: 0.0021 - acc: 1.0000 - val_loss: 4.9591 - val_acc: 0.6800\n",
      "Epoch 60/100\n",
      "50/50 [==============================] - 0s - loss: 0.0036 - acc: 1.0000 - val_loss: 4.8802 - val_acc: 0.6800\n",
      "Epoch 61/100\n",
      "50/50 [==============================] - 0s - loss: 0.0045 - acc: 1.0000 - val_loss: 5.1629 - val_acc: 0.6800\n",
      "Epoch 62/100\n",
      "50/50 [==============================] - 0s - loss: 0.0028 - acc: 1.0000 - val_loss: 5.4810 - val_acc: 0.6600\n",
      "Epoch 63/100\n",
      "50/50 [==============================] - 0s - loss: 0.0017 - acc: 1.0000 - val_loss: 5.4802 - val_acc: 0.6600\n",
      "Epoch 64/100\n",
      "50/50 [==============================] - 0s - loss: 0.0017 - acc: 1.0000 - val_loss: 5.4802 - val_acc: 0.6600\n",
      "Epoch 65/100\n",
      "50/50 [==============================] - 0s - loss: 0.0056 - acc: 1.0000 - val_loss: 5.4802 - val_acc: 0.6600\n",
      "Epoch 66/100\n",
      "50/50 [==============================] - 0s - loss: 6.9597e-04 - acc: 1.0000 - val_loss: 5.4802 - val_acc: 0.6600\n",
      "Epoch 67/100\n",
      "50/50 [==============================] - 0s - loss: 0.0010 - acc: 1.0000 - val_loss: 5.3368 - val_acc: 0.6600\n",
      "Epoch 68/100\n",
      "50/50 [==============================] - 0s - loss: 4.5640e-04 - acc: 1.0000 - val_loss: 5.1592 - val_acc: 0.6800\n",
      "Epoch 69/100\n",
      "50/50 [==============================] - 0s - loss: 5.4774e-04 - acc: 1.0000 - val_loss: 5.4802 - val_acc: 0.6600\n",
      "Epoch 70/100\n",
      "50/50 [==============================] - 0s - loss: 5.1800e-04 - acc: 1.0000 - val_loss: 5.4727 - val_acc: 0.6600\n",
      "Epoch 71/100\n",
      "50/50 [==============================] - 0s - loss: 3.3377e-04 - acc: 1.0000 - val_loss: 5.3365 - val_acc: 0.6600\n",
      "Epoch 72/100\n",
      "50/50 [==============================] - 0s - loss: 6.2322e-04 - acc: 1.0000 - val_loss: 5.2706 - val_acc: 0.6600\n",
      "Epoch 73/100\n",
      "50/50 [==============================] - 0s - loss: 0.0087 - acc: 1.0000 - val_loss: 5.4802 - val_acc: 0.6600\n",
      "Epoch 74/100\n",
      "50/50 [==============================] - 0s - loss: 2.0036e-04 - acc: 1.0000 - val_loss: 5.4802 - val_acc: 0.6600\n",
      "Epoch 75/100\n",
      "50/50 [==============================] - 0s - loss: 7.5309e-04 - acc: 1.0000 - val_loss: 5.4802 - val_acc: 0.6600\n",
      "Epoch 76/100\n",
      "50/50 [==============================] - 0s - loss: 5.1120e-04 - acc: 1.0000 - val_loss: 5.2260 - val_acc: 0.6600\n",
      "Epoch 77/100\n",
      "50/50 [==============================] - 0s - loss: 8.4523e-04 - acc: 1.0000 - val_loss: 5.3933 - val_acc: 0.6600\n",
      "Epoch 78/100\n",
      "50/50 [==============================] - 0s - loss: 6.6183e-04 - acc: 1.0000 - val_loss: 5.2326 - val_acc: 0.6600\n",
      "Epoch 79/100\n",
      "50/50 [==============================] - 0s - loss: 0.0031 - acc: 1.0000 - val_loss: 5.0589 - val_acc: 0.6800\n",
      "Epoch 80/100\n",
      "50/50 [==============================] - 0s - loss: 0.0031 - acc: 1.0000 - val_loss: 4.8354 - val_acc: 0.7000\n",
      "Epoch 81/100\n",
      "50/50 [==============================] - 0s - loss: 0.0059 - acc: 1.0000 - val_loss: 4.8428 - val_acc: 0.7000\n",
      "Epoch 82/100\n",
      "50/50 [==============================] - 0s - loss: 0.0382 - acc: 0.9800 - val_loss: 4.8354 - val_acc: 0.7000\n",
      "Epoch 83/100\n",
      "50/50 [==============================] - 0s - loss: 1.3754e-04 - acc: 1.0000 - val_loss: 5.4515 - val_acc: 0.6600\n",
      "Epoch 84/100\n",
      "50/50 [==============================] - 0s - loss: 5.1090e-04 - acc: 1.0000 - val_loss: 6.0406 - val_acc: 0.6200\n",
      "Epoch 85/100\n",
      "50/50 [==============================] - 0s - loss: 0.0014 - acc: 1.0000 - val_loss: 6.4472 - val_acc: 0.6000\n",
      "Epoch 86/100\n",
      "50/50 [==============================] - 0s - loss: 0.0080 - acc: 1.0000 - val_loss: 6.4472 - val_acc: 0.6000\n",
      "Epoch 87/100\n",
      "50/50 [==============================] - 0s - loss: 0.1018 - acc: 0.9600 - val_loss: 6.6523 - val_acc: 0.5800\n",
      "Epoch 88/100\n",
      "50/50 [==============================] - 0s - loss: 0.0013 - acc: 1.0000 - val_loss: 5.1578 - val_acc: 0.6800\n",
      "Epoch 89/100\n",
      "50/50 [==============================] - 0s - loss: 4.1052e-04 - acc: 1.0000 - val_loss: 4.8354 - val_acc: 0.7000\n",
      "Epoch 90/100\n",
      "50/50 [==============================] - 0s - loss: 0.0020 - acc: 1.0000 - val_loss: 5.4145 - val_acc: 0.6600\n",
      "Epoch 91/100\n",
      "50/50 [==============================] - 0s - loss: 0.0539 - acc: 0.9800 - val_loss: 4.1907 - val_acc: 0.7400\n",
      "Epoch 92/100\n",
      "50/50 [==============================] - 0s - loss: 0.0014 - acc: 1.0000 - val_loss: 4.5131 - val_acc: 0.7200\n",
      "Epoch 93/100\n",
      "50/50 [==============================] - 0s - loss: 0.0010 - acc: 1.0000 - val_loss: 4.5131 - val_acc: 0.7200\n",
      "Epoch 94/100\n",
      "50/50 [==============================] - 0s - loss: 0.0217 - acc: 0.9800 - val_loss: 4.5131 - val_acc: 0.7200\n",
      "Epoch 95/100\n",
      "50/50 [==============================] - 0s - loss: 0.0058 - acc: 1.0000 - val_loss: 4.5131 - val_acc: 0.7200\n",
      "Epoch 96/100\n",
      "50/50 [==============================] - 0s - loss: 0.0083 - acc: 1.0000 - val_loss: 4.5131 - val_acc: 0.7200\n",
      "Epoch 97/100\n",
      "50/50 [==============================] - 0s - loss: 0.0351 - acc: 0.9800 - val_loss: 4.5131 - val_acc: 0.7200\n",
      "Epoch 98/100\n",
      "50/50 [==============================] - 0s - loss: 0.0037 - acc: 1.0000 - val_loss: 5.5245 - val_acc: 0.6400\n",
      "Epoch 99/100\n",
      "50/50 [==============================] - 0s - loss: 0.0068 - acc: 1.0000 - val_loss: 5.4802 - val_acc: 0.6600\n",
      "Epoch 100/100\n",
      "50/50 [==============================] - 0s - loss: 0.0218 - acc: 1.0000 - val_loss: 6.1134 - val_acc: 0.6000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1bb8a8f5240>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAEKCAYAAAARnO4WAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1bb80d78b70>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# preprocess\n",
    "x_train_pp = x_train - np.mean(x_train)\n",
    "x_train_pp /= np.std(x_train_pp, axis=0)\n",
    "\n",
    "x_test_pp  = x_test - np.mean(x_train) \n",
    "x_test_pp /= np.std(x_train_pp, axis=0)\n",
    "\n",
    "# build model\n",
    "model_name = '1st_place'\n",
    "model = Sequential(name=model_name)\n",
    "model.add(Cropping2D(cropping=3, input_shape=image_shape))\n",
    "model.add(Conv2D(32, (5,5), padding='same', activation='relu', kernel_initializer='TruncatedNormal'))\n",
    "model.add(MaxPooling2D((2,2), strides=(2,2), padding='valid'))\n",
    "\n",
    "model.add(ZeroPadding2D(padding=1))\n",
    "model.add(Conv2D(32, (4,4), padding='valid', activation='relu', kernel_initializer='TruncatedNormal'))\n",
    "model.add(AveragePooling2D((2,2), strides=(2,2), padding='valid'))\n",
    "\n",
    "model.add(Conv2D(64, (5,5), padding='same', activation='relu', kernel_initializer='TruncatedNormal'))\n",
    "model.add(AveragePooling2D((2,2), strides=(2,2), padding='valid'))\n",
    "\n",
    "model.add(Flatten())\n",
    "# TODO is this where dropout should be?\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(3072, activation='relu', kernel_initializer='TruncatedNormal'))\n",
    "# TODO svm activation function\n",
    "model.add(Dense(n_classes, activation='softmax', kernel_initializer='TruncatedNormal'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# compile model\n",
    "adam = optimizers.Adam()\n",
    "model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "\n",
    "# fit model\n",
    "epoch_sets = 1\n",
    "epochs = 100\n",
    "batch_size = 60\n",
    "for es in range(epoch_sets):\n",
    "    # get current lr\n",
    "    lr = k.get_value(sgd.lr)\n",
    "    print('epoch set:', es, 'lr value:', lr)\n",
    "    \n",
    "    # fit model\n",
    "    history = model.fit(x_train_pp, y_train_onehot\n",
    "                        , batch_size = batch_size\n",
    "                        , epochs=epochs\n",
    "                        , validation_data=(x_test_pp, y_test_onehot))\n",
    "    # decay lr by half\n",
    "    lr = lr / 2\n",
    "    k.set_value(sgd.lr, lr)\n",
    "    \n",
    "# visualize loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocess: manual center & normalize + sgd_decay on tiny subset + batchsize=60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "cropping2d_9 (Cropping2D)    (None, 42, 42, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_25 (Conv2D)           (None, 42, 42, 32)        832       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 21, 21, 32)        0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_9 (ZeroPaddin (None, 23, 23, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_26 (Conv2D)           (None, 20, 20, 32)        16416     \n",
      "_________________________________________________________________\n",
      "average_pooling2d_17 (Averag (None, 10, 10, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_27 (Conv2D)           (None, 10, 10, 64)        51264     \n",
      "_________________________________________________________________\n",
      "average_pooling2d_18 (Averag (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 3072)              4918272   \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 7)                 21511     \n",
      "=================================================================\n",
      "Total params: 5,008,295\n",
      "Trainable params: 5,008,295\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "epoch set: 0 lr value: 0.01\n",
      "Train on 50 samples, validate on 50 samples\n",
      "Epoch 1/100\n",
      "50/50 [==============================] - 0s - loss: 2.1382 - acc: 0.0000e+00 - val_loss: 5.6811 - val_acc: 0.6400\n",
      "Epoch 2/100\n",
      "50/50 [==============================] - 0s - loss: 1.2726 - acc: 0.5800 - val_loss: 5.8025 - val_acc: 0.6400\n",
      "Epoch 3/100\n",
      "50/50 [==============================] - 0s - loss: 0.8413 - acc: 0.6000 - val_loss: 4.6234 - val_acc: 0.6600\n",
      "Epoch 4/100\n",
      "50/50 [==============================] - 0s - loss: 0.6911 - acc: 0.6000 - val_loss: 4.9147 - val_acc: 0.5600\n",
      "Epoch 5/100\n",
      "50/50 [==============================] - 0s - loss: 0.7313 - acc: 0.4800 - val_loss: 5.8025 - val_acc: 0.6400\n",
      "Epoch 6/100\n",
      "50/50 [==============================] - 0s - loss: 0.7856 - acc: 0.6000 - val_loss: 10.3156 - val_acc: 0.3600\n",
      "Epoch 7/100\n",
      "50/50 [==============================] - 0s - loss: 1.2078 - acc: 0.4000 - val_loss: 5.8025 - val_acc: 0.6400\n",
      "Epoch 8/100\n",
      "50/50 [==============================] - 0s - loss: 1.2260 - acc: 0.6000 - val_loss: 5.4035 - val_acc: 0.6400\n",
      "Epoch 9/100\n",
      "50/50 [==============================] - 0s - loss: 0.6477 - acc: 0.6200 - val_loss: 4.1006 - val_acc: 0.6600\n",
      "Epoch 10/100\n",
      "50/50 [==============================] - 0s - loss: 0.6721 - acc: 0.7200 - val_loss: 4.0063 - val_acc: 0.6200\n",
      "Epoch 11/100\n",
      "50/50 [==============================] - 0s - loss: 0.6556 - acc: 0.7200 - val_loss: 3.9749 - val_acc: 0.6400\n",
      "Epoch 12/100\n",
      "50/50 [==============================] - 0s - loss: 0.5943 - acc: 0.7800 - val_loss: 4.0286 - val_acc: 0.6200\n",
      "Epoch 13/100\n",
      "50/50 [==============================] - 0s - loss: 0.5475 - acc: 0.8000 - val_loss: 4.2094 - val_acc: 0.6200\n",
      "Epoch 14/100\n",
      "50/50 [==============================] - 0s - loss: 0.5100 - acc: 0.7800 - val_loss: 4.6480 - val_acc: 0.6200\n",
      "Epoch 15/100\n",
      "50/50 [==============================] - 0s - loss: 0.4946 - acc: 0.8200 - val_loss: 4.9863 - val_acc: 0.6200\n",
      "Epoch 16/100\n",
      "50/50 [==============================] - 0s - loss: 0.4633 - acc: 0.7600 - val_loss: 5.2314 - val_acc: 0.5600\n",
      "Epoch 17/100\n",
      "50/50 [==============================] - 0s - loss: 0.4211 - acc: 0.8400 - val_loss: 6.1323 - val_acc: 0.5600\n",
      "Epoch 18/100\n",
      "50/50 [==============================] - 0s - loss: 0.4305 - acc: 0.8200 - val_loss: 5.7172 - val_acc: 0.5800\n",
      "Epoch 19/100\n",
      "50/50 [==============================] - 0s - loss: 0.3658 - acc: 0.8600 - val_loss: 6.1194 - val_acc: 0.5600\n",
      "Epoch 20/100\n",
      "50/50 [==============================] - 0s - loss: 0.3894 - acc: 0.8200 - val_loss: 5.3403 - val_acc: 0.6600\n",
      "Epoch 21/100\n",
      "50/50 [==============================] - 0s - loss: 0.4856 - acc: 0.7400 - val_loss: 7.7162 - val_acc: 0.5000\n",
      "Epoch 22/100\n",
      "50/50 [==============================] - 0s - loss: 0.5948 - acc: 0.7000 - val_loss: 5.5469 - val_acc: 0.6400\n",
      "Epoch 23/100\n",
      "50/50 [==============================] - 0s - loss: 0.6969 - acc: 0.6600 - val_loss: 8.0591 - val_acc: 0.5000\n",
      "Epoch 24/100\n",
      "50/50 [==============================] - 0s - loss: 0.4988 - acc: 0.7000 - val_loss: 5.6684 - val_acc: 0.6400\n",
      "Epoch 25/100\n",
      "50/50 [==============================] - 0s - loss: 0.3147 - acc: 0.8600 - val_loss: 5.6169 - val_acc: 0.6000\n",
      "Epoch 26/100\n",
      "50/50 [==============================] - 0s - loss: 0.3476 - acc: 0.8400 - val_loss: 5.5262 - val_acc: 0.6200\n",
      "Epoch 27/100\n",
      "50/50 [==============================] - 0s - loss: 0.2719 - acc: 0.8600 - val_loss: 6.0822 - val_acc: 0.5600\n",
      "Epoch 28/100\n",
      "50/50 [==============================] - 0s - loss: 0.2385 - acc: 0.9200 - val_loss: 6.1942 - val_acc: 0.6000\n",
      "Epoch 29/100\n",
      "50/50 [==============================] - 0s - loss: 0.2307 - acc: 0.9200 - val_loss: 5.8675 - val_acc: 0.6000\n",
      "Epoch 30/100\n",
      "50/50 [==============================] - 0s - loss: 0.2142 - acc: 0.9400 - val_loss: 6.0071 - val_acc: 0.6000\n",
      "Epoch 31/100\n",
      "50/50 [==============================] - 0s - loss: 0.2283 - acc: 0.9200 - val_loss: 5.9050 - val_acc: 0.6200\n",
      "Epoch 32/100\n",
      "50/50 [==============================] - 0s - loss: 0.1782 - acc: 0.9600 - val_loss: 5.8810 - val_acc: 0.6000\n",
      "Epoch 33/100\n",
      "50/50 [==============================] - 0s - loss: 0.1881 - acc: 0.9000 - val_loss: 6.1836 - val_acc: 0.5800\n",
      "Epoch 34/100\n",
      "50/50 [==============================] - 0s - loss: 0.1453 - acc: 0.9800 - val_loss: 6.2971 - val_acc: 0.5800\n",
      "Epoch 35/100\n",
      "50/50 [==============================] - 0s - loss: 0.1462 - acc: 0.9800 - val_loss: 5.6094 - val_acc: 0.6400\n",
      "Epoch 36/100\n",
      "50/50 [==============================] - 0s - loss: 0.1401 - acc: 0.9400 - val_loss: 6.4233 - val_acc: 0.5800\n",
      "Epoch 37/100\n",
      "50/50 [==============================] - 0s - loss: 0.1175 - acc: 0.9600 - val_loss: 5.6410 - val_acc: 0.6200\n",
      "Epoch 38/100\n",
      "50/50 [==============================] - 0s - loss: 0.1481 - acc: 0.9600 - val_loss: 5.5076 - val_acc: 0.6400\n",
      "Epoch 39/100\n",
      "50/50 [==============================] - 0s - loss: 0.0964 - acc: 0.9800 - val_loss: 5.6302 - val_acc: 0.6000\n",
      "Epoch 40/100\n",
      "50/50 [==============================] - 0s - loss: 0.1069 - acc: 0.9400 - val_loss: 5.9786 - val_acc: 0.6000\n",
      "Epoch 41/100\n",
      "50/50 [==============================] - 0s - loss: 0.0768 - acc: 0.9800 - val_loss: 5.7134 - val_acc: 0.6000\n",
      "Epoch 42/100\n",
      "50/50 [==============================] - 0s - loss: 0.0639 - acc: 1.0000 - val_loss: 5.5189 - val_acc: 0.6400\n",
      "Epoch 43/100\n",
      "50/50 [==============================] - 0s - loss: 0.0530 - acc: 1.0000 - val_loss: 5.1686 - val_acc: 0.6800\n",
      "Epoch 44/100\n",
      "50/50 [==============================] - 0s - loss: 0.0855 - acc: 0.9600 - val_loss: 5.1798 - val_acc: 0.6600\n",
      "Epoch 45/100\n",
      "50/50 [==============================] - 0s - loss: 0.0460 - acc: 1.0000 - val_loss: 5.1453 - val_acc: 0.6600\n",
      "Epoch 46/100\n",
      "50/50 [==============================] - 0s - loss: 0.0413 - acc: 0.9800 - val_loss: 5.2407 - val_acc: 0.6600\n",
      "Epoch 47/100\n",
      "50/50 [==============================] - 0s - loss: 0.0352 - acc: 1.0000 - val_loss: 4.8373 - val_acc: 0.7000\n",
      "Epoch 48/100\n",
      "50/50 [==============================] - 0s - loss: 0.0719 - acc: 0.9800 - val_loss: 5.9225 - val_acc: 0.6200\n",
      "Epoch 49/100\n",
      "50/50 [==============================] - 0s - loss: 0.0834 - acc: 0.9800 - val_loss: 5.8682 - val_acc: 0.6000\n",
      "Epoch 50/100\n",
      "50/50 [==============================] - 0s - loss: 0.0506 - acc: 0.9800 - val_loss: 6.5796 - val_acc: 0.5600\n",
      "Epoch 51/100\n",
      "50/50 [==============================] - 0s - loss: 0.0648 - acc: 0.9600 - val_loss: 6.1484 - val_acc: 0.6000\n",
      "Epoch 52/100\n",
      "50/50 [==============================] - 0s - loss: 0.0463 - acc: 1.0000 - val_loss: 6.1065 - val_acc: 0.5800\n",
      "Epoch 53/100\n",
      "50/50 [==============================] - 0s - loss: 0.0354 - acc: 0.9800 - val_loss: 4.8572 - val_acc: 0.6800\n",
      "Epoch 54/100\n",
      "50/50 [==============================] - 0s - loss: 0.0150 - acc: 1.0000 - val_loss: 5.1412 - val_acc: 0.6600\n",
      "Epoch 55/100\n",
      "50/50 [==============================] - 0s - loss: 0.0143 - acc: 1.0000 - val_loss: 5.2386 - val_acc: 0.6600\n",
      "Epoch 56/100\n",
      "50/50 [==============================] - 0s - loss: 0.0158 - acc: 1.0000 - val_loss: 5.0187 - val_acc: 0.6800\n",
      "Epoch 57/100\n",
      "50/50 [==============================] - 0s - loss: 0.0346 - acc: 1.0000 - val_loss: 5.0369 - val_acc: 0.6800\n",
      "Epoch 58/100\n",
      "50/50 [==============================] - 0s - loss: 0.0208 - acc: 1.0000 - val_loss: 6.2646 - val_acc: 0.6000\n",
      "Epoch 59/100\n",
      "50/50 [==============================] - 0s - loss: 0.0465 - acc: 0.9800 - val_loss: 5.4815 - val_acc: 0.6600\n",
      "Epoch 60/100\n",
      "50/50 [==============================] - 0s - loss: 0.0709 - acc: 0.9800 - val_loss: 5.4802 - val_acc: 0.6600\n",
      "Epoch 61/100\n",
      "50/50 [==============================] - 0s - loss: 0.0277 - acc: 1.0000 - val_loss: 5.4802 - val_acc: 0.6600\n",
      "Epoch 62/100\n",
      "50/50 [==============================] - 0s - loss: 0.0509 - acc: 0.9600 - val_loss: 5.6394 - val_acc: 0.6400\n",
      "Epoch 63/100\n",
      "50/50 [==============================] - 0s - loss: 0.0487 - acc: 0.9800 - val_loss: 5.4555 - val_acc: 0.6400\n",
      "Epoch 64/100\n",
      "50/50 [==============================] - 0s - loss: 0.0162 - acc: 1.0000 - val_loss: 5.3780 - val_acc: 0.6400\n",
      "Epoch 65/100\n",
      "50/50 [==============================] - 0s - loss: 0.0467 - acc: 0.9800 - val_loss: 5.5199 - val_acc: 0.6400\n",
      "Epoch 66/100\n",
      "50/50 [==============================] - 0s - loss: 0.0267 - acc: 1.0000 - val_loss: 5.6029 - val_acc: 0.6400\n",
      "Epoch 67/100\n",
      "50/50 [==============================] - 0s - loss: 0.0444 - acc: 0.9800 - val_loss: 5.1607 - val_acc: 0.6800\n",
      "Epoch 68/100\n",
      "50/50 [==============================] - 0s - loss: 0.0676 - acc: 0.9800 - val_loss: 5.1970 - val_acc: 0.6400\n",
      "Epoch 69/100\n",
      "50/50 [==============================] - 0s - loss: 0.0228 - acc: 1.0000 - val_loss: 5.4802 - val_acc: 0.6600\n",
      "Epoch 70/100\n",
      "50/50 [==============================] - 0s - loss: 0.0228 - acc: 1.0000 - val_loss: 5.1617 - val_acc: 0.6800\n",
      "Epoch 71/100\n",
      "50/50 [==============================] - 0s - loss: 0.0091 - acc: 1.0000 - val_loss: 5.0162 - val_acc: 0.6800\n",
      "Epoch 72/100\n",
      "50/50 [==============================] - 0s - loss: 0.0230 - acc: 0.9800 - val_loss: 5.2619 - val_acc: 0.6600\n",
      "Epoch 73/100\n",
      "50/50 [==============================] - 0s - loss: 0.0085 - acc: 1.0000 - val_loss: 5.4683 - val_acc: 0.6600\n",
      "Epoch 74/100\n",
      "50/50 [==============================] - 0s - loss: 0.0139 - acc: 1.0000 - val_loss: 5.7612 - val_acc: 0.6400\n",
      "Epoch 75/100\n",
      "50/50 [==============================] - 0s - loss: 0.0221 - acc: 0.9800 - val_loss: 5.3701 - val_acc: 0.6600\n",
      "Epoch 76/100\n",
      "50/50 [==============================] - 0s - loss: 0.0043 - acc: 1.0000 - val_loss: 5.1594 - val_acc: 0.6800\n",
      "Epoch 77/100\n",
      "50/50 [==============================] - 0s - loss: 0.0034 - acc: 1.0000 - val_loss: 5.1578 - val_acc: 0.6800\n",
      "Epoch 78/100\n",
      "50/50 [==============================] - 0s - loss: 0.0192 - acc: 0.9800 - val_loss: 5.1603 - val_acc: 0.6800\n",
      "Epoch 79/100\n",
      "50/50 [==============================] - 0s - loss: 0.0034 - acc: 1.0000 - val_loss: 5.3295 - val_acc: 0.6600\n",
      "Epoch 80/100\n",
      "50/50 [==============================] - 0s - loss: 0.0053 - acc: 1.0000 - val_loss: 5.1582 - val_acc: 0.6800\n",
      "Epoch 81/100\n",
      "50/50 [==============================] - 0s - loss: 0.0058 - acc: 1.0000 - val_loss: 5.1578 - val_acc: 0.6800\n",
      "Epoch 82/100\n",
      "50/50 [==============================] - 0s - loss: 0.0116 - acc: 1.0000 - val_loss: 5.2827 - val_acc: 0.6600\n",
      "Epoch 83/100\n",
      "50/50 [==============================] - 0s - loss: 0.0034 - acc: 1.0000 - val_loss: 5.3514 - val_acc: 0.6600\n",
      "Epoch 84/100\n",
      "50/50 [==============================] - 0s - loss: 0.0278 - acc: 0.9800 - val_loss: 5.3740 - val_acc: 0.6600\n",
      "Epoch 85/100\n",
      "50/50 [==============================] - 0s - loss: 0.0041 - acc: 1.0000 - val_loss: 5.3314 - val_acc: 0.6600\n",
      "Epoch 86/100\n",
      "50/50 [==============================] - 0s - loss: 0.0042 - acc: 1.0000 - val_loss: 5.1849 - val_acc: 0.6600\n",
      "Epoch 87/100\n",
      "50/50 [==============================] - 0s - loss: 0.0031 - acc: 1.0000 - val_loss: 5.1796 - val_acc: 0.6600\n",
      "Epoch 88/100\n",
      "50/50 [==============================] - 0s - loss: 0.0070 - acc: 1.0000 - val_loss: 5.3936 - val_acc: 0.6600\n",
      "Epoch 89/100\n",
      "50/50 [==============================] - 0s - loss: 0.0098 - acc: 1.0000 - val_loss: 4.8365 - val_acc: 0.7000\n",
      "Epoch 90/100\n",
      "50/50 [==============================] - 0s - loss: 0.0048 - acc: 1.0000 - val_loss: 4.8874 - val_acc: 0.6800\n",
      "Epoch 91/100\n",
      "50/50 [==============================] - 0s - loss: 0.0014 - acc: 1.0000 - val_loss: 5.0110 - val_acc: 0.6800\n",
      "Epoch 92/100\n",
      "50/50 [==============================] - 0s - loss: 0.0077 - acc: 1.0000 - val_loss: 4.7793 - val_acc: 0.6800\n",
      "Epoch 93/100\n",
      "50/50 [==============================] - 0s - loss: 0.0029 - acc: 1.0000 - val_loss: 4.5134 - val_acc: 0.7200\n",
      "Epoch 94/100\n",
      "50/50 [==============================] - 0s - loss: 0.0048 - acc: 1.0000 - val_loss: 4.5131 - val_acc: 0.7200\n",
      "Epoch 95/100\n",
      "50/50 [==============================] - 0s - loss: 0.0064 - acc: 1.0000 - val_loss: 4.5131 - val_acc: 0.7200\n",
      "Epoch 96/100\n",
      "50/50 [==============================] - 0s - loss: 0.0025 - acc: 1.0000 - val_loss: 4.5629 - val_acc: 0.7000\n",
      "Epoch 97/100\n",
      "50/50 [==============================] - 0s - loss: 0.0032 - acc: 1.0000 - val_loss: 4.8354 - val_acc: 0.7000\n",
      "Epoch 98/100\n",
      "50/50 [==============================] - 0s - loss: 0.0070 - acc: 1.0000 - val_loss: 5.0563 - val_acc: 0.6800\n",
      "Epoch 99/100\n",
      "50/50 [==============================] - 0s - loss: 0.0014 - acc: 1.0000 - val_loss: 5.1262 - val_acc: 0.6800\n",
      "Epoch 100/100\n",
      "50/50 [==============================] - 0s - loss: 0.0055 - acc: 1.0000 - val_loss: 5.1578 - val_acc: 0.6800\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1bb8d0b0278>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEKCAYAAAAfGVI8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3Xd8W+W5wPHfK+89Ymc4znAghITsQQNhlk3LKhBKGwpd\n6e2kg9nezrvouLTltiWljNIyy16hBGjYJCEJCWSRTeIMx3a8p8Z7/3gkW5YlWx6yLOn5fj7+SDo6\n0nmPj32e8z7vOMZai1JKqcTliHYBlFJKRZcGAqWUSnAaCJRSKsFpIFBKqQSngUAppRKcBgKllEpw\nGgiUUirBaSBQSqkEp4FAKaUSXHK0CxCOoqIiO3HixGgXQymlYsq6deuqrLXFva0XE4Fg4sSJrF27\nNtrFUEqpmGKM+Tic9TQ1pJRSCU4DgVJKJTgNBEopleBioo0gGKfTSXl5Oa2trdEuSkSlp6dTWlpK\nSkpKtIuilIpTMRsIysvLycnJYeLEiRhjol2ciLDWUl1dTXl5OWVlZdEujlIqTsVsaqi1tZURI0bE\nbRAAMMYwYsSIuK/1KKWiK2YDARDXQcAnEfZRKRVdMR0IIsrjgeZq0Ft5KqXinAaCUNrqoHYfuNqC\nvl1bW8uf/vSnPn/thRdeSG1t7UBLp5RSg0YDQSgetzxad9C3QwUCl8vV49cuX76c/Pz8ARdPKaUG\nS8z2Goo4X0rIeoK+fcstt7Br1y5mz55NSkoK2dnZjBkzhg0bNrBlyxYuvfRS9u/fT2trK9dffz1L\nly4FOqfLaGxs5IILLuCUU07hnXfeYezYsTzzzDNkZGQM1R4qpRQQwUBgjLkX+DRwxFo73busEHgU\nmAjsBRZba2sGuq2fP7eZLQfrB/o1XUwrTuanC5NDthHcdtttbNq0iQ0bNvDaa6/xqU99ik2bNnV0\n87z33nspLCykpaWFBQsWcPnllzNixIgu37Fjxw4efvhh/vKXv7B48WKeeOIJlixZMqj7oZRSvYlk\nauivwPkBy24BXrXWTgZe9b4ennqpEQQ68cQTu/T1v+OOO5g1axYLFy5k//797Nixo9tnysrKmD17\nNgDz5s1j7969Ay62Ukr1VcRqBNbaN4wxEwMWXwKc4X1+P/AacPNAt/XTi04Y6Fd0V38QGivCDgRZ\nWVkdz1977TVeeeUV3n33XTIzMznjjDOCjgVIS0vreJ6UlERLS8vAy62UUn001I3Fo6y1h7zPDwOj\nQq1ojFlqjFlrjFlbWVk5NKXz5wsAIQJBTk4ODQ0NQd+rq6ujoKCAzMxMtm3bxqpVqyJVSqWUGrCo\nNRZba60xJmQnfWvtXcBdAPPnzx/6zvwdgSD4pkeMGMGiRYuYPn06GRkZjBrVGdPOP/98li1bxsyZ\nM5kyZQoLFy4cihIrpVS/DHUgqDDGjLHWHjLGjAGODPH2w9cRAEKnhh566KGgy9PS0njxxReDvudr\nBygqKmLTpk0dy2+44YZ+FVMppQZqqFNDzwLXep9fCzwzxNsPXy+pIaWUihcRCwTGmIeBd4Epxphy\nY8yXgduAc4wxO4Czva+HJw0ESqkEEcleQ1eHeOusSG1zUPXSRqCUUvFCp5gIRWsESqkEoYEglD4O\nKFNKqVilgSAUrREopRKEBoJQemkj6O801AC/+93vaG5u7m/JlFJqUGkgCKnn1JAGAqVUvNBpqEPp\nJTXkPw31Oeecw8iRI/nHP/5BW1sbl112GT//+c9pampi8eLFlJeX43a7+fGPf0xFRQUHDx7kzDPP\npKioiJUrVw7hTimlVHfxEQhevAUOfziIX2ghZzSc/O2wpqFesWIFjz/+OGvWrMFay8UXX8wbb7xB\nZWUlJSUlvPDCC4DMQZSXl8ftt9/OypUrKSoqGsQyK6VU/2hqqDdhNBavWLGCFStWMGfOHObOncu2\nbdvYsWMHM2bM4OWXX+bmm2/mzTffJC8vbwgKrJRSfRMfNYILBnmAssfVWcMIIxBYa7n11lv52te+\n1u299evXs3z5cm699VbOPfdcfvKTnwxuWZVSaoC0RhCMfzoojGmozzvvPO69914aGxsBOHDgAEeO\nHOHgwYNkZmayZMkSbrjhBtavX9/ts0opFW3xUSMYbL6Tv0mio/dQAP9pqC+44AI+97nPcdJJJwGQ\nnZ3NAw88wM6dO7nxxhtxOBykpKRw5513ArB06VLOP/98SkpKtLFYKRV1xsbAXDrz58+3a9eu7bJs\n69atTJ06NTIbdLZA5TZISgN3G4yZDcZEZlthiOi+KqXiljFmnbV2fm/raWooGF+NwOGrMA3/YKmU\nUv2lgSAYXy3JkeR9rdNMKKXiV0wHgoiltQJrBFFMn8VC6k4pFdtiNhCkp6dTXV0dmRNlRyCIbo3A\nWkt1dTXp6elR2b5SKjHEbK+h0tJSysvLqaysHPwvb2+G5ipIb4PWOjiaBEkpg7+dMKSnp1NaWhqV\nbSulEkPMBoKUlBTKysoi8+XvPwAvfRMu+DW8dCN8dSWMnRmZbSmlVJTFbGooopwt8phR0PW1UkrF\nIQ0Ewbha5dEXCFwaCJRS8UsDQTDOgEDge62UUnFIA0EwzmbpOpqWI69dGgiUUvFLA0EwrlZIzoAU\nb7dNbSNQSsUxDQTBOFskCCRnyGutESil4pgGgmC0RqCUSiAaCILRGoFSKoFoIAjG1QrJ6ZCULI3G\nWiNQSsUxDQTBOFsgxVsbSM7QGoFSKq5pIAjGVyMASRE5m6NbHqWUiqCoBAJjzPeMMZuNMZuMMQ8b\nY4bX9JqBNQIdUKaUimNDHgiMMWOB7wDzrbXTgSTgs0Ndjh4F1gjiZYqJmo/B7Yp2KZRSw0y0UkPJ\nQIYxJhnIBA5GqRzBdakRpMdHjaC1Hv6wAD54JNolUUoNM0MeCKy1B4DfAPuAQ0CdtXbFUJejR/6B\nICUzPmoETZXgboPa/dEuiVJqmIlGaqgAuAQoA0qALGPMkiDrLTXGrDXGrI3IzWd64htQBt7G4nio\nEdR5H2ujWw6l1LATjdTQ2cAea22ltdYJPAmcHLiStfYua+18a+384uLioS2hb0AZeLuPxkGNwBcA\nWjQQKKW6ikYg2AcsNMZkGmMMcBawNQrlCM7tBOuO3xpBS010y6GUGnai0UawGngcWA986C3DXUNd\njpB8o4i71AjiKBBoakgpFSAqvYastT+11h5vrZ1urb3GWtsWjXIE5TvpdxlQFkOpofZmePHmzhO/\nT4umhpRSwenI4kAdNQL/AWUxFAgOrIXVy2DPm12Xa41AKRWCBoJAwWoErhawNnpl6gtfG0DL0a7L\nO9oINBAopbrSQBAoWI3AeqQRORb4TvTNgYHAu9zdFls1HKVUxGkgCBSsRgCx04W0o5toiBoBaM8h\npVQXGggC+WYaTcn0PnprBrHShTRkjaCu+zpKKYUGgu58J3z/7qMQOzWCjjaCgKv+llpIz5Pn2mCs\nlPKjgSCQ74TvP6AMYqdG0NpDjaBgojzXGoFSyo8GgkAxXyPooY3AFwi0RqCU8qOBIFCs1wh8KSH/\nGoGzRXoL5U/ouo5SSqGBoLtYrxH49xryjX3wNRTnjweMpoaUUl1oIAgUskYQI4HAd5L3uKCtQZ77\nAkFGAaTnampIKdWFBoJAzlYwDkhKkde+gBALgcDjkZN+7lh57Wsn8AWH9Hz50RqBUsqPBoJAvpvS\nGCOvOwaUxUAbQVsdYKFwkrz2tRP4agTpeZCRrzUCpVQXGggC+d+UBmKrRuC70i8s874OCAQZWiNQ\nSnWngSCQq7VzVDF0jiyOhRqB70q/8Bh5bK7pujw9T9oJtNeQUsqPBoJAzubOeYYgtqaY8J3gfamh\njhqBfyDQ1JBSqisNBIGcrV1TQ0kpYJJio/uoL+XjGzjm30aQnAHJaZ2poViZVlspFXEaCAK5Wjrb\nBXxSMmKjRuC70s8qkqv/jnmH/OYZysgHj7Nzcj2lVMLTQBAosEYAkiqKpRpBej5kFHZtLPYFgvT8\nrusqpRKeBoJAIWsEsRAIaiApTcqbWdg1NZThDQC+R20nUEp5aSAIFKpGEAuBoLVWTvTGeHsH+TUW\nd6SGCuRRew4ppbw0EAQKWiNIj43uoy21namfjIAagaaGBo+rDe49H3a+Gu2SKDUoNBAEClojiKHU\nkO+KP7Ow86q/tc4vQGhqaMAOvg/73oV37oh2SZQaFBoIAvmmmPAXKzUCX2oIpEbQVg+udq0RDLb9\nq+Vx9+tQVx7dsig1CDQQBHK2dA4i80nJjJEagd+Vf2ahPNbtB+vpDBBpuYAZfjWC9ia4+xzYtzra\nJend/jUSaLGw8ZFol0apAdNA4M/tkj72gYEgORZrBN4U0dE98uirETgc3jEGwywQHFgP5Wtg67PR\nLknPrJVAcNx5MP5k2PiwDs5TMU8Dgb+OexEEtBHEwoAyt0tSQf5tBAA1AYEAojvfUPNReP9BKa+/\nQxvk8eD7Q1+mvqjZC01HYNyJMPtzUL0TytdGu1RKDYgGAn8ddycLViMY5qmhjqmm/doIAI7u7roc\nojff0O7X4c5F8Mw34KPlXd87tLHz0eMe+rKFa/8aeSw9EaZdIu1JGx8a/O3seQMe+mz3gKlUBGgg\n8BfLNQLfiT0joI0gMDUE/ZuKetdK2P1a/8rmdsJLP4K/XQypWfL73beq6zoHN4AjBdoboWpH/7Yz\nFMrXQGoOjJwqd3ubehFsemLw/z4+ehG2vwhVHw3u9yoVRFQCgTEm3xjzuDFmmzFmqzHmpGiUo5ue\nagTRmpuntR4qNgd/z//K2Zfq8aWGMnpKDfWhRtDWCM9+G/5+KTzy+c6xCX3x7h/h3T/A/C/B196A\nsfOk+2XHNhokxTL1Ink9nNND+1dD6XxwJMnr2VdLbWzFv8P+9wbvCr7mY3kczr8LFTeiVSP4PfBP\na+3xwCxga5TK0VVPNQLrlivbofbWb+HPp0Pjka7LtzwL/zMOGirktf88QyBX3o4UyWlDZ03Bt044\nNYID62HZIlj/d5hzjVytr/pT38rfUgNv3Q6Tz4VP/xZSM2H8QkkBtTfJOoc3ARZmXAkpWXBwfd+2\nMVTaGiQojzuxc1nZ6bJv7/0F7jkbflUGH/xj4NvyHbcDw/R3oeLKkAcCY0wecBpwD4C1tt1aOzy6\nsHTUCIJMMQHR6UJ6aIP0ZNr4cNflq/4EzqbOPu2BqSFjJD3k6+2Ultv5WV+NoLfeLk/9mwS/Ly6H\nS/4gV+yr/9y3tNLbd8gV81k/6Vw2/iQJrAfWde4jwNi5MGbW8L0KPrBeuuL6BwJHEnz+MbhxF1xx\nn/ytbHt+YNuxFmq1RqCGTjRqBGVAJXCfMeZ9Y8zdxpisKJSju44aQZBJ5yA6XUh9aaH1f+88cR/Z\n1pla8V09B6aGoDM9lJbbmcoAqRF4XHKFH8rRPZKfPvnbMOFkWXbaTdIzafWf5bXHDf/8ISw7RRo3\nAzUchlV3ypX+6Bmdy0sXAKazneDQRsgeDTmjJRgc/jC82lfFZqje1ft6g2X/GsDA2Pnd38sqgumf\nkf30pXX6q7lajk1qDlRskkGBSkVQNAJBMjAXuNNaOwdoAm4JXMkYs9QYs9YYs7aysnJoSjbcagRN\nVdBYAaOmQ/WOzqv/9fdL2qdgYucVY2BqCDobjP2Xgd/Ecz1c2e98RR4nn9u5bMxMmHIhrPqjnOQf\nXSLP6w/C/RfBM9/q2i319V9JbebMHwZsPx9GTvMLZhukJgBQMkcC7pFesoXOFrj/Ymm7aI9g+83+\n9zp7ZO1fLY3EGfmh1y+Y0Hk131++QDLlAnC3w5EQbURKDZJoBIJyoNxa6xtC+jgSGLqw1t5lrZ1v\nrZ1fXFw8NCUbbjUCX23gjFshNVtqBc5W2PCQpGkmnSGBwFpJ9aRkQnJq5+d9J3z/hmIIb76hHSvk\nlpcjjum6/LQb5cT4hxNh+z/hwt/AdzfBouulXP87FZadCo9dJwFr7rWdt870N36hXGG3NUjNo2S2\nLC+ZI4+9pUQ2PATNVVC7T9ogIuHAesn7//pY6cq5f7W3NtOD/PESDFvr+79dXwP/CZfJo6aHVIQN\neSCw1h4G9htjpngXnQVsGepyBBWq11DHfYuHuEZwxPtrGXeinBQ2PwUbHpQT+LzroGSunJSP7par\ne/+0EPjVCAICQW/zDTlbJNXjXxvwGTsXjrtArvQ/+xCc+FVpAD7nF7D0NSlX9ii5ys8eDaffFHwb\n4xdK+uODRyXv7qsRFE6CtLyeG4w9bnjn/6T30Ywr4e3fRyZF5Pv9z7hS0ldt9VB2Ws+fyZ8gjwOp\nFfg+W3aaHFNtMFYRlhyl7X4beNAYkwrsBr4Y0a01VsIz3wRHMlzdw+AfX848VCAY6tG4FZsgqxiy\nR8LcL8D7f5f++IWTYOKpnSf6g+9L2bqlgLzvB6YyeqsR7H1Laj+Tzwn+/hX3yu8qe2TX5WNmyk84\nxi+UR197gy8QGCO1g56ugrc+J1fN5/wcxn0Ctr8Ey2+EJU/I5wdL9U5JwV10BxiHBNxgtRt/Bd5A\nUPNx13aRvqj5WI57WrbUkA5u6N/3DAc1H3f+TtSwFZXuo9baDd60z0xr7aXW2sidYfe+LY2ZO16S\nvLfHE3rd6l2SXskKSEWNnSfLNz8ZsWIGVbEZRp0gz0sXQNEUSV/NvVbmDCo+XtovDqzvOs+QT681\nghC/9h0rJD024ZTg76dmdg8CfZU3DnLHQtV2yCyS5z5j58q+BxukZa3UAAonwfGflgbmM38Iu14d\n/HmKqndKO0xSsvy+i46Vx57kT5TH2n39327N3s6aRclcqZnEwqSHgTY+Cr+fKSPK1bAWvyOLrYU3\nb4f7Py196k9cCu42aDgY+jMVm6QR07+HDciJdPpn4MMnBpb77QuPW3oHjfQGAmMkDZOWK3PcACSl\nwOiZ3hpBbegaQag2gqrt3adzsFYCQdlp3RvNB5MxcjUPUhvwv5IvmSO9moINpNv7lqSNTv5253Fa\n8FUYNQOe/97Ae+z4q94FI47t22cyC6U9Z6CpId9VdMkc6Wp7+MP+f180OFvg1V/I8zV3Rbcsqlfx\nGwiMkZ4t0y6V3PWUC2W5b8qFQNZ2vQIPNO9L0m//w8ciUdruju6Rq3//8iz4CvxgW9er8bFzJX/d\nXN1DG0FAgEjLhZwSybP/5jgZOexLP1TvkivSUGmhwTTeO6Dc11DsU+LtO7Dxoa5jHdqbYeV/S41t\n1tWdy5OS4cr7ZFTvI5+T0dAD5fFIKiiwsbw3xsjVfH8DktsFtfulJgJyfKH3BuODG+Dus2HXv8Lf\nVktN5KZOWb0M6sslhfnRcqg7EJntqEERv4EA4Lz/lnx2em7nP1ZNiEDQcFju8TtqevD3x86VnO+6\n+4Zm2mFfl8FR0zqXGSO1G38lcyVANR4O0hYQokZgDHzrPbjyrzDpdNj0JNx1Bjz1dXj/b7LOUASC\nSadL7t03TsEnr1TSX+/dDY9/UQJA1Q64+yzpcnrWT7q34xRNlmBwZAs89bWeU4DhqD8g7SR9rRHA\nwLqQ1h+QGoAvNZQzRhrfe2owPvi+zONU/h48ek3vtYeGClh+k1wEPP+9/pWzJ03VUhs/7ny4+P/k\n/2X9/YO/HTVo4jsQJCV3phzyxkljsW/ofqCKTfIYqkZgDMz7ovyTDUUvjorNcpIsPr7n9XzdLaH7\nlX/OKHnMDtL9Ni1beiJdcS98fwss+g5selzy70VTOgNnJBVPgR9sh2PP7rrcGLjo99ITafPTEgDu\nOkPGVCx5QhrOgzn2LDj3v2Rk7+u/HFjZqnfKY38Cga9G0J8LBt/fp+/3b4wE+1A1ggPr4G+XSLD/\n0gp5fHBx8Ctwjxv+9V/w+1kSZPNKpd1rsNOdb/5GOhOc/XMoLJPjsu7+6EzRosISViAwxlxvjMk1\n4h5jzHpjTJC+hcNYUrIEg1Cpod4CAXTOhbPu3sEvX7fybIbCY7pf+QYacWzn9BGBNYLCSXDt8zD1\n4p6/Iz1PTrrfek/mFDrthv6Xu6+CBSmQE+Ci6+HqRyRVMnIafO1NOan0ZOHX5Ti9dbvU8vprIIGg\nYILU0pqr+/5ZX03Cv6dNyRwZa3HHXPi/+fCHBTKO4w8nwl8/LRcA170A4z8h0120N8KDV0pvOZ/2\nZqktvPErOP5Tcqwvu0tqPT01sq+9T3qqhat6F6z5i/wdjfRexMz/stRYP3ox/O9RQyrcGsGXrLX1\nwLlAMdLd87aIlSpSCstCp4YqNkug6GnUaHouzLhCUim+0aaR0lN7hT+Ho7PrZWAbAUDZqdKoHI6C\niTKn0MzFYRcz4qacDz/YCl96CfLG9r6+MTIAz+Pq7JraH9W7JOjnjO77Z/PHy2N/2glq9oJJgtzS\nzmWzroLZSzrTkyOnyQjnkVOlE8N1L3Ruc9QJsPhv0hHg9zNlCpDDmyR19NFyuODXcMU90vZROl8u\nFkLdbrPmY3jxZpk59uN3g6/jz9kqqbyUTDkGPsedJ/uz9p6+/z6GUuMQzWAwDIU7jsDXpeNC4D5r\n7UZjBrPD9hApKAud1gn3xDv785Lv3LUSTrh0cMvn09YoAcvXO6g3JXNg75vdU0PxIi2nb+uPOEZG\nXr93D5z6/b5/HqRGMGJS/8Yl+A8qK53Xt8/WfCwpmyS/f82CiXDpH8P/jmPOhK+/I7Wi1ctkGpDk\ndLjq751TfYPs28yr4LXboK5ctuvvlZ9KejKzCF77b7j2uZ63u/wG6bhw9aOQO6ZzuSNJBhqu/E+o\n2NK13WswHdkGe16XRv6ju6W2M/FUGYFfMrfr7zTQhofh6X+D65bDxEWRKd8wFm6NYJ0xZgUSCF4y\nxuQAA2yNi4LCMulvH9h/3tUmV1DhBAJfdXeg88n0pHKbPIZTHujshtmfq9d4dfL10FYH6//Wv89X\n7+xfWgg60zr9+Rup2Ts4A7CKj4PLlsF31sOi73pThBd1X2/mYsB2nzp73yoZyb7oegmme96QMTmh\nrP+bDHg89QapxQWad53UWJ/8anhjIpqq4Z7zJC3VU1uLxw3blsu8U3/6BLx4k9wKteGwdKle+d9w\nzznw2xNCXwQe2QYvfF+eD3Tm2BgVbiD4MjIx3AJrbTOQQqRHA0eCrwEusJ2g8iNJJYTqMeQvPU9+\navcPevE6+NorRoZ55XT8p6ShcHQY5U8UpfNgwiJ490+9N1K6nV3/JlztMiCsv4EgLUd6bIVKDbU1\nhB7MV/vx4DbUF0z0jsAOMUdS4SS5kPjg0c4TrscD/7xVeiwt+o7cUCh7FLz2P8G/49AH8MINMOnM\n7hMM+mQXw2f+In/by3tpg2qtgwcuk7md3v1D53iEQLX75ST/yNUSuM/6CXx/K9y6H/7tTfm5abf0\njktOlYkRA++y194Ej10rvfHGzpMxNAko3EBwEvCRtbbWGLME+HcgwknyCCgok8fAnkO+gUvhBAKQ\nfOxARo72ZvfrMgVxfphXhsZIQ6Hq6uTvSF/2zU/Ja2dr95lK97wJfz4N7pjTed/k2o+lC2d/AwEE\n70Lq8Ujvmd/NhDtP6X63t7ZGaKoM/7gPlplXSS300EbJk7/9Oxm0d9ZP5QSZkgGnfE/Sj3ve7PpZ\nVzs8/XW52r/8nu6DMf1NPkdqDO8/IFftwbQ3Sa+nis3wuX9IT723bpcbNPnbtVKOW+V2uHQZXP8B\nnPoDyC3pms7LLJTecV9aIf+3D14pExZW75KeVctvlAvBz9wlv4fqnZ33+U4g4bYR3AnMMsbMAm5C\nbirzN+D0SBUsIkKNJajYJDnU3uaR8ckbH7rReaB2viJd+k79Qe/TGaieTT5XusI+8y149jve2WWN\ndMktnScn3i1PywkiOR3W/VXuojaQHkM++RO69uev2ALPfUf6+pcukAFgz34brnqg88Tlu7gYiq67\n/k64TBqF779IJtYDGew386rOdeZdB2/9Dl75GVz3fGdvtrd/J/8/n30Yskb0vq0zfyhX+i/8QBqv\njUOCh8ctPzV7JChdcS8cd670Emurl+3uWyXHye2U+zkXTZHfX1EYxyl3jNxg6aHPSuDyd9pNcMwn\nOwPwjlfgE0t7/85IqT8kN2s6tFH+Ti5b1nMnlkEQbiBwWWutMeYS4PfW2nuMMddGsmARkZYNWSO7\np4YqNsnJoafGJH/546VRytrBneSstR6evV7+wE8LMWunCp/DISf2Dx6RdF5GgVzBHlwveWVnM5x+\nC5zyXXj++/DBY3DOf3QGgnAvDIIpmCAnOo9HAtADl8u9BS5dBrM+K/dxXvEj6c9/4lflM4FjCIZK\nZiGcfrOcfMZ9Qma7LZnb9UIkJQPO+y944ivw98vg6oflhPX6r2D65XD8heFty5EkNYenvy7/h9Yt\ns8+aJBnnk5wu3Vp9U3A7kuCyP0tPpP1rACv/d3OWwPm3dR9g2ZOMAvjC0zL6uq1BGpPTcmT2AZBO\nBoXHSHpoqAOB2wUfvSA3cuq4n7eBouNk/MwwCQQNxphbgWuAU40xDqSdIPYUlgVPDR13XvjfkT9O\n+mq31HRO4zAYXvmZjCz98orIzvOTSCYuCt4LxFq5svTdv2HedTKlxeYnJRBkFA7s2OZPkBN/wyFp\nRG04CF/8J0zwTqux8BuSr37pR9LrKy1XUi8w9IEA4PQbe19nxhVyBf/kUrjvQkhKlS7VF/yqb9vK\nGQXX9GECx6QU6dY8GFIypE0tlMnnyuwB7c0yueJQ2PMmPP0NqNsnF5ln/VRG24+aLhevQyDcQHAV\n8DlkPMFhY8x44NeRK1YEFZTJxGU+jUckLxtu+wB09tmu3Td4gWDvW9LPeuE3u94TV0WGMV1v4jPu\nRCieKjn8lIyBpYWgs+fP/lUyWnvapZ1BAORq+9I7YdkiGTntkzUSMsNIsUTL9M/I3/wjn5eLocvv\nkdt0xovJ58DqO+X/8biAMbNVO6VX0rzrYFovgzTD1VAhN3FKz4OrHpS70vXUzhIhYQUC78n/QWCB\nMebTwBprbT/75UVZwUTpIeFqg+S0zjxuuF01QQaeAdTt7z5hWn9YK7008ifAJ/swilMNHmPkH/yf\nN0t64oTPDOz7fPnmF2+RHmln/6z7OtnF0jf/oxelh07eWAlGw32IzqQzpNZavlbSQvFkwiJJQ+1Y\n0TUQbHnBCueNAAAZjklEQVQGnv4mtDdIiqpkducFYX95PDIvVnuTDAr0dU2PgnCnmFgMrAGuBBYD\nq40xV0SyYBFTWAbYzq59+9fI48g+BAL/GsFg2PkqHP5AbgPZl5ynGlwzF0sQcLX2fdbRQL6LhaYj\nMu1FYVnw9YqnSBvFrKtg4inhNbgOB6NOgHnXDv+g1Vcp6TIF+86X5QKtepdM0PePL8ixum45YCWV\nM9CJDd/9P9i9Es7/n6gGAQg/NfQjZAzBEQBjTDHwCnK/4dji34U0q0gaZyaf27d/wIwC75zzgzSW\n4K3b5cYs/r001NDLLIRpl0iNcaCBICVdrvLdTukBpmLH5HPkftx/WADVO2TZgq/KbMbJqXDBL+WO\nh6v+BCd/q3/bOLBOxkdMvVhqolEWbiBw+IKAVzWxOnOp78qsZo9E4/YGmXCtL4wZvLEE+1bBx29L\nDwj/nLWKjoVfl3l1ertJfTjO+y+ZniFwGnA1vE25EF7/tQyiW/AVaVzOH9f5/uzPS6+zV38h3U77\nOmWGxy29A7NHwcV3DItaVbiB4J/GmJeAh72vrwKWR6ZIEZZVLJOJ7VopffbnLJHJu/oqb5y08g/U\nm7dLD5VQUyuroVUyB743SHcDi7f8eaLILYEbPgr9vm+a9D8thOeulwkR+zLmZ+29UPGhjHgONlFk\nFIRVemvtjcBdwEzvz13W2psjWbCIMUZqBdtflG5pZ4QYEt+bwagRHP5Q7qW88BvaNqBULMkuhnP/\nA8rXwIYQo6SDaT4K//pPmQzPN35hGAg7jFlrn7DWft/781QkCxVxvn7aJ32r6yyJfZE/TuZEGch0\n1G/fIW0NJ36l/9+hlIqOWVfLCOxXftp9upBQ/vUfMpjtwl8Pi5SQT4+BwBjTYIypD/LTYIwZoru4\nR0DpArmiX/Sd/n9HR8+hfjYYt9RKl7RZVw+b6qFSqg+MgQt/I//L//rP3tcvXys3+jlxaf/S0RHU\nYxuBtbYfE7nHgFO+K7WBcKeUCCbPGwjq9vdv1s8tz4C7retN2JVSsWX0dDmxr14mYwsyvYPrfFf7\n1srMBR+9ILcbzSqGM26JXnlDGMCZMMYNJAjAwMcSbHwERkyWu04ppWLXmbfK7T6f/XbodUoXyNQR\nM66M+LxB/ZG4gWCgsoogOaN/gaBmL+x7Bz7542GVJ1RK9UN6HnzjXb/JLANupJNTIvMrDWMaCPrL\nGGkw7k8g8N0NajjdG1gp1X/peYMz3UyUxOagsOEib5y0EfSFtZIWmnjqwOcqUUqpQaCBYCD6M5ag\nfC0c3SVz0iul1DCggWAg8sdBc7XMHhiujQ/LxGZTB2kaW6WUGiANBAPhm2o43LEE21fI7RCnXyE3\n9FBKqWEgaoHAGJNkjHnfGPN8tMowYL6phgPveBbMgXXw2LUweobMXqiUUsNENGsE1wNbo7j9gSua\nLFNEPLUUVt8l9x0NpnoXPLhYBpN8/rEhu/2cUkqFIyqBwBhTCnwKuDsa2x80mYXwlVdlxsoXb4Q/\nnybzB+15A5qq5SbZz34b/vJJuUH3kiche2S0S62UUl1EaxzB74CbgJBTWBhjlgJLAcaPH8bdLEce\nD9c8DVufk/nJX/5x1/dTs+U+pIu+C0UDvA+uUkpFwJAHAu89j49Ya9cZY84ItZ619i5k6mvmz59v\nQ603LBgjN7OedjE0VcGhjVCxCQonwbFny83QlVJqmIpGjWARcLEx5kIgHcg1xjxgrV0ShbIMvqwi\nOPYs+VFKqRgw5G0E1tpbrbWl1tqJwGeBf8VNEFBKqRik4wiUUirBRXXSOWvta8Br0SyDUkolOq0R\nKKVUgtNAoJRSCU4DgVJKJTgNBEopleA0ECilVILTQKCUUglOA4FSSiU4DQRKKZXgNBAopVSC00Cg\nlFIJTgOBUkolOA0ESimV4DQQKKVUgtNAoJRSCU4DgVJKJTgNBEopleA0ECilVILTQKCUUglOA4FS\nSiU4DQRKKZXgNBAopVSC00CglFIJTgOBUkolOA0ESimV4DQQKKVUgtNAoJRSCU4DgVJKJTgNBEop\nleA0ECilVIIb8kBgjBlnjFlpjNlijNlsjLk+Utv6xXNb+P4/NkTq65VSKi4kR2GbLuAH1tr1xpgc\nYJ0x5mVr7ZbB3lBdi5M3tlcN9tcqpVRcGfIagbX2kLV2vfd5A7AVGBuJbU0ryaWqsY0jDa2R+Hql\nlIoLUW0jMMZMBOYAqyPx/VPH5ACw9VBDJL5eKaXiQtQCgTEmG3gC+K61tj7I+0uNMWuNMWsrKyv7\ntY1pY3IB2Hqo29crpZTyikogMMakIEHgQWvtk8HWsdbeZa2db62dX1xc3K/t5GemUpKXroFAKaV6\nEI1eQwa4B9hqrb090tubOiaXLQc1ECilVCjRqBEsAq4BPmmM2eD9uTBSG5tWksvuqiZane5IbUIp\npWLakHcftda+BZih2t7UMbm4PZbtFQ3MLM0fqs0qpVTMiPuRxdpgrJRSPYv7QDC+MJOs1CRtJ1BK\nqRDiPhA4HIbjx+TqWAKllAoh7gMByMCyrYfqsdZGuyhKKTXsJEggyKWhzUV5TUu0i6KUUsNOQgQC\nX4PxFm0wVkqpbhIiEEwZnYMxaIOxUkoFkRCBIDM1mbKiLO1CqpRSQSREIAA4oSSP9/fX4vb0r8HY\nWsvRpnY2HajjnZ1V/f4epZQabqJxY5qoOO+EUTy38SDv7Kri1Ml9m8Ru2+F6rlz2Lg2tro5ld1w9\nh4tnlQx2MZVSasglTI3g7KmjyE1P5ol15X3+7IOr9tHu8vDjT09j2ZK5FGal8tq2IxEopVJKDb2E\nqRGkpyRx0awSnlhfTkOrk5z0lLA+1+p088yGA5w/fTRfPqUMgOUfHuaNHVV4PBaHY8imTVJKqYhI\nmBoBwOXzSml1elj+4aGwP7NiSwX1rS4Wzx/Xsey044qpamxj62FtfFZKxb6ECgRzxuUzqSiLJ9Yd\nCPszj63dz9j8DE6aNKJj2WmTiwB4Y3vVoJdRKaWGWkIFAmMMl88rZc3eo+yrbu51/QO1Lby1s4rL\n55V2SQGNzE3n+NE5vLG9f7fQVEqp4SShAgHAZXPGYgw8sb73RuMn15VjLVw5r7Tbe6cfV8zaj4/S\n1OYK8kmllIodCRcISvIzWHRMEY+8t49DdaHnHrLW8vj6chZOKmRcYWa39087rhin27Jqd3Uki6uU\nUhGXcIEA4IbzptDU5uaKO99lT1VTl/dcbg9v76zihsc+4OPq5i6NxP7mTywgIyWJ1zU9pJSKcQnT\nfdTf7HH5PPzVhVx73xquXPYuty+exaG6FlbtPsqbOyqpamwnMzWJxfNLuXDGmKDfkZacxMJJhTHV\nTlDT1E5+ZgrGaJdXpVSnhAwEADNK8/jH105iyd2r+cK9awAoyk7lpGOKuGD6aM6cMpKM1KQev+O0\n44pZ+VElf317D1WN7Xx8tJmMFAejctMZlZvOwkkjOHZk9lDsTo+a2lzc/vJ27nt7D184aSI/u/iE\naBdJKTWMmFi4Wcv8+fPt2rVrI/LdFfWtvL2zipmleRxTnN2nq+U9VU188n9fw1pIchjG5mfQ5nJT\n2dCGbyqiuePzWTx/HBfMGENeRniD2AbTyo+O8O9PbeJAbQszxubx4YE6fnn5DK5aMH7Iy6KUGlrG\nmHXW2vm9rpfogWCgthysJzXZwfjCTFKTpcnF7bEcqmvhxQ8P8+ja/ew80kiSwzBvQgFnThnJqZOL\nmDoml6QIjkqub3Xyi+e28Pi6co4dmc1tn5nB7HH5fPGv77FqdzWPLD2JeRMKIrZ9pVT0aSAYJqy1\nbCyv4+Uth/nXtsqOqbCz05KZMz6fqWNyGZmTRnFOGqUFmUwbk9trSqo3b++s4qbHP+BQXQvfOONY\nvn3WsaQly3fWNrdzyR/fprndzXPfOoXReekD3kel1PCkgWCYOlzXyuo91azdW8N7e4+yu6qJdpen\n432HgWOKs5k1Lp+TjxnBomOLGJUrJ2uX24Pb2o6TeqDKhjZue3EbT6wvZ1JxFv975SzmjO9+1b+9\nooHL/vg2E4uy+MfXTiIrLWGbipSKaxoIYoS1lvpWF5UNreyubGLzwXo2Hahj/b4aapqdAIzISqW5\n3U2L001qsoMLp4/m8wsnMH9CAW6PpbymhX9tO8JvX9lOq9PNV06dxPVnTSY9JXTNYuVHR/jK/Ws5\nbXIRf/nCfJKTErInsVJxTQNBjPN4LFsO1fPOrir2VDWRnZZMTnoKRxpaeeb9gzS0uSjKTqO2uR2X\nt2X61MlF/OziEzimOLyeSg+t3scPn/qQz31iPD+9aBrbDzey6WAd4wszOfmYEdrNVKkYp4EgjjW3\nu3h+4yHe3V3NmLx0yoqyOG5UDjNL8/p88v7lP7dx52u7SHaYjoACMGNsHt844xjOO2G0TrWtVIzS\nQKDC4vFY/vLmbo42tzNzbD7TSnJZtbuaP7++i73VzeRnpjBjbB6zx+VTVpRFVloy2WnJ5GWkMDI3\njaKsNBwOQ6vTTVVjGwClBd2n5FBKDT0NBGpA3B7LS5sP88b2SjaW17G9oiHofZqTHYa0ZAdN7e6O\nZccUZ3HuCaM5adIInG4P9a1OnG7LvAkFTCrK0pSTUkNkWAcCY8z5wO+BJOBua+1tPa2vgSD6Wtrd\nHK5vpanNRVObi5pmJ5UNrVTUt9HULu0VRdmpNLW5eXVbBat2Hw0aOMbkpXNiWSGZqUlYC9ZCWoqD\njJQkMlKTKMnLYMKITEoLMzlc18KWg/VsO9zA+MJMzp8+mgkjsqKw90rFpmEbCIwxScB24BygHHgP\nuNpauyXUZzQQxJ66ZiebD9aRlZZMbkYKbo9lzZ6jvLWzkvf31eLyWHxND20uD61ON61OT9DvyklL\npsE73ffxo3Moyc+grsVJfYuTvIwUppXkMm1MLhOLshiRlUpBVir5GSkdPaGstRyqa+WD8jrqWtqZ\nO76AY0f2Porc5fawq7KJqsY2apudNLY5mTgiixPG5pHt1+W2zeUmNckxJDWd5nYX1Y3tlBZkRGx7\nze0uHMb02OtMxYbhHAhOAn5mrT3P+/pWAGvt/4T6jAaCxOByezhY28re6ib21zQzMiedE0pyGZOX\nTnlNCy9tPszLWypobHORl5FCbnoKR5va2XKonsYg94XI8QahNpebqsb2Lu+NyEplZmkeWWnJpCY7\nSEtO8tZKHLg98EF5LRv219Lsl/LyMQbKRmThsZaqxnYa21ykJjsoyUtnTF4GRTlp5GUkk5+Rysjc\nNEoLMhhXkClTjBgwyAncWovHQmObkwO1rRysbaGpzUVhViojstNIdhh2Vzay80gjOysb2VPZxMG6\nVgCKc9I45dgi5k4o4GhjO3uqGimvacFjLUkOQ0qSg0nFWcwYm8e0MXlUN7Wx5VA92w41kJrsYGx+\nBmMLMryj4Fs5VNvCvqPN7K1uoqK+jbRkByeWFXL6ccVMK8nF4Q06KUkOirKlfOnJDo42tVPV2E5z\nu5S7KCeNnLRkPBZvcHfT1Oamsc1Fq8tNUVYao/LSuo2FsdbS2OaisqGNjNQkirLTSAmjS7PLLRcP\n/kG/qrGd3ZWNNLS6mFiUybjCzJBjb+LdcA4EVwDnW2u/4n19DfAJa+23Qn1GA4Hqicdj2V/TTHlN\nC0eb2jna1E5Nczt1LU7qmp0kOQwzSvOYMTaPvIwU1u6tYdXuarYebqDN5abN6aHNJTWSVqcbC0wd\nk8P8CYXMHpfP6Lx0CjJTyUxNYueRRj4or2PLoTpSk5Moyk6lMDOVxjYXB+vkZH60ybvtFmfQ9Fhf\nZaUmcezIbCYVZzOpKIv8rFTe23OUt3dWUd3UjjFQkpfBuMIMUpIcuNyWNpebHRWNHTUpn7H5cvKv\naGjF969vDIz0jmyfOCKLsqJMapqdvL69kp1HGvtcXoeB3na7MCuVtGQHvjpNTbOTFmdn0DUGCjNT\nSU124PZYPNZKKhE52bvcllaXG6dbNpSe4iA7LZk2l4eG1q777DBQlJ3WEcx8y3w1KpfHg9Ntcbo8\nOD0eXG6L21pSkhykeS8SUpMMyUkOkpMMTreHlnb5WzFGZiJO804v47HWW1557rEWgwTQ1GQHSQ7j\n3Q+LxwPtbg9OtwePx5KWkkR6ioMUhwOnx0O7S36e+sYiJhb1LyUabiAYtkNKjTFLgaUA48frBGkq\nNIfDMGFEVtjtB5OKs1m8IPh9JkACS6gus+MKMznz+JFhbcfjsVQ1tbH/aAvlNc00tLqwgO8M7HAY\nHMaQmZpESX4GJfkZZKclU9PUTlVjG+0uD2XFWYzOTe+WBrpm4QQ8HsvBuhaKstOCpnE8HsvHR5vZ\ncrCeEdmpTB2T2zHxYZvLzaHaVlKTHRTnBL/6/jFyu9aPqzvv2dHm8nC0sZ3qpjaa292MyE6jKCuV\njNQkaprbqWxoo67FSWpSEmkpDtKTHWSlJZOTnkxachKVjW0crmvlcH0rLrcHj7edqCBTeqEV56TR\n0u6hor6VIw1tuNwekhzG+7uSMhgMSQ5DRqrU4qyFpnYXjW0ukh2GsqIsJhVnk5OezL7qZnZXNVHh\nrUmBnIStlWBlsaQmOUjxnuRTk+RkneQwOL0BtdXpweU9YTs9sn6696RtrZzM25weLJYkY7qUN8kY\nPBacbg/tbg9ujwQGY+TYpyabjrSib1tOt6ejTKnJDjIHOOVMODQ1pJRScSrcGkE05hV4D5hsjCkz\nxqQCnwWejUI5lFJKEYXUkLXWZYz5FvAS0n30Xmvt5qEuh1JKKRGVNgJr7XJgeTS2rZRSqiudclIp\npRKcBgKllEpwGgiUUirBaSBQSqkEp4FAKaUSXExMQ22MqQQ+7ufHi4CqQSxOrEjE/U7EfYbE3G/d\n5/BMsNYW97ZSTASCgTDGrA1nZF28ScT9TsR9hsTcb93nwaWpIaWUSnAaCJRSKsElQiC4K9oFiJJE\n3O9E3GdIzP3WfR5Ecd9GoJRSqmeJUCNQSinVg7gOBMaY840xHxljdhpjbol2eSLBGDPOGLPSGLPF\nGLPZGHO9d3mhMeZlY8wO72NBtMs62IwxScaY940xz3tflxljVnuP96Peac7jijEm3xjzuDFmmzFm\nqzHmpHg/1saY73n/tjcZYx42xqTH47E2xtxrjDlijNnktyzosTXiDu/+f2CMmTuQbcdtIDDGJAF/\nBC4ApgFXG2OmRbdUEeECfmCtnQYsBL7p3c9bgFettZOBV72v4831wFa/178EfmutPRaoAb4clVJF\n1u+Bf1prjwdmIfsft8faGDMW+A4w31o7HZm6/rPE57H+K3B+wLJQx/YCYLL3Zylw50A2HLeBADgR\n2Gmt3W2tbQceAS6JcpkGnbX2kLV2vfd5A3JiGIvs6/3e1e4HLo1OCSPDGFMKfAq42/vaAJ8EHveu\nEo/7nAecBtwDYK1tt9bWEufHGpkuP8MYkwxkAoeIw2NtrX0DOBqwONSxvQT4mxWrgHxjzJj+bjue\nA8FYYL/f63LvsrhljJkIzAFWA6OstYe8bx0GRkWpWJHyO+AmwON9PQKotdb67lwej8e7DKgE7vOm\nxO42xmQRx8faWnsA+A2wDwkAdcA64v9Y+4Q6toN6fovnQJBQjDHZwBPAd6219f7vWekaFjfdw4wx\nnwaOWGvXRbssQywZmAvcaa2dAzQRkAaKw2NdgFz9lgElQBbd0ycJIZLHNp4DwQFgnN/rUu+yuGOM\nSUGCwIPW2ie9iyt8VUXv45FolS8CFgEXG2P2Iim/TyK583xv+gDi83iXA+XW2tXe148jgSGej/XZ\nwB5rbaW11gk8iRz/eD/WPqGO7aCe3+I5ELwHTPb2LkhFGpiejXKZBp03N34PsNVae7vfW88C13qf\nXws8M9RlixRr7a3W2lJr7UTkuP7LWvt5YCVwhXe1uNpnAGvtYWC/MWaKd9FZwBbi+FgjKaGFxphM\n79+6b5/j+lj7CXVsnwW+4O09tBCo80sh9Z21Nm5/gAuB7cAu4EfRLk+E9vEUpLr4AbDB+3MhkjN/\nFdgBvAIURrusEdr/M4Dnvc8nAWuAncBjQFq0yxeB/Z0NrPUe76eBgng/1sDPgW3AJuDvQFo8Hmvg\nYaQdxInU/r4c6tgCBukVuQv4EOlV1e9t68hipZRKcPGcGlJKKRUGDQRKKZXgNBAopVSC00CglFIJ\nTgOBUkolOA0ESkWYMeYM3wypSg1HGgiUUirBaSBQyssYs8QYs8YYs8EY82fv/Q4ajTH/a4xZb4x5\n1RhT7F13tjFmlXcu+Kf85ok/1hjzijFmo/czx3i/PtvvPgIPekfJKjUsaCBQCjDGTAWuAhZZa2cD\nbuDzyCRn6621c4HXgZ96P/I34GZr7UxkZKdv+YPAH621s4CTkZGiILPCfhe5N8YkZL4cpYaF5N5X\nUSohnAXMA97zXqxnIBN8eYBHves8ADzpvS9AvrX2de/y+4HHjDE5wFhr7VMA1tpWAO/3rbHWlntf\nbwAmAm9FfreU6p0GAqWEAe631t7aZaExPw5Yr79zsrT5PXej/3tqGNHUkFLiVeAKY8xI6LhX7ATk\nf8Q3y+XngLestXVAjTHmVO/ya4DXrdwhrtwYc6n3O9KMMZlDuhdK9YNelSgFWGu3GGP+HVhhjHEg\nM0B+E7n5ywnGmHXI3bGu8n7kWmCZ90S/G/iid/k1wJ+NMb/wfseVQ7gbSvWLzj6qVA+MMY3W2uxo\nl0OpSNLUkFJKJTitESilVILTGoFSSiU4DQRKKZXgNBAopVSC00CglFIJTgOBUkolOA0ESimV4P4f\nnaaItb+RRCoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1bb8109aeb8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# preprocess\n",
    "x_train_pp = x_train - np.mean(x_train)\n",
    "x_train_pp /= np.std(x_train_pp, axis=0)\n",
    "\n",
    "x_test_pp  = x_test - np.mean(x_train) \n",
    "x_test_pp /= np.std(x_train_pp, axis=0)\n",
    "\n",
    "# build model\n",
    "model_name = '1st_place'\n",
    "model = Sequential(name=model_name)\n",
    "model.add(Cropping2D(cropping=3, input_shape=image_shape))\n",
    "model.add(Conv2D(32, (5,5), padding='same', activation='relu', kernel_initializer='TruncatedNormal'))\n",
    "model.add(MaxPooling2D((2,2), strides=(2,2), padding='valid'))\n",
    "\n",
    "model.add(ZeroPadding2D(padding=1))\n",
    "model.add(Conv2D(32, (4,4), padding='valid', activation='relu', kernel_initializer='TruncatedNormal'))\n",
    "model.add(AveragePooling2D((2,2), strides=(2,2), padding='valid'))\n",
    "\n",
    "model.add(Conv2D(64, (5,5), padding='same', activation='relu', kernel_initializer='TruncatedNormal'))\n",
    "model.add(AveragePooling2D((2,2), strides=(2,2), padding='valid'))\n",
    "\n",
    "model.add(Flatten())\n",
    "# TODO is this where dropout should be?\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(3072, activation='relu', kernel_initializer='TruncatedNormal'))\n",
    "# TODO svm activation function\n",
    "model.add(Dense(n_classes, activation='softmax', kernel_initializer='TruncatedNormal'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# compile model\n",
    "sgd = optimizers.SGD(momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "# fit model\n",
    "epoch_sets = 1\n",
    "epochs = 100\n",
    "batch_size = 60\n",
    "for es in range(epoch_sets):\n",
    "    # get current lr\n",
    "    lr = k.get_value(sgd.lr)\n",
    "    print('epoch set:', es, 'lr value:', lr)\n",
    "    \n",
    "    # fit model\n",
    "    history = model.fit(x_train_pp, y_train_onehot\n",
    "                        , batch_size = batch_size\n",
    "                        , epochs=epochs\n",
    "                        , validation_data=(x_test_pp, y_test_onehot))\n",
    "    # decay lr by half\n",
    "    lr = lr / 2\n",
    "    k.set_value(sgd.lr, lr)\n",
    "    \n",
    "# visualize loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocess: manual center & normalize + sgd_decay on tiny subset + batchsize=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "cropping2d_10 (Cropping2D)   (None, 42, 42, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_28 (Conv2D)           (None, 42, 42, 32)        832       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling (None, 21, 21, 32)        0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_10 (ZeroPaddi (None, 23, 23, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_29 (Conv2D)           (None, 20, 20, 32)        16416     \n",
      "_________________________________________________________________\n",
      "average_pooling2d_19 (Averag (None, 10, 10, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_30 (Conv2D)           (None, 10, 10, 64)        51264     \n",
      "_________________________________________________________________\n",
      "average_pooling2d_20 (Averag (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 3072)              4918272   \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 7)                 21511     \n",
      "=================================================================\n",
      "Total params: 5,008,295\n",
      "Trainable params: 5,008,295\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "epoch set: 0 lr value: 0.01\n",
      "Train on 50 samples, validate on 50 samples\n",
      "Epoch 1/50\n",
      "50/50 [==============================] - 0s - loss: 2.0302 - acc: 0.0200 - val_loss: 4.5920 - val_acc: 0.6400\n",
      "Epoch 2/50\n",
      "50/50 [==============================] - 0s - loss: 1.2807 - acc: 0.6200 - val_loss: 5.8025 - val_acc: 0.6400\n",
      "Epoch 3/50\n",
      "50/50 [==============================] - 0s - loss: 0.8261 - acc: 0.6000 - val_loss: 3.6771 - val_acc: 0.6600\n",
      "Epoch 4/50\n",
      "50/50 [==============================] - 0s - loss: 0.6386 - acc: 0.6800 - val_loss: 5.5559 - val_acc: 0.6400\n",
      "Epoch 5/50\n",
      "50/50 [==============================] - 0s - loss: 0.6887 - acc: 0.5800 - val_loss: 8.8224 - val_acc: 0.3400\n",
      "Epoch 6/50\n",
      "50/50 [==============================] - 0s - loss: 0.7786 - acc: 0.5000 - val_loss: 5.8025 - val_acc: 0.6400\n",
      "Epoch 7/50\n",
      "50/50 [==============================] - 0s - loss: 2.5116 - acc: 0.6000 - val_loss: 10.3156 - val_acc: 0.3600\n",
      "Epoch 8/50\n",
      "50/50 [==============================] - 0s - loss: 1.1218 - acc: 0.4000 - val_loss: 3.1871 - val_acc: 0.5800\n",
      "Epoch 9/50\n",
      "50/50 [==============================] - 0s - loss: 0.8055 - acc: 0.7000 - val_loss: 2.2859 - val_acc: 0.7400\n",
      "Epoch 10/50\n",
      "50/50 [==============================] - 0s - loss: 0.7725 - acc: 0.6800 - val_loss: 2.2986 - val_acc: 0.7800\n",
      "Epoch 11/50\n",
      "50/50 [==============================] - 0s - loss: 0.6759 - acc: 0.6800 - val_loss: 2.3271 - val_acc: 0.8200\n",
      "Epoch 12/50\n",
      "50/50 [==============================] - 0s - loss: 0.6187 - acc: 0.6600 - val_loss: 2.5353 - val_acc: 0.8000\n",
      "Epoch 13/50\n",
      "50/50 [==============================] - 0s - loss: 0.5766 - acc: 0.7200 - val_loss: 3.3792 - val_acc: 0.7400\n",
      "Epoch 14/50\n",
      "50/50 [==============================] - 0s - loss: 0.5362 - acc: 0.7600 - val_loss: 4.3462 - val_acc: 0.6600\n",
      "Epoch 15/50\n",
      "50/50 [==============================] - 0s - loss: 0.5194 - acc: 0.8000 - val_loss: 4.3569 - val_acc: 0.6800\n",
      "Epoch 16/50\n",
      "50/50 [==============================] - 0s - loss: 0.4537 - acc: 0.7600 - val_loss: 4.5124 - val_acc: 0.6400\n",
      "Epoch 17/50\n",
      "50/50 [==============================] - 0s - loss: 0.4466 - acc: 0.8000 - val_loss: 4.9355 - val_acc: 0.6800\n",
      "Epoch 18/50\n",
      "50/50 [==============================] - 0s - loss: 0.5166 - acc: 0.7600 - val_loss: 8.1606 - val_acc: 0.4800\n",
      "Epoch 19/50\n",
      "50/50 [==============================] - 0s - loss: 0.9425 - acc: 0.6000 - val_loss: 5.8025 - val_acc: 0.6400\n",
      "Epoch 20/50\n",
      "50/50 [==============================] - 0s - loss: 1.5909 - acc: 0.6000 - val_loss: 4.6474 - val_acc: 0.7000\n",
      "Epoch 21/50\n",
      "50/50 [==============================] - 0s - loss: 0.3902 - acc: 0.8200 - val_loss: 5.0488 - val_acc: 0.6400\n",
      "Epoch 22/50\n",
      "50/50 [==============================] - 0s - loss: 0.4211 - acc: 0.8400 - val_loss: 5.1890 - val_acc: 0.6200\n",
      "Epoch 23/50\n",
      "50/50 [==============================] - 0s - loss: 0.4420 - acc: 0.8400 - val_loss: 4.6804 - val_acc: 0.6400\n",
      "Epoch 24/50\n",
      "50/50 [==============================] - 0s - loss: 0.4472 - acc: 0.8400 - val_loss: 4.4170 - val_acc: 0.6200\n",
      "Epoch 25/50\n",
      "50/50 [==============================] - 0s - loss: 0.3920 - acc: 0.8400 - val_loss: 4.5124 - val_acc: 0.6400\n",
      "Epoch 26/50\n",
      "50/50 [==============================] - 0s - loss: 0.3956 - acc: 0.8400 - val_loss: 4.8128 - val_acc: 0.6200\n",
      "Epoch 27/50\n",
      "50/50 [==============================] - 0s - loss: 0.3579 - acc: 0.8400 - val_loss: 4.9340 - val_acc: 0.6400\n",
      "Epoch 28/50\n",
      "50/50 [==============================] - 0s - loss: 0.3544 - acc: 0.8600 - val_loss: 5.1046 - val_acc: 0.6400\n",
      "Epoch 29/50\n",
      "50/50 [==============================] - 0s - loss: 0.3080 - acc: 0.9200 - val_loss: 5.5024 - val_acc: 0.6400\n",
      "Epoch 30/50\n",
      "50/50 [==============================] - 0s - loss: 0.2611 - acc: 0.9000 - val_loss: 5.6052 - val_acc: 0.6400\n",
      "Epoch 31/50\n",
      "50/50 [==============================] - 0s - loss: 0.2604 - acc: 0.8400 - val_loss: 5.5333 - val_acc: 0.6400\n",
      "Epoch 32/50\n",
      "50/50 [==============================] - 0s - loss: 0.2176 - acc: 0.8800 - val_loss: 5.4849 - val_acc: 0.6200\n",
      "Epoch 33/50\n",
      "50/50 [==============================] - 0s - loss: 0.2278 - acc: 0.8800 - val_loss: 5.5096 - val_acc: 0.6400\n",
      "Epoch 34/50\n",
      "50/50 [==============================] - 0s - loss: 0.1945 - acc: 0.9200 - val_loss: 6.6000 - val_acc: 0.5800\n",
      "Epoch 35/50\n",
      "50/50 [==============================] - 0s - loss: 0.2483 - acc: 0.9000 - val_loss: 6.0395 - val_acc: 0.6000\n",
      "Epoch 36/50\n",
      "50/50 [==============================] - 0s - loss: 0.1666 - acc: 0.9000 - val_loss: 5.8201 - val_acc: 0.6400\n",
      "Epoch 37/50\n",
      "50/50 [==============================] - 0s - loss: 0.1644 - acc: 0.9400 - val_loss: 5.8093 - val_acc: 0.6400\n",
      "Epoch 38/50\n",
      "50/50 [==============================] - 0s - loss: 0.1756 - acc: 0.9200 - val_loss: 5.8052 - val_acc: 0.6400\n",
      "Epoch 39/50\n",
      "50/50 [==============================] - 0s - loss: 0.2118 - acc: 0.9200 - val_loss: 6.4096 - val_acc: 0.6000\n",
      "Epoch 40/50\n",
      "50/50 [==============================] - 0s - loss: 0.1336 - acc: 0.9600 - val_loss: 5.7770 - val_acc: 0.6200\n",
      "Epoch 41/50\n",
      "50/50 [==============================] - 0s - loss: 0.1148 - acc: 0.9600 - val_loss: 5.9967 - val_acc: 0.6200\n",
      "Epoch 42/50\n",
      "50/50 [==============================] - 0s - loss: 0.1213 - acc: 0.9600 - val_loss: 5.6460 - val_acc: 0.6200\n",
      "Epoch 43/50\n",
      "50/50 [==============================] - 0s - loss: 0.1393 - acc: 0.9400 - val_loss: 6.1030 - val_acc: 0.6000\n",
      "Epoch 44/50\n",
      "50/50 [==============================] - 0s - loss: 0.0802 - acc: 0.9800 - val_loss: 6.0260 - val_acc: 0.6000\n",
      "Epoch 45/50\n",
      "50/50 [==============================] - 0s - loss: 0.0821 - acc: 0.9800 - val_loss: 5.4958 - val_acc: 0.6400\n",
      "Epoch 46/50\n",
      "50/50 [==============================] - 0s - loss: 0.0911 - acc: 0.9800 - val_loss: 5.5166 - val_acc: 0.6400\n",
      "Epoch 47/50\n",
      "50/50 [==============================] - 0s - loss: 0.0603 - acc: 0.9800 - val_loss: 5.7221 - val_acc: 0.6200\n",
      "Epoch 48/50\n",
      "50/50 [==============================] - 0s - loss: 0.0555 - acc: 0.9800 - val_loss: 5.7878 - val_acc: 0.6200\n",
      "Epoch 49/50\n",
      "50/50 [==============================] - 0s - loss: 0.0802 - acc: 0.9600 - val_loss: 5.5693 - val_acc: 0.6200\n",
      "Epoch 50/50\n",
      "50/50 [==============================] - 0s - loss: 0.1179 - acc: 0.9400 - val_loss: 5.8029 - val_acc: 0.6400\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1bb8d379080>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEKCAYAAAAfGVI8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1bb8bb22978>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# preprocess\n",
    "x_train_pp = x_train - np.mean(x_train)\n",
    "x_train_pp /= np.std(x_train_pp, axis=0)\n",
    "\n",
    "x_test_pp  = x_test - np.mean(x_train) \n",
    "x_test_pp /= np.std(x_train_pp, axis=0)\n",
    "\n",
    "# build model\n",
    "model_name = '1st_place'\n",
    "model = Sequential(name=model_name)\n",
    "model.add(Cropping2D(cropping=3, input_shape=image_shape))\n",
    "model.add(Conv2D(32, (5,5), padding='same', activation='relu', kernel_initializer='TruncatedNormal'))\n",
    "model.add(MaxPooling2D((2,2), strides=(2,2), padding='valid'))\n",
    "\n",
    "model.add(ZeroPadding2D(padding=1))\n",
    "model.add(Conv2D(32, (4,4), padding='valid', activation='relu', kernel_initializer='TruncatedNormal'))\n",
    "model.add(AveragePooling2D((2,2), strides=(2,2), padding='valid'))\n",
    "\n",
    "model.add(Conv2D(64, (5,5), padding='same', activation='relu', kernel_initializer='TruncatedNormal'))\n",
    "model.add(AveragePooling2D((2,2), strides=(2,2), padding='valid'))\n",
    "\n",
    "model.add(Flatten())\n",
    "# TODO is this where dropout should be?\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(3072, activation='relu', kernel_initializer='TruncatedNormal'))\n",
    "# TODO svm activation function\n",
    "model.add(Dense(n_classes, activation='softmax', kernel_initializer='TruncatedNormal'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# compile model\n",
    "sgd = optimizers.SGD(momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "# fit model\n",
    "epoch_sets = 1\n",
    "epochs = 50\n",
    "batch_size = 100\n",
    "for es in range(epoch_sets):\n",
    "    # get current lr\n",
    "    lr = k.get_value(sgd.lr)\n",
    "    print('epoch set:', es, 'lr value:', lr)\n",
    "    \n",
    "    # fit model\n",
    "    history = model.fit(x_train_pp, y_train_onehot\n",
    "                        , batch_size = batch_size\n",
    "                        , epochs=epochs\n",
    "                        , validation_data=(x_test_pp, y_test_onehot))\n",
    "    # decay lr by half\n",
    "    lr = lr / 2\n",
    "    k.set_value(sgd.lr, lr)\n",
    "    \n",
    "# visualize loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocess: manual center & normalize + activation tanh + sgd_decay on tiny subset + batchsize=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "cropping2d_11 (Cropping2D)   (None, 42, 42, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_31 (Conv2D)           (None, 42, 42, 32)        832       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling (None, 21, 21, 32)        0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_11 (ZeroPaddi (None, 23, 23, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_32 (Conv2D)           (None, 20, 20, 32)        16416     \n",
      "_________________________________________________________________\n",
      "average_pooling2d_21 (Averag (None, 10, 10, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_33 (Conv2D)           (None, 10, 10, 64)        51264     \n",
      "_________________________________________________________________\n",
      "average_pooling2d_22 (Averag (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 3072)              4918272   \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 7)                 21511     \n",
      "=================================================================\n",
      "Total params: 5,008,295\n",
      "Trainable params: 5,008,295\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "epoch set: 0 lr value: 0.01\n",
      "Train on 50 samples, validate on 50 samples\n",
      "Epoch 1/50\n",
      "50/50 [==============================] - 0s - loss: 2.0145 - acc: 0.1000 - val_loss: 1.0156 - val_acc: 0.5400\n",
      "Epoch 2/50\n",
      "50/50 [==============================] - 0s - loss: 1.1019 - acc: 0.5600 - val_loss: 1.0329 - val_acc: 0.4600\n",
      "Epoch 3/50\n",
      "50/50 [==============================] - 0s - loss: 0.6489 - acc: 0.7400 - val_loss: 0.8668 - val_acc: 0.6800\n",
      "Epoch 4/50\n",
      "50/50 [==============================] - 0s - loss: 0.4496 - acc: 0.8400 - val_loss: 1.0874 - val_acc: 0.4800\n",
      "Epoch 5/50\n",
      "50/50 [==============================] - 0s - loss: 0.4586 - acc: 0.7000 - val_loss: 0.8928 - val_acc: 0.6600\n",
      "Epoch 6/50\n",
      "50/50 [==============================] - 0s - loss: 0.5310 - acc: 0.7600 - val_loss: 1.7491 - val_acc: 0.4600\n",
      "Epoch 7/50\n",
      "50/50 [==============================] - 0s - loss: 0.5305 - acc: 0.7200 - val_loss: 1.1013 - val_acc: 0.6400\n",
      "Epoch 8/50\n",
      "50/50 [==============================] - 0s - loss: 0.7022 - acc: 0.7800 - val_loss: 1.7457 - val_acc: 0.5200\n",
      "Epoch 9/50\n",
      "50/50 [==============================] - 0s - loss: 0.4701 - acc: 0.8400 - val_loss: 1.4124 - val_acc: 0.5400\n",
      "Epoch 10/50\n",
      "50/50 [==============================] - 0s - loss: 0.3382 - acc: 0.8600 - val_loss: 1.6038 - val_acc: 0.5200\n",
      "Epoch 11/50\n",
      "50/50 [==============================] - 0s - loss: 0.4251 - acc: 0.8200 - val_loss: 1.9133 - val_acc: 0.5400\n",
      "Epoch 12/50\n",
      "50/50 [==============================] - 0s - loss: 0.4244 - acc: 0.8600 - val_loss: 1.8542 - val_acc: 0.4400\n",
      "Epoch 13/50\n",
      "50/50 [==============================] - 0s - loss: 0.2748 - acc: 0.8800 - val_loss: 1.6557 - val_acc: 0.5200\n",
      "Epoch 14/50\n",
      "50/50 [==============================] - 0s - loss: 0.1889 - acc: 0.9400 - val_loss: 1.7639 - val_acc: 0.5400\n",
      "Epoch 15/50\n",
      "50/50 [==============================] - 0s - loss: 0.1850 - acc: 0.8600 - val_loss: 1.7034 - val_acc: 0.5000\n",
      "Epoch 16/50\n",
      "50/50 [==============================] - 0s - loss: 0.1204 - acc: 0.9600 - val_loss: 1.7637 - val_acc: 0.5200\n",
      "Epoch 17/50\n",
      "50/50 [==============================] - 0s - loss: 0.1192 - acc: 0.9600 - val_loss: 1.9129 - val_acc: 0.5600\n",
      "Epoch 18/50\n",
      "50/50 [==============================] - 0s - loss: 0.1047 - acc: 0.9600 - val_loss: 1.9242 - val_acc: 0.5200\n",
      "Epoch 19/50\n",
      "50/50 [==============================] - 0s - loss: 0.0762 - acc: 0.9600 - val_loss: 1.8566 - val_acc: 0.5600\n",
      "Epoch 20/50\n",
      "50/50 [==============================] - 0s - loss: 0.0723 - acc: 0.9800 - val_loss: 1.8891 - val_acc: 0.5600\n",
      "Epoch 21/50\n",
      "50/50 [==============================] - 0s - loss: 0.0675 - acc: 1.0000 - val_loss: 1.7915 - val_acc: 0.6200\n",
      "Epoch 22/50\n",
      "50/50 [==============================] - 0s - loss: 0.0602 - acc: 0.9800 - val_loss: 1.8860 - val_acc: 0.6200\n",
      "Epoch 23/50\n",
      "50/50 [==============================] - 0s - loss: 0.0552 - acc: 0.9800 - val_loss: 2.0542 - val_acc: 0.5200\n",
      "Epoch 24/50\n",
      "50/50 [==============================] - 0s - loss: 0.0559 - acc: 0.9800 - val_loss: 1.9505 - val_acc: 0.6400\n",
      "Epoch 25/50\n",
      "50/50 [==============================] - 0s - loss: 0.0394 - acc: 1.0000 - val_loss: 1.9050 - val_acc: 0.6400\n",
      "Epoch 26/50\n",
      "50/50 [==============================] - 0s - loss: 0.0271 - acc: 1.0000 - val_loss: 1.8675 - val_acc: 0.6600\n",
      "Epoch 27/50\n",
      "50/50 [==============================] - 0s - loss: 0.0309 - acc: 1.0000 - val_loss: 1.8950 - val_acc: 0.6400\n",
      "Epoch 28/50\n",
      "50/50 [==============================] - 0s - loss: 0.0241 - acc: 1.0000 - val_loss: 1.9640 - val_acc: 0.6200\n",
      "Epoch 29/50\n",
      "50/50 [==============================] - 0s - loss: 0.0271 - acc: 1.0000 - val_loss: 1.9827 - val_acc: 0.6200\n",
      "Epoch 30/50\n",
      "50/50 [==============================] - 0s - loss: 0.0414 - acc: 0.9800 - val_loss: 2.0456 - val_acc: 0.6400\n",
      "Epoch 31/50\n",
      "50/50 [==============================] - 0s - loss: 0.0178 - acc: 1.0000 - val_loss: 2.1492 - val_acc: 0.5800\n",
      "Epoch 32/50\n",
      "50/50 [==============================] - 0s - loss: 0.0284 - acc: 1.0000 - val_loss: 2.1099 - val_acc: 0.5800\n",
      "Epoch 33/50\n",
      "50/50 [==============================] - 0s - loss: 0.0274 - acc: 1.0000 - val_loss: 1.9915 - val_acc: 0.7000\n",
      "Epoch 34/50\n",
      "50/50 [==============================] - 0s - loss: 0.0357 - acc: 1.0000 - val_loss: 2.0861 - val_acc: 0.6800\n",
      "Epoch 35/50\n",
      "50/50 [==============================] - 0s - loss: 0.0210 - acc: 1.0000 - val_loss: 2.1485 - val_acc: 0.6800\n",
      "Epoch 36/50\n",
      "50/50 [==============================] - 0s - loss: 0.0081 - acc: 1.0000 - val_loss: 2.1918 - val_acc: 0.6400\n",
      "Epoch 37/50\n",
      "50/50 [==============================] - 0s - loss: 0.0194 - acc: 1.0000 - val_loss: 2.1652 - val_acc: 0.7000\n",
      "Epoch 38/50\n",
      "50/50 [==============================] - 0s - loss: 0.0067 - acc: 1.0000 - val_loss: 2.1625 - val_acc: 0.7200\n",
      "Epoch 39/50\n",
      "50/50 [==============================] - 0s - loss: 0.0085 - acc: 1.0000 - val_loss: 2.1790 - val_acc: 0.7200\n",
      "Epoch 40/50\n",
      "50/50 [==============================] - 0s - loss: 0.0074 - acc: 1.0000 - val_loss: 2.1968 - val_acc: 0.7200\n",
      "Epoch 41/50\n",
      "50/50 [==============================] - 0s - loss: 0.0050 - acc: 1.0000 - val_loss: 2.2067 - val_acc: 0.7000\n",
      "Epoch 42/50\n",
      "50/50 [==============================] - 0s - loss: 0.0049 - acc: 1.0000 - val_loss: 2.2299 - val_acc: 0.6800\n",
      "Epoch 43/50\n",
      "50/50 [==============================] - 0s - loss: 0.0050 - acc: 1.0000 - val_loss: 2.2476 - val_acc: 0.6800\n",
      "Epoch 44/50\n",
      "50/50 [==============================] - 0s - loss: 0.0084 - acc: 1.0000 - val_loss: 2.2486 - val_acc: 0.6800\n",
      "Epoch 45/50\n",
      "50/50 [==============================] - 0s - loss: 0.0070 - acc: 1.0000 - val_loss: 2.2343 - val_acc: 0.6800\n",
      "Epoch 46/50\n",
      "50/50 [==============================] - 0s - loss: 0.0040 - acc: 1.0000 - val_loss: 2.2346 - val_acc: 0.6800\n",
      "Epoch 47/50\n",
      "50/50 [==============================] - 0s - loss: 0.0044 - acc: 1.0000 - val_loss: 2.2301 - val_acc: 0.6800\n",
      "Epoch 48/50\n",
      "50/50 [==============================] - 0s - loss: 0.0117 - acc: 1.0000 - val_loss: 2.2683 - val_acc: 0.6800\n",
      "Epoch 49/50\n",
      "50/50 [==============================] - 0s - loss: 0.0080 - acc: 1.0000 - val_loss: 2.2805 - val_acc: 0.6600\n",
      "Epoch 50/50\n",
      "50/50 [==============================] - 0s - loss: 0.0037 - acc: 1.0000 - val_loss: 2.2983 - val_acc: 0.6600\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1bb8db33b38>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3XecVOW9+PHPd2Zne28s7NJBehNEwIYFRDRW7JpoosSU\nG2/uTaLGxMTca5Lfzb3eRI2xkmvXiKJEUUHFaBSQItKVDruUXRa295nn98czs40tszBld+f7fr3m\nNTPnnDnnObrMd572fcQYg1JKKQXgCHcBlFJKdR8aFJRSSjXSoKCUUqqRBgWllFKNNCgopZRqpEFB\nKaVUIw0KSimlGmlQUEop1UiDglJKqUZR4S5AV2VmZppBgwaFuxhKKdWjrF279ogxJquz43pcUBg0\naBBr1qwJdzGUUqpHEZG9/hynzUdKKaUaaVBQSinVSIOCUkqpRj2uT6Et9fX15OfnU1NTE+6iBF1s\nbCx5eXm4XK5wF0Up1Qv1iqCQn59PUlISgwYNQkTCXZygMcZQXFxMfn4+gwcPDndxlFK9UK9oPqqp\nqSEjI6NXBwQAESEjIyMiakRKqfDoFUEB6PUBwSdS7lMpFR69ovlIKaV6LXc9HFgPe/8J/SbBkJlB\nvVyvqSmEU0lJCY8++miXPzd37lxKSkqCUCKlVI/VUAt7V8DHf4BnL4ffD4SnL4D3fw27Pgr65bWm\nEAC+oPD973+/xfaGhgaiotr/T7xkyZJgF00p1V15PFCyBwq3QdFWKNxqXx/5Gty19pjs0TDxBhh0\nBgw8AxKzg14sDQoBcPfdd7Nz504mTpyIy+UiMTGRvn37sn79erZs2cLll1/O/v37qamp4c4772T+\n/PlAU8qOiooKLrroIs4880w+++wzcnNzefPNN4mLiwvznSmlAq6i0NYCvngB6iubtifnQfZIGDoT\n+p8OA2ZAQkbIi9frgsL9f9/MlgNlAT3n6H7J/OobY9rd//vf/55Nmzaxfv16PvroIy6++GI2bdrU\nOGx0wYIFpKenU11dzWmnncZVV11FRkbL/9nbt2/npZde4sknn+Saa67htdde46abbgrofSilmnHX\nQ8k+8DR4H277bDyAQOYwiE0J3PVqSuGzh2HFo9BQA+OvhQHTIHsUZI0I7LVOQq8LCt3B1KlTW8wj\neOihh1i0aBEA+/fvZ/v27ccFhcGDBzNx4kQAJk+ezJ49e0JWXqUiQkMtFKyzHbZ7PoX9n7f8pd6W\n9KHQbyL0neh9nmC/vI3xPjyA97XTBW2NDqyvgdVPwif/A9XHYMyVcN4vIGNoUG7zZPW6oNDRL/pQ\nSUhIaHz90Ucf8f7777NixQri4+OZOXNmm/MMYmJiGl87nU6qq6tDUlalerUjO2DrYtj5IeSvtr/Q\nAbLHwKQb7WieqBhwRIE4weG0r931cHgzHFwP+1bBptc6v5Y4ICYZ4lJt4IhNgdhUKFgLZQUw9Hw4\n/z4bXLqxXhcUwiEpKYny8vI295WWlpKWlkZ8fDzbtm1j5cqVIS6dUhHEGDi8Cbb+3T4Kt9jtOeNh\nyndsh+2A6RCf3vm5Rs5tel15xAaIQxuhvhoQGwREvK+xNZGaUvuoLrHPR7ZD+hC44nEYfFYQbjjw\nNCgEQEZGBmeccQZjx44lLi6OPn36NO6bM2cOjz32GOPHj2fEiBFMmzYtjCVVqhepr4Zje+Dobji2\nG4p32hrBsd32C3vADJjz/2DUJZCSd3LXSsiEYRfYRy8nxphwl6FLpkyZYlovsrN161ZGjRoVphKF\nXqTdr+pBDm6AmCRID3Burroq2/yz91PY+xkU74Dygy2PiUmGvNNg9KUw4mJI7HSRsYgiImuNMVM6\nO05rCkqpk1eaD+/9HLa8ad/3nwYTr4fRl9s29q6qLbdBYM+nNhDkrwFPva0B5IyHIefawJM2uOk5\nPr3tjl7VJRoUlFInrqEOVjxix90bA+feaztqv3wJ/n4nLPmZbZufcD0MPhuiYtv+4i7ZD/tXwb6V\n9vnwJjuyR5y2Y3ba92DQWTDg9G4zdLO30qCglDoxOz6Ad35mm3JGXgIX/hbSBtp9Z/4YDnxhg8PG\nhbB5kfdDAq54cMU1PdeWQ/kBu9uVAHmT4ayf2ADQ/3TbHKVCRoOCUr2JxwMf/c6OeBl/jR1iGWjV\nJbYWsOUNe50bF8LwWS2PEYHcU+1j9gOwY5lN49BQYzuIGx9V4IyGvCk2APQZC079Wgon/a+vVG+y\n6i/w8X/Z1ysegVn32/HxgWprL9kHL1xtR/qc+wuY8S/giu34M1HRMPJi+1DdnmZJVaq3OLwF3r8f\nTrkI5i2wzTLPXwXPXQ4Hvzz58xesgyfPh7KDcPPrcM5POw8IqsfRoBAAJ5o6G+CPf/wjVVVVAS6R\nijgNtfD6fNv+funDMPYq+OEaO07/4AZ4/Gx47XY7SuhEbHsb/u9iGwRuW2Y7jVWvpEEhADQoqLBb\n/ls4vNEGBN/4/KhomHYH3Lkezvw3m+7h2cttAOmKlX+Bl2+ErJFw2wc2eZvqtbRPIQCap86eNWsW\n2dnZ/O1vf6O2tpYrrriC+++/n8rKSq655hry8/Nxu9388pe/5PDhwxw4cIBzzz2XzMxMli9fHu5b\nUT3R3s/g0z/Bqd9smZrBJzYFLvgVDJwBL8yzmTrP/knn5/W47dyDVY/Z0UVXPgnR8YEvv+pWel9Q\neOdum58kkHLGwUW/b3d389TZS5cuZeHChXz++ecYY7j00kv5+OOPKSoqol+/frz99tuAzYmUkpLC\ngw8+yPLly8nMzAxsmVVkqCmDRd+1Q0Ev/F3Hxw6fBaMutXMKxs2DtEEdH//Bb2xAmPZ9mP2fwRnJ\npLodbT4KsKVLl7J06VImTZrEqaeeyrZt29i+fTvjxo1j2bJl3HXXXXzyySekpOgEHBUA795j+wmu\neAJiEjs/fs7v7ISwd+7u+LhtS+DTP8LkW+1nNCBEjN5XU+jgF30oGGO45557+O53v3vcvnXr1rFk\nyRLuueceZs+ezX333ReGEqp2FX1t890POD3cJfHPlsWw/vmmiV7+SMmDmXfDsl/aL/62mpuO7YE3\n7rBrB8wJ778nFXpaUwiA5qmzL7zwQhYsWEBFRQUABQUFFBYWcuDAAeLj47npppv4yU9+wrp16477\nrAojjwdeuRGeucQO7ezujuywE8j6ToBz7uraZ6d9D7JGwTt32URzzTXUwqu3gAGufkaHnEagoAUF\nEekvIstFZIuIbBaRO9s4RkTkIRHZISIbROTUYJUnmJqnzl62bBk33HAD06dPZ9y4ccybN4/y8nI2\nbtzI1KlTmThxIg888AC/+MUvAJg/fz5z5szh3HPPDfNdRLhdy+2C6cZjh3Z2dYROKO1dAU9fYCek\nXfmUHWXUFU4XXPIglO6DT/675b73fm7TU1z+aOAznaqewRgTlAfQFzjV+zoJ+BoY3eqYucA72CUq\npgGrOjvv5MmTTWtbtmw5bltrHo/H1De4jcfj6fTY7s6f+1Vd9PzVxvzXMGM2v2nMr5KNWXpfuEvU\ntk2vG/ObLGP+NMmY4p0nd67X7zDm/gxjCr+y7ze8au/93Z+ffDlVtwOsMX58dwetpmCMOWiMWed9\nXQ5sBXJbHXYZ8Ky3zCuBVBHpG4zylFbXs+VgGbUNnmCcXvVkxTth+1KY8m2bi3/yLXaI597Pwl2y\nJsbApw/Zpp1+k+C2923eoZMx6zd2iOmSf7f9KYt/ZFNeX/DrABRY9VQh6VMQkUHAJGBVq125wP5m\n7/M5PnAEhNNhc7+4PT1rUSEVAqufsumep9xq389+wA7XXPRdO+Qz3NwNsOQntnN49OXwzTf9W06y\nM4lZcP6vYPfHsOBC238wb4FtXlIRK+hBQUQSgdeAfzXGnNC/MBGZLyJrRGRNUVFRm8eYTlaQi/IG\nhYYeHhQ6u0/VRbXl8MXzMOZySMqx22IS4con7FDPd+8Jb/mO7YFXbrKBa8a/wLy/Brbzd/It0O9U\nO+rqqqcgJSi/yVQPEtQhqSLiwgaEF4wxr7dxSAHQv9n7PO+2FowxTwBPgF2Os/X+2NhYiouLycjI\nQNrJBul02Pjn9vTc5iNjDMXFxcTG6oiQgPnyZagtg9PvaLm9/1Q469/tRK8Rc2DUN0JTnvJDsPsT\n2P0P+yjZZ1cbm/vfMPX2wF/P4YTrX4ajO+2MZxXxghYUxH47Pw1sNcY82M5hi4EfisjLwOlAqTHm\nYDvHtisvL4/8/Hzaq0WA/UI9XFJDTVEUSbE9t3ocGxtLXt5JLkKuLI8HVj0OuZNtPv/WzrkLti+z\nQz/zpkJSn+CV5YsXbD/Gka/s+9gUu9LY9H+BYedDxtDgXTupT3DvTfUowawpnAHcDGwUkfXebT8H\nBgAYYx4DlmBHIO0AqoBbT+RCLpeLwYM7Hz437753ufH0Adx7sS56r4BdH0LxdpvTpy1Ol21Gevxs\neON7cO1zEJ0Q+HJ89ggsvdcGp1m/sRlIc8brLGIVFkELCsaYf2KHmnZ0jAF+EKwytJYWH01xZV2o\nLqe6u1VPQEK27bxtT9YIm+bhrR/Dn6fZ8f2tVxk7UcbY5qnlD9gyXPlk1+ccKBVgETWjOT0hmmMa\nFLqvmlIoWAsV7TcDBkzxTtj+nh2G2tkX8ZRvw63v2PWEX5gHC78NFYUnd31j4P1f24Aw/jq46mkN\nCKpb6H25jzqQlhDN0ar6cBcj9ArW2uaI7jTUsHCb7Ug98rX3sR3Kvd1Jsalw02ttt/MHyudPgsPV\nNAy1MwNnwB2fwD//aGcB73gfZv0HTLoZHF38beXxwLt3w+eP24Az93+6fg6lgiSi/hLT412RV1PY\n+xk8eR48dwVUFoe7NFb5YXhiJrzzM9jwqs2/M/Q8O2lq3gKIS4NnLoVd/wjO9dsahuqPqBiYeRd8\n7zO7wPzff2RXIytY5/85PG77uc8fh+k/hIsf1ICgupWIqimkJ8REXlDY/QkgsP9zeHKmHX7YZ0x4\ny7TiYXDXwh3/tF+urYcRDzzDBrEXroar/9rxgu/uetsUlD3S/+uvfwnqyo8fhuqvzOHwrbdshtJl\nv4Inz7WL0Jx7L/QZ3fZnPG7YuRxW/hl2fghn/wzO/fnx965UmEXUT5T0BBfltQ3UNrjDXZTQ2b/S\nfvHe+o79An1qFmx9K3zlqToKqxfA2Hl28aK2vhSTcuCWtyFnLLxyM3z5yvHH1JTaIZx/mgCPng7r\nX/Tv+rXlsOKR9oeh+svhsCud3fklzLzH1mr+MgNeu80GKZ/inXaxmj+OgxeuggPrbTrq8+7VgKC6\npYiqKaQl2I68kqp6+iRHwHA/d4OtIUy4DvImw+3LbXroV260v2rP/mnov5hW/gXqK+3EsI7Ep9t0\nDi9dD4vm2wlmU2+3k7lWPgbrnoG6Cjt8M7kfvP3vkDsFsk5p/5zG2Pw+pfttFtBAiE226xNMnW+D\n1KrHYdPrMO5qe529n9rJZ8MusKOYTpljm6GU6qYiKiikx9ugcLSyjj7JETAruHCz/eIcMN2+T+4L\ntyyxk7GWPwCHN8Plfwndurs1pfZLc9Sl/jX3xCTBjQttErglP4HNb8C+FTaQjbkSZvzQridQdhAe\nOwMW3moTxbni2j7f6qdg8+tw/n0w6MyA3hrx6TDrfrt05Sf/A2v/CqkDbG6hCdfb//ZK9QCRFRS8\nNYWI6VfY580/OGBa0zZXLFzxmO1XWHafTfw26/7QlOfzJ6G21L9F431csXbS2OJ/gW1vw/Tv276A\nlGazupP7whWP2+Gi791r5xK0VrDW5jEaPhvO+PHJ30t7kvrA3P+yaxo7XdpEpHqciAwKETOBbd8K\nSM5r+QUK9ovqjB/ZkUkbXrG/nIM9e7auElb82X4p953Qtc86XTaQeTztj9QZPgtm/Ag+ewgGnwVj\nrmjaV3UU/naL7au44vHQjPbROQeqh4qojmZfn8KxqggICsbAvpUtawmtTbjWzg3YHaShn82t+StU\nH7X9GCeqsy/z8++z/QqLf2Szi4INJIvusPd59TOBSTmtVC8WUUEhNc5O3joaCTWF0v1QfqDjoHDK\nRRCT0vbonkCqr/H+gj/bZh8NFqfLznNA7Kzjhjr49I925vKFv7Wd7UqpDkVUUIhyOkiJi5AJbPtW\n2ueOgoIrFsZcBlv/bpt3gmX981Bx+ORqCf5KGwiXPWz7EP52M3z4H7YpKRhpp5XqhSIqKABkJERI\nUrx9KyAmGbLbmUzlM/46O0Q0WHMX3PU2NUTeVJsKOhRGXwan3QZfv2uXrLz0Ye3wVcpPEdXRDLZf\nISL6FPattE01nXUgD5gOKQNgw8u2jyHQNrxim7IufjC0X8yzH7A5lCZcZ4e2KqX8EnE1hbT4aI5W\n9vKkeNXHoHCrXYS9Mw4HjL8Gdn1kV/3qyNFdsG2J/+VoqINPHrTJ+AKVbtpfrlg4/5c2JYVSym8R\nFxTSE8LUp1B5JHTX2r8aMB33JzQ34TowHtj4avvH1FfD8/Pg5evt7GF3J4G1shieu9wu86g5fpTq\nMSIuKNj02XXY9X1CJH8t/GEYHN7i/2fe+jebzuFE7FsBjiib38cfmcPt4u0djUL68D/tF/zoy+3M\n4BfmQXVJ28cWboOnzoP8NXadgBEXdf0elFJhEXFBISMhmroGD5V1IUyKV7wDMFC0zf/PbFpoR+2c\niP2r7ASxrqSvmHAdHN5oU1+0tm+VnXg25dtwzTNw2aOw51N46oKWyd8Atr8PT8+y6bBvXQLj5p3Y\nPSilwiLigkJafBhSXVR6VxIrO+Df8bUVNk/Q4S1dHyraUGuHY/ryHflr7FW2dvHlyy2311fDm9+H\nlP52/WCASTfCtxZDVTE8db5Nz22Mrdm8eDWkDoT5y4O7SI5SKigiLij4Ul2EdAJbY1Ao8O94X/Aw\nbptquSsOfgkNNdD/9K59LiHTZvLc+KrN/e/z4X/ams5lj7QcxTNwBtz+oV3j+LnL7doH794FI+bC\nt989PrWGUqpHiLig4Et1cTSUw1KrvJ3MfgeF/KbXBWu6di1/Jq21Z7wv7cXH3nM1azYacs7xx6cP\nhtuWweBzYMcyOPPHcM1zEJPY9WsrpbqFiJunkOELChWhrCl4g0Kpn0HBd5wr3nbWdsW+lZA+FBKz\nu/Y5sB3CMcl2bsGAacc3G7UlNgVufNXmGsoY2vVrKqW6lYgLCmFJiucLCv72KfiOG3Z+14KCMXal\ntVPmdK18Pq44Oxt40+v2dfEOu9BNZ5O/HE4NCEr1EhHXfJQUE0WUQ8LTp1BxyK6G1pmyfNtWP/AM\nm9TO32BSvMN2/p5I05HPBG/aizULvM1GM0/8XEqpHifigoKIhD7VRVUxRCfaCWIVncwaBhsEUnJt\nGmjwv7awb4V97urIo+YGzLArhqUM6LjZSCnVK0VcUAC7LGdxqPoU6qvtkpg54+x7f/oVSgsgORf6\njgdnNOSv9u9a+1ZCfAZkDDvx8joccPMbcOvbmjNIqQgUmUEhlDUFX3+Cb7Uxf0YglXmDQlSMDSYF\na/271r6VNt/RyaaUyBhqawtKqYgTsUEhZH0Kvv4Ef4NCTRnUlkFyP/s+7zQ48EXnfREVhTYNxYAu\nzk9QSqlmIjIopCW4OFYVokypVcX2OWMYuBI67zT27fdN/sqdAvVVUNhJ3qTG+Qkn0Z+glIp4ERkU\n0uOjKamqw+0JQVI8X00hIdP++i/N7/h438S15Fz77FtCsrNJbFsX2zkDfSeeeFmVUhEvIoNCWkI0\nHgOl1SGoLfj6FBKy7Igif2sKvuajtMG28zi/g36F6hK7pOa4qyEq+uTLrJSKWBEZFEKa/6iyCJwx\ndkhqcm7nfQqlBYA0BQUR24TU0QikzYtsvqOJNwSs2EqpyBTRQSEkI5Aqj9hagogNCuWHOl6gpiwf\nEvuA09W0Le80OPK1zZzalvUvQtZIuyaCUkqdhIgMCr702SGpKVQdgYQM+zq5H2A6Xvay7EBTLcEn\nb7L9XMG6448v+hryP4eJN+rqZkqpkxa0oCAiC0SkUEQ2tbN/poiUish67+O+YJWltZA3HyVk2de+\nEUUd9SuUFti+h+Z8NYC2ZjZ/+SKI02Y4VUqpkxTMmsL/AZ1lZvvEGDPR+whZToXQBoViiM+0r301\ngLJ2RiAZ0zRxrbm4VMgccfwIJI/bLoozfBYk9QlsuZVSESloQcEY8zFwNFjnPxmxLifx0c7QrL5W\nWWSHo0LTl317NYXaMpsSo3VQALuKWf5qGzh8di636x9oB7NSKkDC3acwXUS+FJF3RGRMKC+cFh8d\n/IV26iqhobopKMSm2Als7eU/8m1v3XwENihUFdt1C3zWvwBxaSeeKlsppVoJZ1BYBww0xkwAHgbe\naO9AEZkvImtEZE1RUVFALh6SVBeNE9e8fQoi3rkK7QQF3/a2agq+jKm+PEjVx2Db2zDuGpsjSSml\nAiBsQcEYU2aMqfC+XgK4RCSznWOfMMZMMcZMycrKCsj10xKig998VOlNcZHQrMzJ/dpvPuooKGSP\n9q7E5p2vsOk1cNdq05FSKqDCFhREJEfEjqEUkaneshSH6voZCSFoPvLVFOKbxbrkvPZrCqUFIA5I\nyjl+nzMK+k1qGoH0xQvQZ2xToj2llAqAoC3HKSIvATOBTBHJB34FuACMMY8B84DviUgDUA1cZ4wJ\nQTIiKy0+mmOVQU5zUeVLcdE8KPRrmsDWfIIa2BpE64lrzeVOhlWPwcEv4cA6uPC3OjdBKRVQQQsK\nxpjrO9n/CPBIsK7fmfQEFxW1DdQ2uImJcgbnIs2T4fmk5NI4gS21f8vjy/LbbjryyTsNPnsI3r0H\nHFG2P0EppQIo3KOPwibNl+oimLWFyiO2HyA6oWlbR8NSSwuOn83cXJ63s3nvpzD8QkgMTP+KUkr5\nRGxQSA9FqovKIy37E6BZUGg1gc0Y79rMee2fL7kfJHmDhnYwK6WCIHKDQiiS4jWfuObTOKu5VU2h\npgTqKztuPgIYOB0SsmH47MCVUymlvILWp9DdhSTVRdUR23HcXGyKTaPdegKb731HzUcAF/0B6sp1\n3QSlVFBEbE0hLRRBwZc2uznxrpXQelhq62U425OQAWmDAlZEpZRqLmKDQmqcHfYZtKBgjDcotDEf\nr63Fdlovw6mUUmEQsUEhyukgNd4VvD6F2nI747h1RzN4g0KrPgXfxLXWzU1KKRVCERsUwI5AClpN\noarZ2sytpbSxAlvZAUjqa2cuK6VUmER0UEhLiA5eTaGyjdnMPm2twFaW33kns1JKBVlkB4X4aIor\nwhEUfCuwNetXKG1jcR2llAqxiA4K6QlB7FNoKxmeT+NcBW9Q8GfimlJKhUCEB4UYjlXWE5Q8fG3l\nPfLxLaLjm5tQfcwuxqPNR0qpMIvwoOCizu2hss4d+JNXFdtJaq644/fFJNt9vhFIHa2joJRSIRTR\nQSHNl/8oGP0KbaW48BHxDkv1zk1oXIZTm4+UUuEV0UGhMdVFMPoV2prN3FzzFdgaJ65p85FSKrw0\nKEBwluVsK0Nqcym5TTWEsgMgTp24ppQKOw0KBCnVRVU7KS58knOh4rCdwFZaYCeuOYK02I9SSvkp\nooNC0JLidZT3yCe52QpsZQVNI5KUUiqM/AoKInKniCSL9bSIrBORHp/QPykmiiiHBL5PoaYUPPWd\n9Cn4FtspsA8deaSU6gb8rSl82xhTBswGsoBbgd8HrVQhIiI21UWgawq+2cyd9SkAlObbPgXtZFZK\ndQP+BgXxPs8F/mqM+bLZth4tIyEISfGqOkhx4eMLAoc2QkONDkdVSnUL/gaFtSKyFBsU3hORJMAT\nvGKFTlp8EJLidTSb2Sc2BaKTIH+1fa/NR0qpbsDfPM3fASYCu4wxVSKSjm1C6vHSE6LZeqgssCet\n7CBtdnPJ/aBgnfe1BgWlVPj5W1OYDnxljCkRkZuAXwClwStW6KQluILYp5DR8XEpuTbnke+1UkqF\nmb9B4S9AlYhMAH4G7AWeDVqpQig9PpqS6nrcngAmxassgpgUiIrp+Dhfv4IjChKyA3d9pZQ6Qf4G\nhQZjU4leBvzJGPMnICl4xQqd9IRojIHS6vrOD/ZXZxPXfHzrKiT1A0dETxlRSnUT/n4TlYvIPcDN\nwNsi4gBcwStW6DRNYKsN3Ek7SobXnK+moE1HSqluwt+gcC1Qi52vcAjIA/4QtFKFUFOqiwDWFCqL\nO+9khqZgoHMUlFLdhF9BwRsIXgBSROQSoMYY0yv6FBrTZweys7myqPNOZmgacaQjj5RS3YS/aS6u\nAT4HrgauAVaJyLxgFixUspNsZ/Dhshr/P1RXCTXtDGP1eOwCO/7UFFIHQGwq9Jvo/7WVUiqI/J2n\ncC9wmjGmEEBEsoD3gYXBKlioZCXFkBgTxc6iCv8/9NptULofvvuJXTCnuZoSMG7/+hSiE+CnO+zo\nI6WU6gb87VNw+AKCV3EXPtutiQhDsxPZUehnUKg6CtuX2vQUB744fr+/E9d8nK7jA4tSSoWJv1/s\n74rIeyJyi4jcArwNLAlesUJrWFYXgsJX74CnARD48qXj9/tSXPjTp6CUUt2Mvx3NPwWeAMZ7H08Y\nY+4KZsFCaWh2AoXltZTV+DECaetiSOkPoy+DjQuhoVUHdVUXawpKKdWN+N0EZIx5zRjzb97Hos6O\nF5EFIlIoIpva2S8i8pCI7BCRDSJyalcKHhD11bDnn5xX8hopVLCzs9pCTSns/NAGhIk3QvVR2P5e\ny2Mak+FpUFBK9Twd9nCKSDnQVv4HAYwxJrmDj/8f8Ajtp8O4CBjufZyOTaVxeiflPTk1pbBvFez7\nDPaugAPrwF3HSOCHUXPZUTiDSQPS2v/81++Buw5GXQq5k+2ayl++DKO+0XRMY96j9KDeilJKBUOH\nQcEYc8KpLIwxH4vIoA4OuQx41ps+Y6WIpIpIX2PMwRO9Zoc2LrSjhjB2tE+/SXD6HTDwDDxfPM+1\nWz/giUOFQP/2z7HlTbuWct5pNi3FuKth1ePeyWrePoTKIxCXZjuQlVKqhwnnWMhcYH+z9/nebcEJ\nCv0mwcy7YeAMyJ0C0fGNuxzxGSRv+zt9dy8CJrf9+doK2PE+nPrNpjxFE66HFY/ApoVw+nfttsqi\njldcU0osFYJQAAAYuElEQVSpbqxHDCsVkfkiskZE1hQVFZ3YSTKG2qAw+OwWAQGA/qexO2YkZx19\nzU4+a8v2pXaFtNGXNW3LGQs542D9i03b/J24ppRS3VA4g0IBLdtq8rzbjmOMecIYM8UYMyUrKzhf\nuFsG3MAAc4D6r5e2fcDWxfbLfsD0ltsn3AAH10PhVvu+sqipKUkppXqYcAaFxcA3vaOQpgGlQetP\n8IN71KUcNqnUffro8Tvrq+HrpTDyEnA4W+4bd7Xto/DVFiqPaE1BKdVjBS0oiMhLwApghIjki8h3\nROQOEbnDe8gSYBewA3gS+H6wyuKPIX3Sea5hFgn7/wFFX7XcueMDqK9s2XTkk5gFw2bBhr/ZOQtV\nxdqnoJTqsYLW0WyMub6T/Qb4QbCu31VDshJ40X0+/xr9JlGrHoNL/rdp55Y3IS4dBp3Z9ocnXg9f\nvwObFwFGawpKqR6rR3Q0h0J8dBRxqX1YnXyBnXtQfczuaKiFr9+FkXPbH2Z6yhyb7fSzh+17f5Lh\nKaVUN6RBoZlh2Yk8by6C+ipY551zt+sjqC2D0Ze3/8GoGBh7FRzeaN9rUFBK9VAaFJoZlp3IB8ey\nMAPPhM+fBHeDbTqKSYHB53T84Yk3NL3W5iOlVA+lQaGZoVmJ1NR7KB77bbtewpY3YNvbMOIiiIru\n+MO5kyFjuH2tHc1KqR5Kg0Izw7ITAdiYOANSB8KSn9pFc9oaddSaiJ3VnNRP8x4ppXosDQrN+ILC\nziPVMHW+zYIanQhDz/PvBKfdBj/efPxcBqWU6iE0KDSTnhBNekK0XZrz1JshOglGzAVXrH8nEGnK\ni6SUUj2QLg7cytCsBLsKW2wKzP9Im4KUUhFFf9a2Mqz5es2ZwzQoKKUiigaFVoZmJXKsqp6jlXWd\nH6yUUr2MBoVWfJ3NOzpbmlMppXohDQqtDM3SoKCUilwaFFrJTY0jzuXUoKCUikgaFFpxOIQhWQl2\nWKpSSkUYDQptaDECSSmlIogGhTYMzUqkoKSaqrqGcBdFKaVCSoNCG3wjkHYVVYa5JEopFVoaFNrQ\nmANJ+xWUUhFGg0IbBmbE43SI9isopSKOBoU2xEQ5GZAer0FBKRVxNCi0Y2hWYtiaj4wx3PjUSv7r\n3W1hub5SKnJpUGjHsOxEdh+ppMHtCfm11+w9xqc7ivnbmv24PSbk11dKRS4NCu0Ylp1Ivduw72hV\nyK/9zGd7ADhSUcf6/cdCfn2lVOTSoNCOcCXGKyyr4d1Nh7h2Sn9cTmHp5sMhvb5SKrJpUGjHkKwE\nAHaGeK7Ci5/vo8Fj+N7MoUwbksHSLYcxRpuQlFKhoUGhHcmxLvLS4liz52jIrlnv9vDiqn3MHJHF\noMwEZo/JYfeRSp0voZQKGQ0KHbh4XF/+8XURxRW1Ibnee5sPUVheyzenDwRg1qg+ACzdok1ISqnQ\n0KDQgStOzaXBY3hrw8GQXO/ZFXvpnx7HOadkA5CTEsuEvBTtV1BKhYwGhQ6MzElmZE4Si74oCPq1\nth0q4/PdR7l52kCcDmncPmt0H9bvL6GwrCboZVBKKQ0Knbjy1FzW7y9hV5Db9Z9dsZeYKAfXTOnf\nYvvsMTkALNuqtQWlVPBpUOjEZRNzEYE3glhbKK2uZ9G6Ai6b2I/U+OgW+4ZnJzIwI55l2q+glAoB\nDQqd6JMcyxlDM1m0viBoQ0NfW5tPdb2bb04fdNw+EWH26D58tqOY8pr6oFxfKaV8NCj44YpJuew/\nWs3avYGfXezxGJ5buZdTB6QyNjelzWNmjc6hzu3hH18XBfz6SinVnAYFP1w4NodYlyMoHc7/3HGE\n3Ucq26wl+EwemEZ6QrQ2ISmlgi4q3AXoCRJjorhwTA5vbTjIfd8YTUyUs0ufN8awIb+Uqjo3IiCA\nwyEI8OQnu8hMjOaicTntft7pEM4fmc27mw9R7/bgcmosV0oFR1CDgojMAf4EOIGnjDG/b7X/FuAP\ngO8n+CPGmKeCWaYTdcWkXN5cf4Dl24qYM7b9L/DWjDH88s1NPL9yX7vH/Oi8YZ0Gmtljcnh1bT6r\ndh3lzOGZfl9fKaW6ImhBQUScwJ+BWUA+sFpEFhtjtrQ69BVjzA+DVY5AOXNYJpmJMbzxRYHfQcEY\nw31vbub5lfv49hmDuWB0NhjwGDAYjAGHCKcNTvPr+rEuB0u3HNKgoJQKmmDWFKYCO4wxuwBE5GXg\nMqB1UOgRopwOLp3Qj+dX7qW0qp6UeFeHxxtj+NXizTy3ci/fPXsId180EhHp8DMdiYt2cvbwLJZt\nOcz9l445qXMppVR7gtk4nQvsb/Y+37uttatEZIOILBSR/m3sR0Tmi8gaEVlTVBS+EThXTMqlzu3h\nrY0HOjzOGMOvF2/m2RV7uf2swScdEHxmje7DwdIaNhWUnfS5lFKqLeHusfw7MMgYMx5YBjzT1kHG\nmCeMMVOMMVOysrJCWsDmxuYmMyw7scOJbL6A8MyKvdx25mB+PndUwH7Vnz+qDw6BZVsOBeR8SinV\nWjCDQgHQ/Jd/Hk0dygAYY4qNMb4UpE8Bk4NYnpMmIlwxKZfVe46xv40V2Ywx3P/3LTyzYi/fOXMw\n914cuIAAkJ4QzZRB6by14aAu06mUCopg9imsBoaLyGBsMLgOuKH5ASLS1xjjS0F6KbA1iOUJiMsm\n9uMP733Fnz7Yzui+yRSUVJN/rIr8Y9XkH6umtLqeb58xmF8EOCD4fGv6IH7w4jpeW5d/XJ4kpZQ6\nWUELCsaYBhH5IfAedkjqAmPMZhH5DbDGGLMY+JGIXAo0AEeBW4JVnkDJS4tnxtAMFq7NByDO5SQv\nLY68tDgmDUhlfF4qV0/OC1pH8NxxOUzon8qDS7/mG+P7ERfdtTkTSinVEelpSz1OmTLFrFmzJqxl\nKKmqY9/RKnJT40hPiA75SKBVu4q59omV/PTCEfzg3GGdHl9cUUtVnRuHQ3AIOEUQEVxOOS4Bn1Kq\ndxKRtcaYKZ0dpzOaT0BqfHRYv0xPH5LBBaP68NhHO7l+6gDSE9ovy5vrC/jxK+tprwvirjkj+d7M\noUEqqVKqp9Gg0EPdfdEIZv/vxzz84XZ+9Y0xbR6zfn8JP124gckD07hmSn88xuAx2GePYfGXB3j8\n453cPH0giTH6p6CU0qDQYw3LTuLa0wbw/Mq93DJjEAMzElrsP1haze3PrqFPcgyP3zylzdrEuLxU\nLv/zp7ywci/fPUdrC0qp8M9TUCfhxxcMJ8rh4A/vfdVie3Wdm9ufXUNVbQNPf+u0dpuXJvZP5azh\nmTz5yW5q6t2hKLJSqpvToNCDZSfHcvvZQ3hrw0HW7y8B7PoMP3n1SzYfKOOh6ydxSp+kDs/xg3OH\ncaSilldW7+/wOKVUZNCg0MPNP3sImYnR/G7JVowxPPThdt7eeJB7LhrJ+aP6dPr50wenM2VgGo//\nYyd1DZ4QlFgp1Z1pUOjhEmOiuPOCU1i1+yj3vrGJP76/natOzeP2s4b49XkR4YfnDeNAaU1Q16FW\nSvUMGhR6getO68+QzAReXLWPKQPT+O2VY7s0d+KcU7IYl5vCox/toMGttQWlIpkGhV7A5XTwn1eM\n5dwRWTx28+QurwwnIvzg3GHsKa7i7Y0HO/+AUqrX0iGpvcSMoZnMGHrii+/MHt2HU/ok8ujynXxj\nfD8cDl2vQalIpDUFBdg1o78/cxhfHS5n2dbD4S6OUipMNCioRpeM78vAjHj+vHwHPS0nllIqMDQo\nqEZRTgffO2coG/JL+WT7kXAXRykVBhoUVAtXnppHv5RY/uOtLZTX1Ie7OEqpENOgoFqIjnLw39dM\nYPeRSv7lpS90iKpSEUaDgjrOjKGZ/OaysXz0VREPLOn2i+EppQJIh6SqNt1w+gB2FFaw4NPdDMtO\n5MbTB4a7SEqpENCagmrXvReP4twRWdz35mY+3aEdz0pFAg0Kql1Oh/DQ9ZMYmpXA955fy86iinAX\nSSkVZBoUVIeSYl08/a3TiHI6uO2ZNZRU1YW7SEqpINI+BdWp/unxPHHzZG54chXfeOSfDM5MJCkm\nisSYKBJj7XNuWhxXTsolyqm/M5TqyTQoKL9MGZTOIzdM4tkVeymtrqfgWBUVtQ1U1DRQWWdXbVu2\n5TAPXz+JWFfXEvIppboPDQrKb7PH5DB7TM5x290ew3Mr9nD/W1u48alVPP2tKaTGt70EqFKqe9O6\nvjppTodwyxmD+fMNp7Ixv5R5j62goKQ63MVSSp0ADQoqYOaO68uz35nK4bIarnz0U7YdKgt3kZRS\nXaRBQQXUtCEZvHrHdACu/ssKVuwsDnOJlFJdIT0tRfKUKVPMmjVrwl0M1YmCkmq+teBz9hZXkpMS\nS5TDgdMhRDmk8bl/ejzjclMYl5fC2NwUkmNd4S62Ur2WiKw1xkzp7DjtaFZBkZsax8I7pvPwhzs4\nVllHg8fg9hgaPB4a3IY6t4cv9pXw1oam5T8HZyYwNjeFM4ZmcMmEfiTG6J+nUqGmNQUVVkcr69hY\nUMqmglI25JewIb+Ug6U1xEc7uXRCP66bOoAJeSmI6PKgSp0Mf2sKGhRUt2KM4Yv9Jbz8+T7+/uVB\nquvdjMxJ4vqpA5g7ri/pCdE4df1opbpMg4Lq8cpr6nlz/QFeXr2PTQVNI5nio50kxETZWdXeGdUp\nca7GR7L3OTXeRVZiDNnJsfRJjiE+WpujVOTSoKB6lY35pazaXdw4i7qitulRXtNAaXV946Ouoe2F\ngRJjoshOjiE7KYaMxBhSvYEjNS6alHgXqXEuYl1OPMbYh4fG17EuJxP7p+qkPNVjaUez6lXG5dlR\nSv6oqXdTWl1PSVU9ReW1HC6robDxuYbCslq2HiyjtKqekup63B7/fxgNzUpgysB0Jg9MY/KgNIZk\nJoStv6Om3q0pRVTAaVBQvU6sy0msy0mf5FhG5CR1eKwxhoraBkqqbC2jpt6NwyE4RHCKIGJnbJdU\n1bNu3zHW7j3Gu5sP8cqa/QCkxrsYmZPEKX2aPxJb1Cg8HkNVvZtKb60m1uUgMzGmS1/oheU1bC4o\nY2NBaWPH/MHSGkb3TWbuuBwuGteXoVmJJ/YfzKum3s3B0hoOlFR7HzWU19RzSk4S4/NSGJaVqAkP\nI0BQm49EZA7wJ8AJPGWM+X2r/THAs8BkoBi41hizp6NzavORCjePx7DrSAVr9x7ji30lfHW4nO2H\nK6iobWg8JispBodAZa2byroG2vpnlhwbRVZSDNlJsWQlxRAf7aS63k1VnZvqOnfj6+KKWgrLawEQ\nsUN3x+WmMCA9ns92FrN27zEARvRJ4qJxOcwZm0OUQ9hVVMme4kp2H6lkV5F9rqhtwNlsrohD7HNN\ng4ejlcenRY+JclDrbY6LczkZ0y+ZcXkpjM9LYXTfFIZkJeDqJFAYYzhUVgNATnKsjiQLk7D3KYiI\nE/gamAXkA6uB640xW5od831gvDHmDhG5DrjCGHNtR+fVoKC6I2MMB0pr+PpwOV8fKm9ckCgxxkVi\njJPE2CgSvOnGa+s9FJbXUFReS1FFLYVl9rmqzk18tJM4l5O4aKf3te1EH90vmbH9khmTm3Lc/I1D\npTW8u+kgSzYdYvWeo8cFoIyEaAZnJjA4M4GUOBduY+eMNH+4ohz0S4mlX2ocfVPiyE2No09KDC6H\ng93FlY3DhTfkl7L5QCk19TZQuJzC0KxERvVNtjWmnCRq6tzsKKxgZ1EFO4sq2VVU0ZhJNzk2ipE5\nyYzISWJEThIjc5Lonx5PvdtDvdtQ1+Ch3u2htsGD22NweGtqjZMfnTagRTsdxLqcxLgcxEQ5iHY6\nTjjYeDyG6nobvKu8Qbym3o2I4HI4iHIKLqctQ5RTiHXZ/zexUU4cfo6Eq6l3t2jGPFxWS2l1feOg\nicQYJwnRLdPRJ8W6SIqNClgTYXcICtOBXxtjLvS+vwfAGPO7Zse85z1mhYhEAYeALNNBoTQoKNW+\nwvIaPtpWRHSUg8GZCQzyBoJAanB72FFUwbaD5Ww7VM62Q2VsO1jeWBvwyU2NY0hWAkOzEhmanQjG\neI+3gbO8Wc3qZInYWk2Uo6nW4s/XdYM3IJyoWJeD+Ogo4lxOnA7BYDAG78PgMVBV10BZzYnfa7TT\nQVJsFEmxUdw0bSC3nTXkhM7THTqac4H9zd7nA6e3d4wxpkFESoEMoMWCwCIyH5gPMGDAgGCVV6ke\nLzsplmtO6x/Ua0Q5HYzMSWZkTnKL7SVVdXx9uIL4aCdDshI6HALsq1l9daiMAyU1RDsdREc5cDU+\nCy6nw9ZkjMHtNi1mxdc2eB/17sbnGm/twp6/1fUwSBthwumA+OgoEmKcLZ59o9Aa3IYGt4d6j/fZ\nW4tp2cTXQFWdG4/H4BABAUFwiA1Wvv6t7KQY+iTHeh8xJMe6qGlwU1HbYJsZfSPqahoor62nvMb2\nQZXVNL3OTIw5+f+BnegRHc3GmCeAJ8DWFMJcHKVUG1Ljo5k6ON2vY0WE3FTbTBXJ4qOjbPDseDxE\nSAVzKEEB0PwnS553W5vHeJuPUrAdzkoppcIgmEFhNTBcRAaLSDRwHbC41TGLgW95X88DPuyoP0Ep\npVRwBa35yNtH8EPgPeyQ1AXGmM0i8htgjTFmMfA08JyI7ACOYgOHUkqpMAlqn4IxZgmwpNW2+5q9\nrgGuDmYZlFJK+U+nJyqllGqkQUEppVQjDQpKKaUaaVBQSinVqMetpyAiRcDeE/x4Jq1mS0eQSL13\nve/IovfdvoHGmKzOTtTjgsLJEJE1/uT+6I0i9d71viOL3vfJ0+YjpZRSjTQoKKWUahRpQeGJcBcg\njCL13vW+I4ve90mKqD4FpZRSHYu0moJSSqkORExQEJE5IvKViOwQkbvDXZ5gEZEFIlIoIpuabUsX\nkWUist37nBbOMgaDiPQXkeUiskVENovInd7tvfreRSRWRD4XkS+9932/d/tgEVnl/Xt/xZupuNcR\nEaeIfCEib3nf9/r7FpE9IrJRRNaLyBrvtoD9nUdEUPCuF/1n4CJgNHC9iIwOb6mC5v+AOa223Q18\nYIwZDnzgfd/bNAD/bowZDUwDfuD9f9zb770WOM8YMwGYCMwRkWnA/wP+1xgzDDgGfCeMZQymO4Gt\nzd5Hyn2fa4yZ2GwYasD+ziMiKABTgR3GmF3GmDrgZeCyMJcpKIwxH2PTkDd3GfCM9/UzwOUhLVQI\nGGMOGmPWeV+XY78ocunl926sCu9bl/dhgPOAhd7tve6+AUQkD7gYeMr7XoiA+25HwP7OIyUotLVe\ndG6YyhIOfYwxB72vDwF9wlmYYBORQcAkYBURcO/eJpT1QCGwDNgJlBhjfKvF99a/9z8CPwM83vcZ\nRMZ9G2CpiKz1rl8PAfw77xFrNKvAMcYYEem1Q85EJBF4DfhXY0yZ/fFo9dZ7N8a4gYkikgosAkaG\nuUhBJyKXAIXGmLUiMjPc5QmxM40xBSKSDSwTkW3Nd57s33mk1BT8WS+6NzssIn0BvM+FYS5PUIiI\nCxsQXjDGvO7dHBH3DmCMKQGWA9OBVO+659A7/97PAC4VkT3Y5uDzgD/R++8bY0yB97kQ+yNgKgH8\nO4+UoODPetG9WfO1sL8FvBnGsgSFtz35aWCrMebBZrt69b2LSJa3hoCIxAGzsP0py7HrnkMvvG9j\nzD3GmDxjzCDsv+cPjTE30svvW0QSRCTJ9xqYDWwigH/nETN5TUTmYtsgfetFPxDmIgWFiLwEzMRm\nTTwM/Ap4A/gbMACbYfYaY0zrzugeTUTOBD4BNtLUxvxzbL9Cr713ERmP7Vh0Yn/k/c0Y8xsRGYL9\nBZ0OfAHcZIypDV9Jg8fbfPQTY8wlvf2+vfe3yPs2CnjRGPOAiGQQoL/ziAkKSimlOhcpzUdKKaX8\noEFBKaVUIw0KSimlGmlQUEop1UiDglJKqUYaFJQKIRGZ6cvoqVR3pEFBKaVUIw0KSrVBRG7yrlOw\nXkQe9yadqxCR/xGRdSLygYhkeY+dKCIrRWSDiCzy5bIXkWEi8r53rYN1IjLUe/pEEVkoIttE5AVp\nnqBJqTDToKBUKyIyCrgWOMMYMxFwAzcCCcA6Y8ypwD+ws8UBngXuMsaMx86o9m1/Afizd62DGYAv\ni+Uk4F+xa3sMwebxUapb0CypSh3vfGAysNr7Iz4Om2DMA7ziPeZ54HURSQFSjTH/8G5/BnjVm58m\n1xizCMAYUwPgPd/nxph87/v1wCDgn8G/LaU6p0FBqeMJ8Iwx5p4WG0V+2eq4E80R0zwXjxv9d6i6\nEW0+Uup4HwDzvPnqfevfDsT+e/Fl4LwB+KcxphQ4JiJnebffDPzDu/pbvohc7j1HjIjEh/QulDoB\n+gtFqVaMMVtE5BfY1a0cQD3wA6ASGCMia4FSbL8D2FTFj3m/9HcBt3q33ww8LiK/8Z7j6hDehlIn\nRLOkKuUnEakwxiSGuxxKBZM2HymllGqkNQWllFKNtKaglFKqkQYFpZRSjTQoKKWUaqRBQSmlVCMN\nCkoppRppUFBKKdXo/wMOkh+NEeqa0gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1bb8d503a58>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# preprocess\n",
    "x_train_pp = x_train - np.mean(x_train)\n",
    "x_train_pp /= np.std(x_train_pp, axis=0)\n",
    "\n",
    "x_test_pp  = x_test - np.mean(x_train) \n",
    "x_test_pp /= np.std(x_train_pp, axis=0)\n",
    "\n",
    "# build model\n",
    "model_name = '1st_place'\n",
    "model = Sequential(name=model_name)\n",
    "model.add(Cropping2D(cropping=3, input_shape=image_shape))\n",
    "model.add(Conv2D(32, (5,5), padding='same', activation='tanh', kernel_initializer='TruncatedNormal'))\n",
    "model.add(MaxPooling2D((2,2), strides=(2,2), padding='valid'))\n",
    "\n",
    "model.add(ZeroPadding2D(padding=1))\n",
    "model.add(Conv2D(32, (4,4), padding='valid', activation='tanh', kernel_initializer='TruncatedNormal'))\n",
    "model.add(AveragePooling2D((2,2), strides=(2,2), padding='valid'))\n",
    "\n",
    "model.add(Conv2D(64, (5,5), padding='same', activation='tanh', kernel_initializer='TruncatedNormal'))\n",
    "model.add(AveragePooling2D((2,2), strides=(2,2), padding='valid'))\n",
    "\n",
    "model.add(Flatten())\n",
    "# TODO is this where dropout should be?\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(3072, activation='tanh', kernel_initializer='TruncatedNormal'))\n",
    "# TODO svm activation function\n",
    "model.add(Dense(n_classes, activation='softmax', kernel_initializer='TruncatedNormal'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# compile model\n",
    "sgd = optimizers.SGD(momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "# fit model\n",
    "epoch_sets = 1\n",
    "epochs = 50\n",
    "batch_size = 100\n",
    "for es in range(epoch_sets):\n",
    "    # get current lr\n",
    "    lr = k.get_value(sgd.lr)\n",
    "    print('epoch set:', es, 'lr value:', lr)\n",
    "    \n",
    "    # fit model\n",
    "    history = model.fit(x_train_pp, y_train_onehot\n",
    "                        , batch_size = batch_size\n",
    "                        , epochs=epochs\n",
    "                        , validation_data=(x_test_pp, y_test_onehot))\n",
    "    # decay lr by half\n",
    "    lr = lr / 2\n",
    "    k.set_value(sgd.lr, lr)\n",
    "    \n",
    "# visualize loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocess: manual center & normalize + sgd_decay on tiny subset + lr=0.001 + batchsize=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "cropping2d_1 (Cropping2D)    (None, 42, 42, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 42, 42, 32)        832       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 21, 21, 32)        0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_1 (ZeroPaddin (None, 23, 23, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 20, 20, 32)        16416     \n",
      "_________________________________________________________________\n",
      "average_pooling2d_1 (Average (None, 10, 10, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 10, 10, 64)        51264     \n",
      "_________________________________________________________________\n",
      "average_pooling2d_2 (Average (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 3072)              4918272   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 7)                 21511     \n",
      "=================================================================\n",
      "Total params: 5,008,295\n",
      "Trainable params: 5,008,295\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "epoch set: 0 lr value: 0.001\n",
      "Train on 50 samples, validate on 50 samples\n",
      "Epoch 1/50\n",
      "50/50 [==============================] - 0s - loss: 1.9169 - acc: 0.4800 - val_loss: 5.8163 - val_acc: 0.6400\n",
      "Epoch 2/50\n",
      "50/50 [==============================] - 0s - loss: 1.8274 - acc: 0.5200 - val_loss: 5.8025 - val_acc: 0.6400\n",
      "Epoch 3/50\n",
      "50/50 [==============================] - 0s - loss: 1.7408 - acc: 0.5800 - val_loss: 5.8025 - val_acc: 0.6400\n",
      "Epoch 4/50\n",
      "50/50 [==============================] - 0s - loss: 1.6181 - acc: 0.6000 - val_loss: 5.8025 - val_acc: 0.6400\n",
      "Epoch 5/50\n",
      "50/50 [==============================] - 0s - loss: 1.5391 - acc: 0.6000 - val_loss: 5.8025 - val_acc: 0.6400\n",
      "Epoch 6/50\n",
      "50/50 [==============================] - 0s - loss: 1.3955 - acc: 0.6000 - val_loss: 5.8025 - val_acc: 0.6400\n",
      "Epoch 7/50\n",
      "50/50 [==============================] - 0s - loss: 1.3140 - acc: 0.6000 - val_loss: 5.8025 - val_acc: 0.6400\n",
      "Epoch 8/50\n",
      "50/50 [==============================] - 0s - loss: 1.2236 - acc: 0.6000 - val_loss: 5.8025 - val_acc: 0.6400\n",
      "Epoch 9/50\n",
      "50/50 [==============================] - 0s - loss: 1.0751 - acc: 0.6000 - val_loss: 5.7869 - val_acc: 0.6400\n",
      "Epoch 10/50\n",
      "50/50 [==============================] - 0s - loss: 1.0304 - acc: 0.6000 - val_loss: 5.7214 - val_acc: 0.6400\n",
      "Epoch 11/50\n",
      "50/50 [==============================] - 0s - loss: 0.8975 - acc: 0.6200 - val_loss: 5.4841 - val_acc: 0.6400\n",
      "Epoch 12/50\n",
      "50/50 [==============================] - 0s - loss: 0.8636 - acc: 0.6200 - val_loss: 5.0125 - val_acc: 0.6400\n",
      "Epoch 13/50\n",
      "50/50 [==============================] - 0s - loss: 0.7792 - acc: 0.6000 - val_loss: 4.0845 - val_acc: 0.6800\n",
      "Epoch 14/50\n",
      "50/50 [==============================] - 0s - loss: 0.7147 - acc: 0.6400 - val_loss: 3.2862 - val_acc: 0.7200\n",
      "Epoch 15/50\n",
      "50/50 [==============================] - 0s - loss: 0.6683 - acc: 0.6600 - val_loss: 3.2026 - val_acc: 0.7000\n",
      "Epoch 16/50\n",
      "50/50 [==============================] - 0s - loss: 0.7200 - acc: 0.5600 - val_loss: 3.6726 - val_acc: 0.6200\n",
      "Epoch 17/50\n",
      "50/50 [==============================] - 0s - loss: 0.6980 - acc: 0.5600 - val_loss: 3.8015 - val_acc: 0.6200\n",
      "Epoch 18/50\n",
      "50/50 [==============================] - 0s - loss: 0.7026 - acc: 0.5800 - val_loss: 3.8891 - val_acc: 0.6600\n",
      "Epoch 19/50\n",
      "50/50 [==============================] - 0s - loss: 0.6624 - acc: 0.6400 - val_loss: 3.7990 - val_acc: 0.6800\n",
      "Epoch 20/50\n",
      "50/50 [==============================] - 0s - loss: 0.6303 - acc: 0.6000 - val_loss: 3.6944 - val_acc: 0.6600\n",
      "Epoch 21/50\n",
      "50/50 [==============================] - 0s - loss: 0.6404 - acc: 0.7400 - val_loss: 3.7473 - val_acc: 0.6600\n",
      "Epoch 22/50\n",
      "50/50 [==============================] - 0s - loss: 0.6228 - acc: 0.6800 - val_loss: 3.8440 - val_acc: 0.6600\n",
      "Epoch 23/50\n",
      "50/50 [==============================] - 0s - loss: 0.6198 - acc: 0.6000 - val_loss: 3.8734 - val_acc: 0.6600\n",
      "Epoch 24/50\n",
      "50/50 [==============================] - 0s - loss: 0.6623 - acc: 0.6000 - val_loss: 3.8712 - val_acc: 0.6800\n",
      "Epoch 25/50\n",
      "50/50 [==============================] - 0s - loss: 0.6152 - acc: 0.6400 - val_loss: 3.9318 - val_acc: 0.6800\n",
      "Epoch 26/50\n",
      "50/50 [==============================] - 0s - loss: 0.6165 - acc: 0.6600 - val_loss: 3.9818 - val_acc: 0.7000\n",
      "Epoch 27/50\n",
      "50/50 [==============================] - 0s - loss: 0.6637 - acc: 0.5400 - val_loss: 4.0142 - val_acc: 0.7000\n",
      "Epoch 28/50\n",
      "50/50 [==============================] - 0s - loss: 0.5859 - acc: 0.6800 - val_loss: 3.9477 - val_acc: 0.7000\n",
      "Epoch 29/50\n",
      "50/50 [==============================] - 0s - loss: 0.5593 - acc: 0.6800 - val_loss: 4.0131 - val_acc: 0.6400\n",
      "Epoch 30/50\n",
      "50/50 [==============================] - 0s - loss: 0.5417 - acc: 0.7400 - val_loss: 4.0665 - val_acc: 0.6400\n",
      "Epoch 31/50\n",
      "50/50 [==============================] - 0s - loss: 0.5191 - acc: 0.8400 - val_loss: 4.1241 - val_acc: 0.6400\n",
      "Epoch 32/50\n",
      "50/50 [==============================] - 0s - loss: 0.6193 - acc: 0.6400 - val_loss: 4.1034 - val_acc: 0.6400\n",
      "Epoch 33/50\n",
      "50/50 [==============================] - 0s - loss: 0.5501 - acc: 0.7200 - val_loss: 4.1305 - val_acc: 0.6800\n",
      "Epoch 34/50\n",
      "50/50 [==============================] - 0s - loss: 0.5309 - acc: 0.7200 - val_loss: 4.1779 - val_acc: 0.6800\n",
      "Epoch 35/50\n",
      "50/50 [==============================] - 0s - loss: 0.5376 - acc: 0.7200 - val_loss: 4.3022 - val_acc: 0.7200\n",
      "Epoch 36/50\n",
      "50/50 [==============================] - 0s - loss: 0.5059 - acc: 0.7600 - val_loss: 4.3403 - val_acc: 0.7200\n",
      "Epoch 37/50\n",
      "50/50 [==============================] - 0s - loss: 0.5953 - acc: 0.6600 - val_loss: 4.3523 - val_acc: 0.6800\n",
      "Epoch 38/50\n",
      "50/50 [==============================] - 0s - loss: 0.5498 - acc: 0.6800 - val_loss: 4.4069 - val_acc: 0.7000\n",
      "Epoch 39/50\n",
      "50/50 [==============================] - 0s - loss: 0.4580 - acc: 0.8000 - val_loss: 4.4169 - val_acc: 0.7000\n",
      "Epoch 40/50\n",
      "50/50 [==============================] - 0s - loss: 0.4986 - acc: 0.7800 - val_loss: 4.4282 - val_acc: 0.7000\n",
      "Epoch 41/50\n",
      "50/50 [==============================] - 0s - loss: 0.4516 - acc: 0.7400 - val_loss: 4.4617 - val_acc: 0.7000\n",
      "Epoch 42/50\n",
      "50/50 [==============================] - 0s - loss: 0.5213 - acc: 0.7400 - val_loss: 4.4713 - val_acc: 0.7000\n",
      "Epoch 43/50\n",
      "50/50 [==============================] - 0s - loss: 0.5243 - acc: 0.7200 - val_loss: 4.6267 - val_acc: 0.6800\n",
      "Epoch 44/50\n",
      "50/50 [==============================] - 0s - loss: 0.4790 - acc: 0.7800 - val_loss: 4.7153 - val_acc: 0.6800\n",
      "Epoch 45/50\n",
      "50/50 [==============================] - 0s - loss: 0.4711 - acc: 0.7800 - val_loss: 4.7206 - val_acc: 0.6800\n",
      "Epoch 46/50\n",
      "50/50 [==============================] - 0s - loss: 0.4699 - acc: 0.8000 - val_loss: 4.8418 - val_acc: 0.6800\n",
      "Epoch 47/50\n",
      "50/50 [==============================] - 0s - loss: 0.4845 - acc: 0.7400 - val_loss: 5.0447 - val_acc: 0.6600\n",
      "Epoch 48/50\n",
      "50/50 [==============================] - 0s - loss: 0.4404 - acc: 0.8200 - val_loss: 5.1214 - val_acc: 0.6600\n",
      "Epoch 49/50\n",
      "50/50 [==============================] - 0s - loss: 0.4524 - acc: 0.8400 - val_loss: 5.1328 - val_acc: 0.6600\n",
      "Epoch 50/50\n",
      "50/50 [==============================] - 0s - loss: 0.4952 - acc: 0.6800 - val_loss: 5.1073 - val_acc: 0.6600\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1ca41435278>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAEKCAYAAAARnO4WAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n+P9m8i+vE4MobwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1ca15a25c88>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# preprocess\n",
    "x_train_pp = x_train - np.mean(x_train)\n",
    "x_train_pp /= np.std(x_train_pp, axis=0)\n",
    "\n",
    "x_test_pp  = x_test - np.mean(x_train) \n",
    "x_test_pp /= np.std(x_train_pp, axis=0)\n",
    "\n",
    "# build model\n",
    "model_name = '1st_place'\n",
    "model = Sequential(name=model_name)\n",
    "model.add(Cropping2D(cropping=3, input_shape=image_shape))\n",
    "model.add(Conv2D(32, (5,5), padding='same', activation='relu', kernel_initializer='TruncatedNormal'))\n",
    "model.add(MaxPooling2D((2,2), strides=(2,2), padding='valid'))\n",
    "\n",
    "model.add(ZeroPadding2D(padding=1))\n",
    "model.add(Conv2D(32, (4,4), padding='valid', activation='relu', kernel_initializer='TruncatedNormal'))\n",
    "model.add(AveragePooling2D((2,2), strides=(2,2), padding='valid'))\n",
    "\n",
    "model.add(Conv2D(64, (5,5), padding='same', activation='relu', kernel_initializer='TruncatedNormal'))\n",
    "model.add(AveragePooling2D((2,2), strides=(2,2), padding='valid'))\n",
    "\n",
    "model.add(Flatten())\n",
    "# TODO is this where dropout should be?\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(3072, activation='relu', kernel_initializer='TruncatedNormal'))\n",
    "# TODO svm activation function\n",
    "model.add(Dense(n_classes, activation='softmax', kernel_initializer='TruncatedNormal'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# compile model\n",
    "sgd = optimizers.SGD(momentum=0.9, nesterov=True, lr=0.001)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "# fit model\n",
    "epoch_sets = 1\n",
    "epochs = 50\n",
    "batch_size = 100\n",
    "for es in range(epoch_sets):\n",
    "    # get current lr\n",
    "    lr = k.get_value(sgd.lr)\n",
    "    print('epoch set:', es, 'lr value:', lr)\n",
    "    \n",
    "    # fit model\n",
    "    history = model.fit(x_train_pp, y_train_onehot\n",
    "                        , batch_size = batch_size\n",
    "                        , epochs=epochs\n",
    "                        , validation_data=(x_test_pp, y_test_onehot))\n",
    "    # decay lr by half\n",
    "    lr = lr / 2\n",
    "    k.set_value(sgd.lr, lr)\n",
    "    \n",
    "# visualize loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "plot_model(model, to_file='model.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocess: featurewise + sgd_decay on tiny subset + lr=0.001 + batchsize=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files (x86)\\Python35-64bit\\lib\\site-packages\\keras\\preprocessing\\image.py:648: UserWarning: Expected input to be images (as Numpy array) following the data format convention \"channels_last\" (channels on axis 3), i.e. expected either 1, 3 or 4 channels on axis 3. However, it was passed an array with shape (50, 48, 48, 1) (1 channels).\n",
      "  ' (' + str(x.shape[self.channel_axis]) + ' channels).')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "cropping2d_4 (Cropping2D)    (None, 42, 42, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 42, 42, 32)        832       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 21, 21, 32)        0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_4 (ZeroPaddin (None, 23, 23, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 20, 20, 32)        16416     \n",
      "_________________________________________________________________\n",
      "average_pooling2d_7 (Average (None, 10, 10, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 10, 10, 64)        51264     \n",
      "_________________________________________________________________\n",
      "average_pooling2d_8 (Average (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 3072)              4918272   \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 7)                 21511     \n",
      "=================================================================\n",
      "Total params: 5,008,295\n",
      "Trainable params: 5,008,295\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "epoch set: 0 lr value: 0.001\n",
      "Epoch 1/50\n",
      "1/1 [==============================] - 0s - loss: 1.9263 - acc: 0.0400 - val_loss: 1.8795 - val_acc: 0.1600\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s - loss: 1.8794 - acc: 0.1000 - val_loss: 1.8258 - val_acc: 0.2200\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s - loss: 1.8307 - acc: 0.2600 - val_loss: 1.7601 - val_acc: 0.2800\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s - loss: 1.7545 - acc: 0.3200 - val_loss: 1.6836 - val_acc: 0.3600\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s - loss: 1.6804 - acc: 0.5000 - val_loss: 1.5974 - val_acc: 0.5000\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s - loss: 1.6068 - acc: 0.5200 - val_loss: 1.5038 - val_acc: 0.5600\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s - loss: 1.4912 - acc: 0.5200 - val_loss: 1.4026 - val_acc: 0.6000\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s - loss: 1.4007 - acc: 0.5800 - val_loss: 1.2961 - val_acc: 0.6200\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s - loss: 1.2738 - acc: 0.5800 - val_loss: 1.1889 - val_acc: 0.6400\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s - loss: 1.1713 - acc: 0.6000 - val_loss: 1.0855 - val_acc: 0.6400\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s - loss: 1.0516 - acc: 0.6400 - val_loss: 0.9913 - val_acc: 0.6400\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s - loss: 0.9760 - acc: 0.6200 - val_loss: 0.9114 - val_acc: 0.6400\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s - loss: 0.8895 - acc: 0.6200 - val_loss: 0.8477 - val_acc: 0.6400\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s - loss: 0.8542 - acc: 0.5600 - val_loss: 0.7989 - val_acc: 0.6400\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 0s - loss: 0.8323 - acc: 0.6000 - val_loss: 0.7606 - val_acc: 0.6400\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - 0s - loss: 0.7314 - acc: 0.5800 - val_loss: 0.7313 - val_acc: 0.6400\n",
      "Epoch 17/50\n",
      "1/1 [==============================] - 0s - loss: 0.7130 - acc: 0.6000 - val_loss: 0.7099 - val_acc: 0.6400\n",
      "Epoch 18/50\n",
      "1/1 [==============================] - 0s - loss: 0.6584 - acc: 0.6200 - val_loss: 0.6950 - val_acc: 0.6200\n",
      "Epoch 19/50\n",
      "1/1 [==============================] - 0s - loss: 0.6495 - acc: 0.6600 - val_loss: 0.6831 - val_acc: 0.6200\n",
      "Epoch 20/50\n",
      "1/1 [==============================] - 0s - loss: 0.6732 - acc: 0.6000 - val_loss: 0.6731 - val_acc: 0.6400\n",
      "Epoch 21/50\n",
      "1/1 [==============================] - 0s - loss: 0.6421 - acc: 0.6600 - val_loss: 0.6650 - val_acc: 0.6000\n",
      "Epoch 22/50\n",
      "1/1 [==============================] - 0s - loss: 0.6718 - acc: 0.5200 - val_loss: 0.6560 - val_acc: 0.6000\n",
      "Epoch 23/50\n",
      "1/1 [==============================] - 0s - loss: 0.6246 - acc: 0.6600 - val_loss: 0.6471 - val_acc: 0.6200\n",
      "Epoch 24/50\n",
      "1/1 [==============================] - 0s - loss: 0.6229 - acc: 0.6000 - val_loss: 0.6395 - val_acc: 0.6400\n",
      "Epoch 25/50\n",
      "1/1 [==============================] - 0s - loss: 0.5729 - acc: 0.7200 - val_loss: 0.6360 - val_acc: 0.6400\n",
      "Epoch 26/50\n",
      "1/1 [==============================] - 0s - loss: 0.5479 - acc: 0.7200 - val_loss: 0.6347 - val_acc: 0.6600\n",
      "Epoch 27/50\n",
      "1/1 [==============================] - 0s - loss: 0.5765 - acc: 0.6600 - val_loss: 0.6310 - val_acc: 0.6800\n",
      "Epoch 28/50\n",
      "1/1 [==============================] - 0s - loss: 0.5039 - acc: 0.8200 - val_loss: 0.6257 - val_acc: 0.6800\n",
      "Epoch 29/50\n",
      "1/1 [==============================] - 0s - loss: 0.5994 - acc: 0.6400 - val_loss: 0.6232 - val_acc: 0.6800\n",
      "Epoch 30/50\n",
      "1/1 [==============================] - 0s - loss: 0.6820 - acc: 0.5600 - val_loss: 0.6242 - val_acc: 0.7200\n",
      "Epoch 31/50\n",
      "1/1 [==============================] - 0s - loss: 0.5467 - acc: 0.7000 - val_loss: 0.6176 - val_acc: 0.6800\n",
      "Epoch 32/50\n",
      "1/1 [==============================] - 0s - loss: 0.5887 - acc: 0.7000 - val_loss: 0.6185 - val_acc: 0.6800\n",
      "Epoch 33/50\n",
      "1/1 [==============================] - 0s - loss: 0.6169 - acc: 0.6200 - val_loss: 0.6218 - val_acc: 0.7400\n",
      "Epoch 34/50\n",
      "1/1 [==============================] - 0s - loss: 0.5694 - acc: 0.7000 - val_loss: 0.6245 - val_acc: 0.7400\n",
      "Epoch 35/50\n",
      "1/1 [==============================] - 0s - loss: 0.5740 - acc: 0.6800 - val_loss: 0.6338 - val_acc: 0.7400\n",
      "Epoch 36/50\n",
      "1/1 [==============================] - 0s - loss: 0.5653 - acc: 0.7400 - val_loss: 0.6384 - val_acc: 0.7400\n",
      "Epoch 37/50\n",
      "1/1 [==============================] - 0s - loss: 0.5073 - acc: 0.7800 - val_loss: 0.6423 - val_acc: 0.7000\n",
      "Epoch 38/50\n",
      "1/1 [==============================] - 0s - loss: 0.5533 - acc: 0.6800 - val_loss: 0.6441 - val_acc: 0.6800\n",
      "Epoch 39/50\n",
      "1/1 [==============================] - 0s - loss: 0.4943 - acc: 0.7800 - val_loss: 0.6486 - val_acc: 0.7000\n",
      "Epoch 40/50\n",
      "1/1 [==============================] - 0s - loss: 0.5564 - acc: 0.7400 - val_loss: 0.6478 - val_acc: 0.6800\n",
      "Epoch 41/50\n",
      "1/1 [==============================] - 0s - loss: 0.5245 - acc: 0.6600 - val_loss: 0.6512 - val_acc: 0.6800\n",
      "Epoch 42/50\n",
      "1/1 [==============================] - 0s - loss: 0.4928 - acc: 0.7400 - val_loss: 0.6518 - val_acc: 0.6800\n",
      "Epoch 43/50\n",
      "1/1 [==============================] - 0s - loss: 0.4827 - acc: 0.8000 - val_loss: 0.6564 - val_acc: 0.7000\n",
      "Epoch 44/50\n",
      "1/1 [==============================] - 0s - loss: 0.5245 - acc: 0.7000 - val_loss: 0.6576 - val_acc: 0.7000\n",
      "Epoch 45/50\n",
      "1/1 [==============================] - 0s - loss: 0.5455 - acc: 0.6800 - val_loss: 0.6617 - val_acc: 0.7200\n",
      "Epoch 46/50\n",
      "1/1 [==============================] - 0s - loss: 0.4933 - acc: 0.7400 - val_loss: 0.6660 - val_acc: 0.6600\n",
      "Epoch 47/50\n",
      "1/1 [==============================] - 0s - loss: 0.4484 - acc: 0.8000 - val_loss: 0.6690 - val_acc: 0.6600\n",
      "Epoch 48/50\n",
      "1/1 [==============================] - 0s - loss: 0.4766 - acc: 0.7000 - val_loss: 0.6660 - val_acc: 0.6600\n",
      "Epoch 49/50\n",
      "1/1 [==============================] - 0s - loss: 0.4365 - acc: 0.7600 - val_loss: 0.6620 - val_acc: 0.7000\n",
      "Epoch 50/50\n",
      "1/1 [==============================] - 0s - loss: 0.4167 - acc: 0.8000 - val_loss: 0.6586 - val_acc: 0.7200\n",
      "epoch set: 1 lr value: 0.0005\n",
      "Epoch 1/50\n",
      "1/1 [==============================] - 0s - loss: 0.4750 - acc: 0.7400 - val_loss: 0.6605 - val_acc: 0.7000\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s - loss: 0.4086 - acc: 0.8400 - val_loss: 0.6618 - val_acc: 0.7000\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s - loss: 0.4432 - acc: 0.8000 - val_loss: 0.6630 - val_acc: 0.7000\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s - loss: 0.4173 - acc: 0.7800 - val_loss: 0.6633 - val_acc: 0.7200\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s - loss: 0.4410 - acc: 0.7600 - val_loss: 0.6633 - val_acc: 0.7200\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s - loss: 0.4800 - acc: 0.7800 - val_loss: 0.6653 - val_acc: 0.7000\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s - loss: 0.4478 - acc: 0.7800 - val_loss: 0.6650 - val_acc: 0.7000\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s - loss: 0.4913 - acc: 0.7400 - val_loss: 0.6667 - val_acc: 0.6800\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s - loss: 0.3749 - acc: 0.9000 - val_loss: 0.6676 - val_acc: 0.6800\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s - loss: 0.4155 - acc: 0.8600 - val_loss: 0.6684 - val_acc: 0.6800\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s - loss: 0.4306 - acc: 0.8200 - val_loss: 0.6679 - val_acc: 0.6600\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s - loss: 0.3891 - acc: 0.8200 - val_loss: 0.6705 - val_acc: 0.6600\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s - loss: 0.3724 - acc: 0.8600 - val_loss: 0.6725 - val_acc: 0.6600\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s - loss: 0.4061 - acc: 0.8400 - val_loss: 0.6723 - val_acc: 0.6600\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 0s - loss: 0.4070 - acc: 0.8200 - val_loss: 0.6741 - val_acc: 0.6600\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - 0s - loss: 0.4100 - acc: 0.7800 - val_loss: 0.6767 - val_acc: 0.6600\n",
      "Epoch 17/50\n",
      "1/1 [==============================] - 0s - loss: 0.4619 - acc: 0.7800 - val_loss: 0.6821 - val_acc: 0.6600\n",
      "Epoch 18/50\n",
      "1/1 [==============================] - 0s - loss: 0.4225 - acc: 0.7800 - val_loss: 0.6874 - val_acc: 0.6600\n",
      "Epoch 19/50\n",
      "1/1 [==============================] - 0s - loss: 0.3911 - acc: 0.8200 - val_loss: 0.6915 - val_acc: 0.6600\n",
      "Epoch 20/50\n",
      "1/1 [==============================] - 0s - loss: 0.3806 - acc: 0.8400 - val_loss: 0.6951 - val_acc: 0.6600\n",
      "Epoch 21/50\n",
      "1/1 [==============================] - 0s - loss: 0.4040 - acc: 0.8200 - val_loss: 0.6999 - val_acc: 0.6600\n",
      "Epoch 22/50\n",
      "1/1 [==============================] - 0s - loss: 0.4021 - acc: 0.8000 - val_loss: 0.7048 - val_acc: 0.6600\n",
      "Epoch 23/50\n",
      "1/1 [==============================] - 0s - loss: 0.4482 - acc: 0.7600 - val_loss: 0.7099 - val_acc: 0.6800\n",
      "Epoch 24/50\n",
      "1/1 [==============================] - 0s - loss: 0.4002 - acc: 0.8000 - val_loss: 0.7148 - val_acc: 0.6800\n",
      "Epoch 25/50\n",
      "1/1 [==============================] - 0s - loss: 0.3595 - acc: 0.8200 - val_loss: 0.7182 - val_acc: 0.6800\n",
      "Epoch 26/50\n",
      "1/1 [==============================] - 0s - loss: 0.4231 - acc: 0.8200 - val_loss: 0.7203 - val_acc: 0.6600\n",
      "Epoch 27/50\n",
      "1/1 [==============================] - 0s - loss: 0.4051 - acc: 0.8200 - val_loss: 0.7223 - val_acc: 0.6600\n",
      "Epoch 28/50\n",
      "1/1 [==============================] - 0s - loss: 0.3553 - acc: 0.8600 - val_loss: 0.7224 - val_acc: 0.6600\n",
      "Epoch 29/50\n",
      "1/1 [==============================] - 0s - loss: 0.3782 - acc: 0.8400 - val_loss: 0.7232 - val_acc: 0.6600\n",
      "Epoch 30/50\n",
      "1/1 [==============================] - 0s - loss: 0.3557 - acc: 0.9000 - val_loss: 0.7241 - val_acc: 0.6600\n",
      "Epoch 31/50\n",
      "1/1 [==============================] - 0s - loss: 0.3768 - acc: 0.8400 - val_loss: 0.7242 - val_acc: 0.6600\n",
      "Epoch 32/50\n",
      "1/1 [==============================] - 0s - loss: 0.3556 - acc: 0.8800 - val_loss: 0.7247 - val_acc: 0.6600\n",
      "Epoch 33/50\n",
      "1/1 [==============================] - 0s - loss: 0.3788 - acc: 0.8400 - val_loss: 0.7232 - val_acc: 0.6600\n",
      "Epoch 34/50\n",
      "1/1 [==============================] - 0s - loss: 0.3370 - acc: 0.8600 - val_loss: 0.7231 - val_acc: 0.6600\n",
      "Epoch 35/50\n",
      "1/1 [==============================] - 0s - loss: 0.3430 - acc: 0.8800 - val_loss: 0.7232 - val_acc: 0.6600\n",
      "Epoch 36/50\n",
      "1/1 [==============================] - 0s - loss: 0.4664 - acc: 0.8000 - val_loss: 0.7220 - val_acc: 0.6600\n",
      "Epoch 37/50\n",
      "1/1 [==============================] - 0s - loss: 0.4011 - acc: 0.8000 - val_loss: 0.7223 - val_acc: 0.6600\n",
      "Epoch 38/50\n",
      "1/1 [==============================] - 0s - loss: 0.4358 - acc: 0.7400 - val_loss: 0.7237 - val_acc: 0.6600\n",
      "Epoch 39/50\n",
      "1/1 [==============================] - 0s - loss: 0.3491 - acc: 0.8400 - val_loss: 0.7274 - val_acc: 0.6600\n",
      "Epoch 40/50\n",
      "1/1 [==============================] - 0s - loss: 0.3846 - acc: 0.7800 - val_loss: 0.7279 - val_acc: 0.6600\n",
      "Epoch 41/50\n",
      "1/1 [==============================] - 0s - loss: 0.3319 - acc: 0.8800 - val_loss: 0.7307 - val_acc: 0.6600\n",
      "Epoch 42/50\n",
      "1/1 [==============================] - 0s - loss: 0.4347 - acc: 0.8400 - val_loss: 0.7339 - val_acc: 0.6600\n",
      "Epoch 43/50\n",
      "1/1 [==============================] - 0s - loss: 0.3484 - acc: 0.8000 - val_loss: 0.7378 - val_acc: 0.6600\n",
      "Epoch 44/50\n",
      "1/1 [==============================] - 0s - loss: 0.3281 - acc: 0.9200 - val_loss: 0.7391 - val_acc: 0.6600\n",
      "Epoch 45/50\n",
      "1/1 [==============================] - 0s - loss: 0.3330 - acc: 0.8200 - val_loss: 0.7447 - val_acc: 0.6600\n",
      "Epoch 46/50\n",
      "1/1 [==============================] - 0s - loss: 0.3747 - acc: 0.8400 - val_loss: 0.7490 - val_acc: 0.6600\n",
      "Epoch 47/50\n",
      "1/1 [==============================] - 0s - loss: 0.3805 - acc: 0.8400 - val_loss: 0.7513 - val_acc: 0.6600\n",
      "Epoch 48/50\n",
      "1/1 [==============================] - 0s - loss: 0.3529 - acc: 0.8400 - val_loss: 0.7571 - val_acc: 0.6400\n",
      "Epoch 49/50\n",
      "1/1 [==============================] - 0s - loss: 0.3600 - acc: 0.8400 - val_loss: 0.7631 - val_acc: 0.6400\n",
      "Epoch 50/50\n",
      "1/1 [==============================] - 0s - loss: 0.3620 - acc: 0.8200 - val_loss: 0.7692 - val_acc: 0.6400\n",
      "epoch set: 2 lr value: 0.00025\n",
      "Epoch 1/50\n",
      "1/1 [==============================] - 0s - loss: 0.3644 - acc: 0.8400 - val_loss: 0.7726 - val_acc: 0.6400\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s - loss: 0.3410 - acc: 0.9000 - val_loss: 0.7747 - val_acc: 0.6400\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s - loss: 0.4004 - acc: 0.8000 - val_loss: 0.7761 - val_acc: 0.6400\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s - loss: 0.3702 - acc: 0.8600 - val_loss: 0.7784 - val_acc: 0.6400\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s - loss: 0.3648 - acc: 0.8200 - val_loss: 0.7781 - val_acc: 0.6400\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s - loss: 0.4005 - acc: 0.8200 - val_loss: 0.7787 - val_acc: 0.6400\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s - loss: 0.3541 - acc: 0.8200 - val_loss: 0.7795 - val_acc: 0.6400\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s - loss: 0.3361 - acc: 0.8800 - val_loss: 0.7798 - val_acc: 0.6400\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s - loss: 0.3121 - acc: 0.8800 - val_loss: 0.7807 - val_acc: 0.6400\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s - loss: 0.3309 - acc: 0.8400 - val_loss: 0.7797 - val_acc: 0.6400\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s - loss: 0.2983 - acc: 0.8400 - val_loss: 0.7798 - val_acc: 0.6400\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s - loss: 0.3041 - acc: 0.9200 - val_loss: 0.7791 - val_acc: 0.6400\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s - loss: 0.3707 - acc: 0.8800 - val_loss: 0.7768 - val_acc: 0.6400\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s - loss: 0.3283 - acc: 0.8600 - val_loss: 0.7763 - val_acc: 0.6400\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 0s - loss: 0.3292 - acc: 0.8600 - val_loss: 0.7766 - val_acc: 0.6400\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - 0s - loss: 0.3336 - acc: 0.8800 - val_loss: 0.7770 - val_acc: 0.6400\n",
      "Epoch 17/50\n",
      "1/1 [==============================] - 0s - loss: 0.3279 - acc: 0.8000 - val_loss: 0.7773 - val_acc: 0.6400\n",
      "Epoch 18/50\n",
      "1/1 [==============================] - 0s - loss: 0.3523 - acc: 0.8400 - val_loss: 0.7768 - val_acc: 0.6400\n",
      "Epoch 19/50\n",
      "1/1 [==============================] - 0s - loss: 0.3443 - acc: 0.8600 - val_loss: 0.7772 - val_acc: 0.6400\n",
      "Epoch 20/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s - loss: 0.3878 - acc: 0.8000 - val_loss: 0.7782 - val_acc: 0.6400\n",
      "Epoch 21/50\n",
      "1/1 [==============================] - 0s - loss: 0.2979 - acc: 0.9000 - val_loss: 0.7785 - val_acc: 0.6400\n",
      "Epoch 22/50\n",
      "1/1 [==============================] - 0s - loss: 0.2996 - acc: 0.9200 - val_loss: 0.7785 - val_acc: 0.6400\n",
      "Epoch 23/50\n",
      "1/1 [==============================] - 0s - loss: 0.3654 - acc: 0.8800 - val_loss: 0.7782 - val_acc: 0.6400\n",
      "Epoch 24/50\n",
      "1/1 [==============================] - 0s - loss: 0.2896 - acc: 0.9200 - val_loss: 0.7778 - val_acc: 0.6400\n",
      "Epoch 25/50\n",
      "1/1 [==============================] - 0s - loss: 0.3249 - acc: 0.8600 - val_loss: 0.7773 - val_acc: 0.6400\n",
      "Epoch 26/50\n",
      "1/1 [==============================] - 0s - loss: 0.3140 - acc: 0.9200 - val_loss: 0.7762 - val_acc: 0.6400\n",
      "Epoch 27/50\n",
      "1/1 [==============================] - 0s - loss: 0.3329 - acc: 0.8200 - val_loss: 0.7757 - val_acc: 0.6400\n",
      "Epoch 28/50\n",
      "1/1 [==============================] - 0s - loss: 0.3143 - acc: 0.8200 - val_loss: 0.7757 - val_acc: 0.6400\n",
      "Epoch 29/50\n",
      "1/1 [==============================] - 0s - loss: 0.3345 - acc: 0.8400 - val_loss: 0.7768 - val_acc: 0.6400\n",
      "Epoch 30/50\n",
      "1/1 [==============================] - 0s - loss: 0.3263 - acc: 0.9000 - val_loss: 0.7772 - val_acc: 0.6400\n",
      "Epoch 31/50\n",
      "1/1 [==============================] - 0s - loss: 0.2446 - acc: 0.9800 - val_loss: 0.7768 - val_acc: 0.6400\n",
      "Epoch 32/50\n",
      "1/1 [==============================] - 0s - loss: 0.3102 - acc: 0.9000 - val_loss: 0.7774 - val_acc: 0.6400\n",
      "Epoch 33/50\n",
      "1/1 [==============================] - 0s - loss: 0.3105 - acc: 0.9200 - val_loss: 0.7774 - val_acc: 0.6400\n",
      "Epoch 34/50\n",
      "1/1 [==============================] - 0s - loss: 0.2944 - acc: 0.9000 - val_loss: 0.7775 - val_acc: 0.6400\n",
      "Epoch 35/50\n",
      "1/1 [==============================] - 0s - loss: 0.3636 - acc: 0.8200 - val_loss: 0.7778 - val_acc: 0.6400\n",
      "Epoch 36/50\n",
      "1/1 [==============================] - 0s - loss: 0.2577 - acc: 0.9200 - val_loss: 0.7785 - val_acc: 0.6400\n",
      "Epoch 37/50\n",
      "1/1 [==============================] - 0s - loss: 0.3564 - acc: 0.8600 - val_loss: 0.7787 - val_acc: 0.6400\n",
      "Epoch 38/50\n",
      "1/1 [==============================] - 0s - loss: 0.2860 - acc: 0.9000 - val_loss: 0.7790 - val_acc: 0.6400\n",
      "Epoch 39/50\n",
      "1/1 [==============================] - 0s - loss: 0.3201 - acc: 0.8400 - val_loss: 0.7794 - val_acc: 0.6400\n",
      "Epoch 40/50\n",
      "1/1 [==============================] - 0s - loss: 0.2680 - acc: 0.9400 - val_loss: 0.7804 - val_acc: 0.6400\n",
      "Epoch 41/50\n",
      "1/1 [==============================] - 0s - loss: 0.3316 - acc: 0.8600 - val_loss: 0.7814 - val_acc: 0.6400\n",
      "Epoch 42/50\n",
      "1/1 [==============================] - 0s - loss: 0.3285 - acc: 0.8600 - val_loss: 0.7831 - val_acc: 0.6400\n",
      "Epoch 43/50\n",
      "1/1 [==============================] - 0s - loss: 0.2885 - acc: 0.8800 - val_loss: 0.7849 - val_acc: 0.6400\n",
      "Epoch 44/50\n",
      "1/1 [==============================] - 0s - loss: 0.2878 - acc: 0.9400 - val_loss: 0.7859 - val_acc: 0.6400\n",
      "Epoch 45/50\n",
      "1/1 [==============================] - 0s - loss: 0.3005 - acc: 0.8600 - val_loss: 0.7864 - val_acc: 0.6400\n",
      "Epoch 46/50\n",
      "1/1 [==============================] - 0s - loss: 0.3905 - acc: 0.7800 - val_loss: 0.7874 - val_acc: 0.6400\n",
      "Epoch 47/50\n",
      "1/1 [==============================] - 0s - loss: 0.2881 - acc: 0.8800 - val_loss: 0.7882 - val_acc: 0.6400\n",
      "Epoch 48/50\n",
      "1/1 [==============================] - 0s - loss: 0.2651 - acc: 0.9000 - val_loss: 0.7885 - val_acc: 0.6400\n",
      "Epoch 49/50\n",
      "1/1 [==============================] - 0s - loss: 0.2846 - acc: 0.9000 - val_loss: 0.7896 - val_acc: 0.6400\n",
      "Epoch 50/50\n",
      "1/1 [==============================] - 0s - loss: 0.2975 - acc: 0.9000 - val_loss: 0.7905 - val_acc: 0.6400\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1ca204a27b8>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3Xd8HOWZwPHfs+q9S1azJVly7xVjmgkQm1BDIEBIAgkH\nuQAhuYQAuUAu5Ei43AXSCC1ASGihhISACdUU27jIxr1KcpOLmmX1trvv/TG7o7W6ba1W5fl+Pvpo\nZ3Z29p3d2Xnm7WKMQSmllAJwBDoBSimlBg8NCkoppWwaFJRSStk0KCillLJpUFBKKWXToKCUUsqm\nQUEppZRNg4JSSimbBgWllFK24EAn4EQlJyebnJycQCdDKaWGlHXr1lUaY1J6227IBYWcnBwKCwsD\nnQyllBpSRGRfX7bT4iOllFI2vwYFEVksIjtFpEhE7uri+dEiskxEPhORTSJyoT/To5RSqmd+Cwoi\nEgQ8DCwBJgHXiMikDpv9GHjJGDMTuBr4g7/So5RSqnf+rFOYBxQZY0oARORF4FJgm882Boj1PI4D\nDp3MG7W1tVFaWkpzc/MpJHfwCw8PJysri5CQkEAnRSk1TPkzKGQCB3yWS4H5Hbb5L+AdEbkNiALO\nO5k3Ki0tJSYmhpycHETkZHYx6BljqKqqorS0lNzc3EAnRyk1TAW6ovka4E/GmCzgQuAvItIpTSJy\nk4gUikhhRUVFp500NzeTlJQ0bAMCgIiQlJQ07HNDSqnA8mdQOAhk+yxnedb5+ibwEoAx5lMgHEju\nuCNjzOPGmDnGmDkpKV03sx3OAcFrJByjUiqw/Fl8tBYoEJFcrGBwNXBth232A58D/iQiE7GCQues\ngFJKDWeuNmiuaf9rrYeWemhtgNY6z+N6GLcYMmf5NSl+CwrGGKeI3Aq8DQQBTxljtorIfUChMeZ1\n4PvAEyLyPaxK5+vNEJw0+tixYzz//PN8+9vfPqHXXXjhhTz//PPEx8f7KWVKqX7jdoOzGVrqoKUW\nmmuhxXshr7Uu4G0Nngt5Y/tyWzO426wLv6vN87jVWt9Sa72+rbFvaYhOG7pBAcAYsxRY2mHdvT6P\ntwEL/ZmGgXDs2DH+8Ic/dAoKTqeT4ODuP+KlS5d2+5xSqp+5XVBfDnWHoPYw1B2GhgrPxb3W50Jf\na134nS1WEPD+d7X27X0cIRAa1f4XHAZBodb6oBAICbceB4dBeFznv7BYCIuG0GgIi/HsJ9r67wjy\n72fEEBzmYjC66667KC4uZsaMGYSEhBAdHU16ejobNmxg27ZtXHbZZRw4cIDm5mZuv/12brrpJqB9\nyI76+nqWLFnCGWecwcqVK8nMzOQf//gHERERAT4ypQYxY6wilWbP3XZLrXWRry+DujLrf325Z/mI\n9d+4Ou8nNAbCY62LcXgsRCZDQi6ERFgX7uDw9v9BodaF2nvxDo9rf21YNIREQXDowH8W/WjYBYWf\n/nMr2w7V9us+J2XE8pOLJ1sLxoDbCcYNCIjwwP3/zZYtW9iwfh0ffvghX7j4ErZsWE9u7hhoa+Kp\nR39HYkI8TY0NzD1jEVcsPoekxHjrBK07AnX17N69mxcef5Anfvljrrrh27z6zKNcd+Ul1vsYl5V1\nNS6oOQI/P9/nRPQ5oUMiAeN5jfE8NiBineAhkdZfqPd/FMRmQHwOxGdbJ/5Q42yxsulup+fP1f79\nuF2e76mLz8QR1H7nFhTS9eMBuCsb8txuT5FJY/t/d5vne3B5zl3f/26fZZ/vy/du3Nni+WvyFNXU\nee7gPY9batrv6I27m4QJRCVbxS3RqZAyAWLTISbdOue9/6NS9HvuYNgFhVNnjr94GGNV8lQVe07Y\nVqDDiVh5yDqhj2yCoyXMmz6J3KhGKN8OwG9/9SivvbUMgAOlh9m98VOSZk+zfhANFdDcRO7oTGYU\nZEFLHbOnjGfv3j3Wj0u8F68gcDggtA5mfdWnPLMWGo9C9V5oawJxYAUr8AYtjLHS5/3hdvlDEk+A\nGAMJYyAiwXOHFG5ld72PxeEpE3W2l5N6l12tPmWnrR3KUDs8drsgKBiCwjrcjXkCk6vNulj4vsbZ\nfHxlXHONtc5v5PggYd85RrR/JiERx98xhsdBmOdxSMTxd5ne/51bXbd/R8010Hys/fiajllBz9Xq\n85m2tl94HUHHF0140+tqtfbX1uQpAmm2yrC7vYjiE0DN8cHU7Wr/Prx/rlbrfHI2+evD99yVd7jx\niR99/LLvnXp4rHWRj06z7vaD9PJ2Mobdp2bf0XfFezI7W31+ZL4XLydWfXcH4rCeDw6zTr6gUOsC\n7Q0e1S5wBFt3H5FJRMXGQ1w2iIMPP1nBe59u4tMVnxAZFc055y+hOTIDRk219jNqKtTXExYZYz0G\nguIzaKqvt+5uOopogMW/OPkPyBjPD7rBynrXHLQCyrF9UL3PerznYyvYOJs8n0kfOYKtY7IvpKHW\nD7PT4xBr27YmaKq2vg/fslvo+u49JNy6CMRm+pTBxlrZf0eQtU9HcPtjcVhB0Q6U0n5B9t6her//\nToHLN+h5tmtrar+DbWtuv4gfO9BeHt3fF0lxWOXJQSGdP1tHUHvg9K3AdDut548L6p6A5ujuJ298\nPifP5+Z9HOT9XD3fm3c5NMoqLgmNPP6xw5PLcgR5bmo6/nd4vp+g9hxbcGh78UxwmHWz4Ah0N6qR\nadgFhW7VHbEqlnyJo/2HFhx+/Alv/wA8P74e+gjEjHJQ19AEMaMgMtE6qaOs7hY1zW4SkpKJTEhj\nx44drFq9pv2iGAgi7XfkkYnWndeYBd1v73K232k6mz13+KHtx+C9SDmC9UcMVoDztijxBhDfCsu2\nJrq88QDrQhgRD+Hx7UEvNFo/VzWgRk5QCIuxLohBoe1/juAeL/Z9lZSUxMKFC5kyZQoRERGkpaXZ\nzy1evJhHH32UadOmMX78eE477bRTfr8BFRQMQdFWJZrqXXAoBCfbNwVKDTUy1LoFzJkzx3ScZGf7\n9u1MnDgxQCkaWCPpWJVS/UdE1hlj5vS2neZLlVJK2TQoKKWUsmlQUEopZdOgoJRSyqZBQSmllE2D\nglJKKZsGhX7gHSX1ZPz617+msbGPw+YqpZSfaVDoBxoUlFLDxcjp0exHvkNnn3/++aSmpvLSSy/R\n0tLC5Zdfzk9/+lMaGhq46qqrKC0txeVycc8991BWVsahQ4dYtGgRycnJLFu2LNCHopQa4YZfUHjr\nLjiyuX/3OWoqLHmg26cfeOABa+jsDRt45513eOWVV1izZg3GGC655BI+/vhjKioqyMjI4M033wSg\npqaGuLg4HnzwQZYtW0Zysg6LoJQKPC0+6mfvvPMO77zzDjNnzmTWrFns2LGD3bt3M3XqVN59913u\nvPNOPvnkE+Li4gKdVKWU6mT45RR6uKMfCMYY7r77bm6++eZOz61fv56lS5dy9913c8EFF3Dvvfd2\nsQellAoczSn0g5iYGOrq6gD4/Oc/z1NPPUV9fT0ABw8epLy8nEOHDhEZGcl1113HD37wA9avX9/p\ntUopFWjDL6cQAL5DZy9ZsoRrr72WBQusOQqio6N59tlnKSoq4o477sDhcBASEsIjjzwCwE033cTi\nxYvJyMjQimalVMDp0NlDzEg6VqVU/9Ghs5VSSp0wDQpKKaVswyYoDLVisJMxEo5RKRVYwyIohIeH\nU1VVNawvmsYYqqqqCA8PD3RSlFLD2LBofZSVlUVpaSkVFRWBTopfhYeHk5WVFehkKKWGMb8GBRFZ\nDPwGCAL+aIx5oMPzDwGLPIuRQKoxJv5E3yckJITc3NxTTa5SSo14fgsKIhIEPAycD5QCa0XkdWPM\nNu82xpjv+Wx/GzDTX+lRSinVO3/WKcwDiowxJcaYVuBF4NIetr8GeMGP6VFKKdULfwaFTOCAz3Kp\nZ10nIjIGyAU+8GN6lFJK9WKwtD66GnjFGOPq6kkRuUlECkWkcLhXJiulVCD5MygcBLJ9lrM867py\nNT0UHRljHjfGzDHGzElJSenHJCqllPLlz6CwFigQkVwRCcW68L/ecSMRmQAkAJ/6MS1KKaX6wG9B\nwRjjBG4F3ga2Ay8ZY7aKyH0iconPplcDL5rh3PNMKaWGCL/2UzDGLAWWdlh3b4fl//JnGpRSSvXd\nYKloVkopNQhoUFBKKWXToKCUUsqmQUEppZRNg4JSSimbBgWllFI2DQpKKaVsGhSUUkrZNCgopZSy\naVBQSill06CglFLKpkFBKaWUTYOCUkopmwYFpZRSNg0KSimlbBoUlFJK2TQoKKWUsmlQUEopZdOg\noJRSyqZBQSmllE2DglJKKZsGBaWUUjYNCkoppWwaFJRSStk0KCillLJpUFBKKWXToKCUUsqmQUEp\npZTNr0FBRBaLyE4RKRKRu7rZ5ioR2SYiW0XkeX+mRymlVM+C/bVjEQkCHgbOB0qBtSLyujFmm882\nBcDdwEJjTLWIpPorPUoppXrnz5zCPKDIGFNijGkFXgQu7bDNvwEPG2OqAYwx5X5Mj1JKqV74Myhk\nAgd8lks963yNA8aJyAoRWSUii7vakYjcJCKFIlJYUVHhp+QqpZQKdEVzMFAAnANcAzwhIvEdNzLG\nPG6MmWOMmZOSkjLASVRKqZHDn0HhIJDts5zlWeerFHjdGNNmjNkD7MIKEkoppQLAn0FhLVAgIrki\nEgpcDbzeYZu/Y+USEJFkrOKkEj+mSSmlVA/8FhSMMU7gVuBtYDvwkjFmq4jcJyKXeDZ7G6gSkW3A\nMuAOY0yVv9KklFKqZ2KMCXQaTsicOXNMYWFhoJOhlFJDioisM8bM6W27QFc0K6WUGkQ0KCillLJp\nUFBKKWXToKCUUsqmQUEppZRNg4JSSimbBgWllFI2DQpKKaVsGhSUUkrZNCgopZSyaVBQSill06Cg\nlFLKpkFBKaWUTYOCUkopmwYFpZRSNg0KSimlbBoUlFJK2foUFETkdhGJFcuTIrJeRC7wd+KUUkoN\nrL7mFL5hjKkFLgBSgBuAB/yWKqWUUgHR16Agnv8XAk8bYzb6rFNKKTVM9DUorBORd7CCwtsiEgO4\n/ZcspZRSgRDcx+2+CcwASowxjSKSiFWEpJRSahjpa05hAbDTGHNMRK4DfgzU+C9ZSimlAqGvQeER\noFFEpgM/BPYBf/ZbqpRSSgVEX4OC0xhjgEuB3xhjfgPE+C9ZSimlAqGvdQp1InI38FXgTBFxACH+\nS5ZSSqlA6GtO4ctAC1Z/hSNAFvC/fkuVUkqpgOhTUPAEgueAOBG5CGg2xvRapyAii0Vkp4gUichd\nXTx/vYhUiMgGz9+NJ3wESiml+k1fh7m4ClgDXAlcBawWkS/18pog4GFgCTAJuEZEJnWx6V+NMTM8\nf388odQrpZTqV32tU/hPYK4xphxARFKA94BXenjNPKDIGFPiec2LWBXV204+uUoppfypr3UKDm9A\n8Kjqw2szgQM+y6WedR1dISKbROQVEcnuakcicpOIFIpIYUVFRR+TrJRS6kT1NSj8S0Te9tQBXA+8\nCSzth/f/J5BjjJkGvAs809VGxpjHjTFzjDFzUlJS+uFtlVJKdaVPxUfGmDtE5ApgoWfV48aY13p5\n2UHA984/y7POd79VPot/BH7Zl/QopZTyj77WKWCMeRV49QT2vRYoEJFcrGBwNXCt7wYikm6MOexZ\nvATYfgL7V0op1c96DAoiUgeYrp4CjDEmtrvXGmOcInIr8DYQBDxljNkqIvcBhcaY14HviMglgBM4\nClx/coehlFKqP4g1esXQMWfOHFNYWBjoZCil1JAiIuuMMXN6207naFZKKWXToKCUUsqmQUEppZRN\ng4JSSimbBgWllFI2DQpKKaVsGhSUUkrZNCgopZSyaVBQSill06CglFLKpkFBKaWUTYOCUkopmwYF\npZRSNg0KSimlbBoUlFJK2TQoKKWUsmlQUEopZdOgoJRSyqZBQSmllE2DglJKKZsGBaWUUjYNCkop\npWwaFJRSStk0KCillLJpUFBKKWXToKCUUsqmQUEppZTNr0FBRBaLyE4RKRKRu3rY7goRMSIyx5/p\nUUop1TO/BQURCQIeBpYAk4BrRGRSF9vFALcDq/2VFqWUUn3jz5zCPKDIGFNijGkFXgQu7WK7nwH/\nAzT7MS0nZX9VI06X+5T343S52Xaoth9SpJRS/uXPoJAJHPBZLvWss4nILCDbGPOmH9NxUqobWjnv\noY/47ze3n/K+fvdBEV/43SfsKqvrh5QppZT/BKyiWUQcwIPA9/uw7U0iUigihRUVFf5PHPDx7gpa\nnW7+smofu0/hYl7X3MbTK/ZgDLy6vrQfU6iUUv3Pn0HhIJDts5zlWecVA0wBPhSRvcBpwOtdVTYb\nYx43xswxxsxJSUnxY5LbLdtRTkJkCJGhQdy/9ORzC8+u2k9ts5P81Gj+/tlBXG7Tj6lUSqn+5c+g\nsBYoEJFcEQkFrgZe9z5pjKkxxiQbY3KMMTnAKuASY0yhH9PUJy634aNdFSyakMp3zi3gw50VfLiz\n/IT309zm4snlJZxZkMz3zx9HWW0LK4oq/ZBipVQgPf5xMev2VQc6Gf3Cb0HBGOMEbgXeBrYDLxlj\ntorIfSJyib/etz9sLD1GdWMbi8an8vXTc8hJiuS/39x+wpXOL67ZT2V9K7cuyufcianERYTwNy1C\n6le7y+r4xdLtuDUHpgKkocXJL97awZ8/3RvopPQLv9YpGGOWGmPGGWPGGmPu96y71xjzehfbnjMY\ncgkAH+4oxyFwVkEKocEOfnThRIrK63l+zf4+76PV6eaxj0uYm5PA/LwkwoKDuHh6Ov/aeoS65jY/\npn5kefDdXTz2cQkllQ2BTooaobYfrsUY2FVWH+ik9Avt0dyFZTsrmD0mgbjIEADOn5TGgrwkHnp3\nFzWNfbugv/ZZKYdrmrllUb697ouzsmhuc/PWliN+SfdIU1HXwrvbygDYeqgmwKlRI9W2w1Zz8+KK\n+mFRZ6hBoYPyumY2H6zhnPGp9joR4Z6LJnGsqY3ffrC71304XW4e+bCYqZlxnD2uvWJ8ZnY8eclR\nvLpOi5D6w9/Wl+J0G4Icov1AVMBsPWide61ON/uPNgY4NadOg0IHH+20mrwu8gkKAJMyYrl6bjbP\nrNxLSUXP2cQ3Nx9mb1Ujtywai4jY60WEL87KZPWeoxwYBidPIBlj+OvaA8zNSWBiegxbNSioANl6\nuIYET6nCcOiLpEGhgw93VpAWG8bE9JhOz/3H+eMJDwni50t3dPt6t9vwh2XFFKRGc8GkUZ2ev3xW\nFgCvfXaw03Oq71bvOUpJZQNXzx3N5PQ4th2uxZihn3VXQ0uby82uI/VcODUd4JT6NA0WGhR8tLnc\nfLy7gnPGpR53h++VEhPGLYvyeW97GW9vPdLlRei97WXsLKvj24vG4nB03kdmfAQL8pL42/rSQXsR\n+3BnOXsHecXti2v2ExMezIVT05mcGcvRhlaO1A66kVLUMFdUXk+ry8283EQy4yOGRWWzBgUf6/dV\nU9fsZNGE7jvI3bDQaqJ681/Wcc7/fcj9b25j7d6juNwGYwwPLytidGIkF0/L6HYfV8zOYm9VI+v3\nD752zc1tLm76yzr+8++bA52Ubh1rbGXpliNcPjOTiNAgJqXHAu1lu0oNFG+x5eSMOArSorX4aChp\ndbpZWdxzx7FlOysIdggL85O73SY8JIi/37KQX3xxKrnJUTyzch9XPvop83/+Ht96dh0bS2v41tlj\nCQ7q/qNdPGUUESFBvLp+8BUhrdtXTavTzYqiql7rTgLltc8O0up0c/Xc0QBMTI9FhFOuV/hwZzmF\ne4/2RxLVCLH1UA0RIUHkJkcxLi2GksqGfhlEM5BGTFD4zfu7+NqTa/ish7vzD3eWMzcnkZjwkB73\nFR8ZyjXzRvOnG+ax7p7z+N01MzktL4kVRVVkJURwxezMHl8fHRbMkimjeGPjIZrbXCd1PP6ysriS\nYIcQ7BBeOIF+GQPFGMOLaw4wPSuOSRlWDiEqLJjcpKhTapZqjOEHL2/kZ/0wAOJwt7+qkYt/t5wj\nNVpct/VQLRPSYwhyCAWp0cOiBdKICQo3nTWWtNhwbnvhM2qaOvc1OHSsiR1H6nosOupKTHgIF0/P\n4PfXzmLdPefxzvfOIiw4qNfXfXFWFrXNTt7ffuLDZ/jTyuIqpmfHc/6kNF5ZVzrogtZnB46xs6yO\nL3tyCV6TMmJPKaews6yOyvpWth2qGXTHPNj8a+thNh+sYfWeqkAnJaCMMWw/VMtkz83JuDSrccpQ\nr1cYMUEhLiKE3107k8M1zfzotc2dKnk/7KYp6okICw4iMjS4T9suGJtEelz4oBo5ta65jU2lNZw+\nNomvzB9DdWMb/xpkHe1eXLOfyNAgLplxfJ3N5Iw4Dh5r4lhj60ntd0WRdYFrcxk2H9SOcD1ZVWIV\nsRWV9/3iV1rdyNGGk/tuBqsDR5uoa3EyOSMOgPzUaGDot0AaMUEBYNboBL5/wTje3HSYv649cNxz\ny3aWkxkfYX+x/hbkEC6bmclHuyooHyStZrwV5gvGJnH62CRykiJ5bvW+QCfLVtfcxj83HubiaRlE\nhx0ffL13ayfbie3T4kqSo8MAhs3AZv7gchvW7rGCwu4TuCP++lNr+M/XBm/jhZPhLa6c7FOMmRkf\nwa4TCJaD0YgKCgDfOmssZ+Qn81//3Gq3FGhxulhRVMmiCSldNkX1lytnZ+EQ+PZz62lsdQ7Y+3Zn\nZVEVocEOZo1OwOEQrp0/mrV7qwesRYXbbbjzlU384OWNXb7nPzcepqnNxdXzsjs9561f8A45cCKc\nLjerS45y/qQ08pKjNCj0YNuhWupanIQFOyjqY0OEhhYnxRUNrCyuGlYDF249VEuQQ+xiI4BxadGa\nUxhqHA7hwS9PJzosmFufX09zm4vCvdU0tro4Z9zJFx2djLyUaH5z9UzW76/mpj+vC3hZ9sriKuaM\nSSA8xKoT+dLsbEKDHDy/emAqnJ9asYe/Fh7g9Q2HuOChj7nxmbWs29feGujFtfuZMCqGGdnxnV6b\nHB1GWmzYSdUrbDpYQ12Lk4X5Scwak8D6fdWDtg9JoHnrES6alsHeygba+tDSxhvga5ra2DnEL5i+\nth2uJT8l2v69gFWvUFIxtFsgjbigAJAaE86vrprBrrJ6fvbGNpbtKCc0yMHp+UkDnpYLp6bzf1dO\nZ3lRJbc8t75PPzJ/qG5oZdvhWk4f2/4ZJEaFsmTqKF5dX+r3nMyOI7X88l87OX9SGqt+9Dm+e14B\nhfuqueKRT7nq0U95esUeNpXWcPXc7G5zc5Mz4k6qBdJKzxwXC/KSmD0mgaqGVvZVDe0WJP6yqqSK\n3OQoFuYn4XQb9lX13snRN9e3Zs/wafK79VCNXXTkVZAWQ6vLzb4h3AJpRAYFgLPHpXDzWXk8t3o/\nL649wPy8xD5XEve3L87K4r8vm8L7O8r57l83BGSkxVUl1h3ggrHH99H4yvwx1DU7eWPjYb+9d3Ob\ni9tf2EBsRAgPfHEqiVGhfPe8cay861zuvWgSpdWN/PSf2wgLdnD5zKxu9zM5I5biioYTznGtLK5i\nYnosSdFhzB6TAGi9QldcbsOaPUc5LS/Rp1K19yKkHUfqiAgJIiMufNi0WKqsb6GstsUutvQalzb0\nK5tHbFAA+P4F45meHU99i/OUWh31h+tOG8N/XjiRNzcd5s5XNw142evK4iqiQoOYlhV33Pq5OQkU\npEb7tcL5f9/eyc6yOv73ymkkeSp7ASJDg/nGGbl89MNFPPTl6Tz05Rn2cOZdmZwRi8tt2HGk7z/I\n5jYXhfuq7RxSfko0MeHBrBuEvc0DbfvhWmqbnczPTWJsinXx60sLpF1ldYxLi+a0vCTW7Dk6LIrm\nvMWUHYOCN1gO5WapIzoohAY7+P01M7lw6igump4e6OTwb2fl8b3zxvHKulJ+8vrWAf3xrCyuZF5u\nIiEdemKLWBXOG0tr2OKHpprLd1fy5PI9fG3BmG4Dc0iQlUPwDjrWHW/TwBMpQvL24F7oKTp0OIRZ\no616BXW81Z6in/l5iXZLm919CAo7j9QxLi2GebmJVNa3UlwxuMfV6gtvK7fJ6cffREWGBpOVEDGk\nh7sY0UEBIDsxkj98ZTapMeGBTgoA3/lcPjeflcdfVu3j4WVFA/KeZbXNFFc0cPrYrof3+OLMLMJD\nHDx3EhXOrc7u60iqG1r5/ssbyE+N5u4lE0943x1lJUQQEx58QpXNK4qsHtzzctvrUmaPSWBnWR21\ng2CGvIq6FpbtLB8Uk7esKqliTFIk6XERgHVX3FtOobK+hcr6VsaPsoICDI96ha2HashKiOgy5zou\nLeaEmusONiM+KAw2IsJdSyZw6YwMHnpv9wmVbZ/shePTYm99QtcV7XGRIVw8LYN/bDjY61SiRxta\nWbr5MD/++2bO/dWHjL/nLa54ZCVPLt/DoWNN9nbGGH702maONrTy6y/PICK0917gvRERJqWfWM9m\nbw9u334Ps0YnYAxs2H/slNPkq7HVyXde+IyXCw/0vjFWa51rnljFDU+v5bwHP+KltQd6DLL+5PbW\nJ/gEz/zU6F5nG9vlKcobPyqG3OQoUmLCWOOneoXNpTV8tr96QJp3b/PpydxRQVo0JZX1AWs0cqoC\nU7OqeiQi/OyyKazbV813//oZS79zZo/jMbnc1rg96/ZV89btZxIVdmJf68riSuIiQuzRRrvyldPG\n8PK6Up5cvoeF+ck0tDhpaHHR0OqkocVJaXUTK4ur2O7pJxAVGsS83ETOm5jGx7sq+Nkb2/jZG9uY\nOTqeL3iKgd7acoQ7F09gSmZct+97oiZnxPHc6n04Xe4eByUEqG1uY1PpseOmTAWYnh2HQ6yipbPG\nndiwJ91pc7n592fX89GuCv656RChwQ4undH9GFltLje3Pr+evZUN/HDxeN7cdJgfvrqJh97bxU1n\n5XH13NH9Ekj7aseROmqa2pifl2ivK0iNpsXp5mB1E6OTIrt8nbcJ6vhRMYgI83ITWe2pV+jPPkE1\njW188ZEVtLkMIpCbFMWE9BgmjoplYnosp+cn9VtDkoYWJ3uqGrr9/salxtDmMuyrahywzrD9SYPC\nIBUbHsKp92MaAAAaYklEQVRvrp7BlY9+yk/+sZUHvzyjy+2MMfzX61vtSXueXrGHW88tOKH3Wllc\nxYK8pC7nf/CanhXH5IxYfv3ebn79XucpScOCHczJSeCOz49nwdgkpmbG2fUTP7pwIiUV9by15Qhv\nbjrMf3sGnZuXm8hNZ+WdUFp7Mzkjlhanmz2VDRSkdZ4oydfqkqO4DZ2KzWLCQxg/KrbfhjZ3uw13\nvLyRj3ZVcN+lk3lj02G+/9JG4iJCjpv21csYw09e38onuyv55ZemcdWcbP797LF8tKuCPywr5qf/\n3MbvPyji5rPz+Lcz8wakw6W3ddr8vONzCgC7y+u6DwpH6kiIDCHF04DgtNxE3tx0mNLqJrITu37N\nyfi0pJI2l+GHi8fT6nSz/XAtWw7WsnSzNUzLwvwknrvxtH55r+2HazGGbnMK3s5su8vqNCio/jV7\nTCK3nVvAb97fzdnjU7q8M/nt+0X8ZdU+bjorj5KKBh77uITrThtDfGRon97jwNFGSquber04iwiP\nfXU2m0triAoLJiosmOiwYCJDg4gOCyY6PLhTJbWvvJRoblmUzy2L8tlb2cDHuytYPGUUQT0EopMx\nOdMzt8Kh2l6DwoqiSsJDHMwa07kz3Owx8fz9s0O4PHNAd2VlcSX3/XMbtyzK56Jp6V1enI0x3L90\nO3/fcIg7Pj+ery3I4bKZmVz92Cq+9ew6nrtxPrPHJB73mieX7+H51fv593PGctUcq/e2iHDO+FTO\nGZ/Kmj1H+c37u/j50h3MHpNoN6P1p9V7qshOjCAzPsJe573gFZXX87mJaV2+bmdZnZ1LAOy6m1Ul\nVf0aFD7ZXUlUaBD/dmbecedhfYuTxz4q5ncfFLHlYE2/5Eq9vea951pH+anRiFgtkJZMPeW3G3Ba\npzDI3XZuPrPHJPDj17Z0mtf52VX7eOi9XVwxK4u7Fk/gjs+Pp77FySMfFfd5/945Jk7vpj7BV1ZC\nJEumpnPWuBRmj0lg/KgYshMjSYgK7TEgdJSTHMXXFuT4pXJ/bEo0ocGOPrVA+rS4irk5iV2Oajt7\nTAL1Lc5uW5EYY3jgrR3sOFLHbS98xjefKaS0unOHpcc+LuHJ5Xu4/vQcvn3OWMDKBT7zjXmMig3n\nhqfXsuNIex3Iu9vKuH/pdpZMGcUdF4zv8r3n5Sby0FVWznHDgf6t9+iK221Y3aE+Aawh5JOjw7qt\nbHa7DbuO1DHeJzgXpEaTEBnSa2WzMeaEBtBbXlTJaXlJnc7D6LBgbjwzj4iQIJ5ZubfP++vJ1oO1\nJEaFMiq26/M3IjSI7IRIdpUPzRZIGhQGueAgB7/2FB19768b7O7zSzcf5p5/bOHcCak8cMVUHA5h\n/KgYLpuRyZ9W7KWsj4PsrSyuIiUmzG53PtSFBDmYMCqm18rmiroWdpbVdVu5Pnu0dffeXUX/x7sr\n2VRaw/2XT+GeiyaxqqSKCx76mCeX77ErXl8uPMADb+3g4ukZ3HvRpONyEikxYfzlm/OJCA3ia0+u\nYX9VI1sO1vCdFz5jWmYcD141o8fivNTYcDLiwtk4AEFhV3kdxxrbjis68ipIje62WerBY000tLoY\nP6r9jtrhEObmJLKml8mMnvikhAW/eL9P5/H+qkb2VTVyRkHXrefiIkK4YnYm/9h4qF9Gat16uIZJ\n6bE9FtsVpA7dMZA0KAwB2YmR/OyyKRTuq+bhZcWsLKrkuy9uYNboBB6+dtZxd0ffO28cLrfht+93\nLvfvyBjDyuIqTh+bNKADAfrbZM/cCj318/DmkBZ20ww3OzGC5OiwLvsrGGP43fu7yYgL58rZ2Xzz\njFze+d5ZzM9N5GdvbOPyP6zgqeV7uOtvmzkjP5lfXTm9ywt8dmIkf/nmfFqcbr761Gq++cxaEiJD\neOLrc/pUiTwtK56Npf4PCqs8rdPm5yZ2ei4/NZri8vouP+uddsuj42845uUmsq+qsdtJeuqa2/jD\nh8W0ON19Grr9kyJr2PszC7pvFPD1BTm0Ot2nPHFUm8vNriP13dYneBWkxbCnj2NDDTYaFIaIy2Zm\nctmMDH77wW7+7c+F5CZH8dTX53a6eIxOiuSaeaP569oDvY5LU1xRT0VdS5+KjoaSSemx1DS1cdCn\nCWxHK4uqiA0P7raMWUSYPSa+y57Nq0qOUrivmm+dM5bQYOsnlJUQyVPXz+V318zk0LEm7ntjG5Mz\nYnn0q7PtbboyLi2Gp2+YS0VdC/XNTp68fm6fi9WmZ8ezr6rxpOeQ6KvVe46SGR/RZR1AQVo0dS1O\nympbOj3nbXk0rkPdzmmeHEd3Q148s3IvxxrbSIoKZenm3odXWb67kvS4cMamRHW7TUFaDGcWJPPs\nqn2ndKEuKq+n1eXu1JO5o3Fp0Z4WSEOvo55WNA8h9102hXX7q3G74ZlvzOt2yIfbzs3n5XUHePDd\nXfzm6pnd7m+l5w6wu05rQ9Uku2dzLVkJXVdmriyxyqB7quiePSaBt7eWUVHXQkpM+/Abv1+2m5SY\nMLsS2EtEuHh6BmcWJPO39Qe5bGZmp3kfujJrdAKvfXshBsOEUT1fbHxNz7aOc2NpDWf3oensK+tK\n2XCgGqfL4HQbnC43bW6Dy2WYnh3Pt87u3JLJGKs+obve5vk+w12Mijs+mO0qqyMzPqJTc+qJ6bHE\nhAWzZs/RTo0napvbeOKTPXxuQipTMuP47Qe7Ka9r7jZQutxWbveCSWm95navPz2Hbz5TyDtby/jC\ntJMbwcBbLOntPd8d31nY8lN7bvAw2Pg1pyAii0Vkp4gUichdXTz/LRHZLCIbRGS5iEzyZ3qGutjw\nEN647Uze/t5ZnX6AvlJjw7lhYS6vbzxk9xvoysqiqm7vAIeyiekxiHQ/4c6Bo40cONrUaw7J26rH\nt2nqun3VrCiq4uaz8o4bMtlXfGQo3zgjl8SovrUAA6sd/4kEBICpmXGI0Kd6haZWFz/622ZeW3+Q\nD3aU82lxFZ8dOMb2w7XsLKvjf/61gztf3dSpI9ru8nqONrQe1z/BV35ae7PUjnYesVoedRTkEObk\nJNjDZvj604q91DS18d3zxnHh1HSMgbe3lnV7XJsP1lDT1NZtfYKvReNTGZMUyZ9W7ul12+5sPVRD\nREgQucnd50rAavBgtUAaevUKfgsKIhIEPAwsASYB13Rx0X/eGDPVGDMD+CXwoL/SM1zERYT06e7z\nW2eNJSYsmF+9s7PL56sbWlm1p2rYFR2BNf5MXnJUt5XNKzxDZS/M7/lCMjkjjtAgx3H1Cr//YDcJ\nkSFcO390D68cGDHhIYxNie5TUFi79yitLjd/uG42a/7zPFbcdS4f3bGID75/Dh98/2y+87kCXios\n5dbn19PibB9l1h49t4tKZoCU6DBiw4M7tUBqc7kprqjvVHTkNS83iaLyeirr24udapvb+OMnJZw3\nMY2pWXGMS4smLyWKt3ooQlq+26pP6O27BKuS+2sLcli7t/qkx/HaeqiWCekxvTaljggNYnRi5JAc\n7sKfOYV5QJExpsQY0wq8CFzqu4ExxvdXGwUEfoCXYSIuMoSbzx7Le9vL7YlqahrbeKnwAF97ag1z\n7n+PY41tLJk6KsAp9Y/JGXFs66ZZ6oriKlJjwnrtWBQeEsSUzFi7BdKWgzUs21nBjWfmBWyY9Y6m\nZ8WzsbSm18ETVxRXEhIkzM3p3KdBRPiP88fx4y9M5K0tR7jxmUJ7qIjVJUfJiAsnKyGi0+u8ry1I\ni+nUAsmqZDVM6CKnANjjIK31yS08vXwvtc1Ovntegb3vC6eks6qkiqr6znUWYPVPmJQea0+l2psr\n52QRGRrEn06ieaoxhu09DG/RUUFqjOYUOsgEfAd5KfWsO46I3CIixVg5he/4MT0jzg0Lc0iODuOe\nv2/lG39ay5z73+WHr2xiT2U9N52Vx5vfOYNzJ3Td6Wiom5wRy6GaZj7cWc6bmw7z9Io9PPDWDv7j\npQ0s21He5xZXs8cksOlgDS1OF7//oIjY8GC+tmDMABxB38zIjqOyvoVD3bTk8VpZVMXM0Qk9BrMb\nz8zjl1dMY0VRJV99cg01jW2s3lPFaXk9f1b5KVYLJF/elkfd5RSmZsYRERJkFyHVNLXxx+UlXDAp\n7bjK/wunpuM28M62zkVIDS1O1u+v5sw+FB15xYaH8KXZWby+4dBxuZTeGGN44pMS6lqcTOmlPsGr\nIC2aPZUN/TZeVVV9y4CMnBzw1kfGmIeNMWOBO4Efd7WNiNwkIoUiUlhRUTGwCRzCIkODuf28ArYd\nrmXnkTqrnuHWhXx8xyLuXDyh18qyoWyqZ16I659eyy3Pr+en/9zGk8tLWFVcRX5qNNfO79uFffaY\nBFqdbl5bf5B/bT3C9QtzexyHaqBNy7J6Y/dUhHSssZUth2q6bX7r66q52Tx87Sw2lR7j4t8vp7K+\n+/oEr4K0aKoaWo/rA7DzSB1BDmFsatdl76HBVk9ybye2p5bvoa7Zye3nHT9Ey8T0GHKSIrtshbRm\nz1HaXKZP9Qm+vrYgh1aXmxf72Dy1uqGVG58p5OdLd3DBpLQex6zyNS4tus+z0/Xm3W1lLPq/D3mp\nj4Mpngp/5oEPAr7NM7I867rzIvBIV08YYx4HHgeYM2eOFjGdgOvmj+asgmRGJ0YOq74IvVmQl8QT\nX5tDaLCD1Jgw0mLDiY8I6bFDWFdmjbaKW+57YxtRoUF8Y2GOH1J78iakxxAa5GDjgWPdzjexqqQK\nY7DnjOjNkqnp/DEsmG/9ZR3Q3oS0O2N9hrvwFgvtLKsjNzmqy97iXvNzk3jovV0cONrIU8v38PnJ\naZ1uVESEJVPTefzjEqobWknwqbz/ZHclocEO5ub0HLQ6yk+N5qxxKfxl1T5uPntsj73xC/ce5bYX\nPqOqvpX/ungSXz89p8+/o4LU9hZIvQ250p02l5tf/msHT3yyhymZsb1+F/3BnzmFtUCBiOSKSChw\nNfC67wYi4ntb8AWg9x5X6oSICGOSokZUQADruM+flMbZ41KYmB5LYlToCQcEsFpyZSdG0Njq4qsL\ncvo8ptRACQsOYmJGbI+d2FYUWbPqTc/uPMZTd84el8KLN53GvRdNYnQvrdMKUjvPwtZdyyNf83IT\nMQZufeEz6lqcfPe8cV1u94Wp6bjchnc7FCEtL6pgXk5it63AenLD6TmU1bZ02znO7TY8vKyILz++\nitBgB6/+++lcvzD3hH5H+anROE6hBdLBY01c9dinPPGJNQnVq/9+OmOSem711B/8llMwxjhF5Fbg\nbSAIeMoYs1VE7gMKjTGvA7eKyHlAG1ANfN1f6VHqZM3NSaSi7jDfPCM30Enp0vSsOF5dV9rt4H0r\nuplVr9f9Zsf3KZBkxEUQERJkN0ttbHWy/2gjX5rd/XzaADOy4+1czpIpo5jYzdDtkzNiyU6MYOmW\nw1w11yp8KKttZldZPV+c1fN7dOfscSnkJEVy/5vbeWPTISJDrcEdrb9g1u2rZnlRJRdNS+cXX5x6\nUkWG4SFWC6STCQof7CjjP17aiNNl+P21M7loWsYJ7+Nk+bUJhTFmKbC0w7p7fR7f7s/3V6o/3L1k\nIjeekXdcB7bBZHpWPH/+dF+XTUCP1DRTUtHAtfP814TW4ZDjZmHzzk/cW04hPCSIGdnxrNl7tFNd\ngi9vK6SnVuyhprGNuMgQlu+2mhWf0YemqN2l+e4LJ/LYR8Xsq2qkodVJU6uLhhYXTW0uIkKC+Pnl\nU7lmXvYp5bIXjE3i5cJS1u+vtosie+J0ufnfd3by2EclTEqP5eGvzOq1T0R/Gxzt6pQaxFJiwgZt\nQADsu/kNB451CgrePhn+7rWenxpt92mwZ1vrQzn6bZ/Lp6i8vteOe0umpvPYxyW8u72ML83OYnlR\nJUlRoT1ODNWbz08execnd26S7XYb3Mb0OklTX9y1ZCLLiyq57fnPePM7Z/RY/GiM4Z5/bOGFNQe4\ndv5o7r1o0kkVjZ2qgLc+UkqdmrzkKGLCgtnURb3CiuJKEqNCu+0v0F/yU6M5XNNMXXMbO47UER7i\n6LUuAqxB7G5Y2Hux3PSsODLjI3hr82GMMSwvquT0/OSTqifqjcMh/RIQwOps+vtrZlFe18wPXt7U\nY5PSh5cV8cKaA9yyaCw/v3xqQAICaFBQashzOISpWXFsPHB8Zz1jDCuLqlgwtudZ9fqDtyNgcUUD\nu8rqGJcW06/vKSIsmTKKT3ZXsm5fNRV1LZx5kkVHA216djx3LZnIe9vLeGrF3i63eXVdKf/3zi4u\nn5nJD7qZR2OgaFBQahiYnh3P9sO1NLe1D1FRUtnAkdrmPvVPOFW+LZB2dJhYp78smZpOq8vNfW9s\nAzjh/gmB9I2FOZw/KY0H3treqU/J8t2V3PnqJk4fm8T/XDEt4C0FNSgoNQxMz4rH6TbHDYC40h7j\nyf9t20cnRhIa5GDNnioq61t6rWQ+GTOz4xkVG86m0hryUqLIiO966I3BSET43y9NIzUmnFtfWE9N\nUxtgDdr4rWfXkZ8a3esw6wMl8ClQSp0yexhtn7vQFZ5RcPtStn+qgoMc5CZH2SOa+iMoOBxij9U1\nVIqOfMVHhvK7a2dy+Fgzd726iUPHmrjhT2uIDgvm6RvmEjtIesprUFBqGBgVG05qTBgbS616BZfb\n8GlJFQvzB25WvfzUaPsO2B/FRwCXTM9ABM6bNDTH7Jo1OoEfLh7PW1uO8IXffkJji4unb5hLetzg\nyfVok1SlhgERYXp2vJ1T2Haolpqmtj4NKd1fvJXNCZEhfmvCO3N0Aqvv/hypsX2bnW4wuvGMPFaX\nHOWjXRU884153XbaCxQNCkoNE9Oz4nh3Wxk1TW2s8MxBvWAA58vwBoVxaTF+zZ0M5YAAVjHYI9fN\npryuuduZAQNJi4+UGia8ndg2l9awoqiScWnRfZ7vuT8UeGZh83efiOEgNNgxKAMCaFBQatiYlmkF\nhTV7j7J279EBn3s7LzmaWaPj+dzEoVneryxafKTUMBEXGUJechQvrNlPc5t7QOsTwLr7/du3Fw7o\ne6r+pzkFpYaRaVlxVNS14BB6nRxHqa5oUFBqGPHWK0zNih807d7V0KJBQalhxBsUFg5gqyM1vGhQ\nUGoYmZYZx81n53HtfP/Nn6CGN61oVmoYCQ5ycPeSiYFOhhrCNKeglFLKpkFBKaWUTYOCUkopmwYF\npZRSNg0KSimlbBoUlFJK2TQoKKWUsmlQUEopZRNjTKDTcEJEpALYd5IvTwYq+zE5Q8VIPW4Yuceu\nxz2y9OW4xxhjUnrb0ZALCqdCRAqNMXMCnY6BNlKPG0busetxjyz9edxafKSUUsqmQUEppZRtpAWF\nxwOdgAAZqccNI/fY9bhHln477hFVp6CUUqpnIy2noJRSqgcjJiiIyGIR2SkiRSJyV6DT4y8i8pSI\nlIvIFp91iSLyrojs9vxPCGQa/UFEskVkmYhsE5GtInK7Z/2wPnYRCReRNSKy0XPcP/WszxWR1Z7z\n/a8iEhrotPqDiASJyGci8oZnedgft4jsFZHNIrJBRAo96/rtPB8RQUFEgoCHgSXAJOAaEZkU2FT5\nzZ+AxR3W3QW8b4wpAN73LA83TuD7xphJwGnALZ7veLgfewtwrjFmOjADWCwipwH/AzxkjMkHqoFv\nBjCN/nQ7sN1neaQc9yJjzAyfZqj9dp6PiKAAzAOKjDElxphW4EXg0gCnyS+MMR8DRzusvhR4xvP4\nGeCyAU3UADDGHDbGrPc8rsO6UGQyzI/dWOo9iyGePwOcC7ziWT/sjhtARLKALwB/9CwLI+C4u9Fv\n5/lICQqZwAGf5VLPupEizRhz2PP4CJAWyMT4m4jkADOB1YyAY/cUoWwAyoF3gWLgmDHG6dlkuJ7v\nvwZ+CLg9y0mMjOM2wDsisk5EbvKs67fzXOdoHmGMMUZEhm2TMxGJBl4FvmuMqbVuHi3D9diNMS5g\nhojEA68BEwKcJL8TkYuAcmPMOhE5J9DpGWBnGGMOikgq8K6I7PB98lTP85GSUzgIZPssZ3nWjRRl\nIpIO4PlfHuD0+IWIhGAFhOeMMX/zrB4Rxw5gjDkGLAMWAPEi4r3pG47n+0LgEhHZi1UcfC7wG4b/\ncWOMOej5X451EzCPfjzPR0pQWAsUeFomhAJXA68HOE0D6XXg657HXwf+EcC0+IWnPPlJYLsx5kGf\np4b1sYtIiieHgIhEAOdj1acsA77k2WzYHbcx5m5jTJYxJgfr9/yBMeYrDPPjFpEoEYnxPgYuALbQ\nj+f5iOm8JiIXYpVBBgFPGWPuD3CS/EJEXgDOwRo1sQz4CfB34CVgNNYIs1cZYzpWRg9pInIG8Amw\nmfYy5h9h1SsM22MXkWlYFYtBWDd5Lxlj7hORPKw76ETgM+A6Y0xL4FLqP57iox8YYy4a7sftOb7X\nPIvBwPPGmPtFJIl+Os9HTFBQSinVu5FSfKSUUqoPNCgopZSyaVBQSill06CglFLKpkFBKaWUTYOC\nUgNIRM7xjuip1GCkQUEppZRNg4JSXRCR6zzzFGwQkcc8g87Vi8ivRGS9iLwvIimebWeIyCoR2SQi\nr3nHsheRfBF5zzPXwXoRGevZfbSIvCIiO0TkOfEdoEmpANOgoFQHIjIR+DKw0BgzA3ABXwGigPXG\nmFnAR1i9xQH+DNxpjJmG1aPau/454GHPXAenA95RLGcC38Wa2yMPaxwfpQYFHSVVqc4+B8wG1npu\n4iOwBhhzA3/1bPMs8DcRiQPijTEfedY/A7zsGZ8m0xjzGoAxphnAs781xphSz/IGIAdY7v/DUqp3\nGhSU6kyAZ4wxdx+3UuSeDtud7BgxvmPxuNDfoRpEtPhIqc7eB77kGa/eO//tGKzfi3cEzmuB5caY\nGqBaRM70rP8q8JFn9rdSEbnMs48wEYkc0KNQ6iToHYpSHRhjtonIj7Fmt3IAbcAtQAMwWUTWATVY\n9Q5gDVX8qOeiXwLc4Fn/VeAxEbnPs48rB/AwlDopOkqqUn0kIvXGmOhAp0Mpf9LiI6WUUjbNKSil\nlLJpTkEppZRNg4JSSimbBgWllFI2DQpKKaVsGhSUUkrZNCgopZSy/T/UA24m7nG3lgAAAABJRU5E\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1ca1cdb9ef0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# preprocess\n",
    "rescale_factor = 255\n",
    "train_datagen = ImageDataGenerator(featurewise_center=True, featurewise_std_normalization=True)\n",
    "train_datagen.fit(x_train)\n",
    "\n",
    "test_datagen = ImageDataGenerator(featurewise_center=True, featurewise_std_normalization=True)\n",
    "test_datagen.fit(x_train)\n",
    "\n",
    "# build model\n",
    "model_name = '1st_place'\n",
    "model = Sequential(name=model_name)\n",
    "model.add(Cropping2D(cropping=3, input_shape=image_shape))\n",
    "model.add(Conv2D(32, (5,5), padding='same', activation='relu', kernel_initializer='TruncatedNormal'))\n",
    "model.add(MaxPooling2D((2,2), strides=(2,2), padding='valid'))\n",
    "\n",
    "model.add(ZeroPadding2D(padding=1))\n",
    "model.add(Conv2D(32, (4,4), padding='valid', activation='relu', kernel_initializer='TruncatedNormal'))\n",
    "model.add(AveragePooling2D((2,2), strides=(2,2), padding='valid'))\n",
    "\n",
    "model.add(Conv2D(64, (5,5), padding='same', activation='relu', kernel_initializer='TruncatedNormal'))\n",
    "model.add(AveragePooling2D((2,2), strides=(2,2), padding='valid'))\n",
    "\n",
    "model.add(Flatten())\n",
    "# TODO is this where dropout should be?\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(3072, activation='relu', kernel_initializer='TruncatedNormal'))\n",
    "# TODO svm activation function\n",
    "model.add(Dense(n_classes, activation='softmax', kernel_initializer='TruncatedNormal'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# compile model\n",
    "sgd = optimizers.SGD(momentum=0.9, nesterov=True, lr=0.001)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "# fit model\n",
    "epoch_sets = 3\n",
    "epochs = 50\n",
    "batch_size = 50\n",
    "for es in range(epoch_sets):\n",
    "    # get current lr\n",
    "    lr = k.get_value(sgd.lr)\n",
    "    print('epoch set:', es, 'lr value:', lr)\n",
    "    \n",
    "    # fit model\n",
    "    history = model.fit_generator(train_datagen.flow(x_train, y_train_onehot, batch_size=batch_size)\n",
    "                        , steps_per_epoch=n_train//batch_size\n",
    "                        , epochs=epochs\n",
    "                        , validation_data=test_datagen.flow(x_test, y_test_onehot, batch_size=batch_size)\n",
    "                        , validation_steps=n_test//batch_size)\n",
    "    # decay lr by half\n",
    "    lr = lr / 2\n",
    "    k.set_value(sgd.lr, lr)\n",
    "    \n",
    "# visualize loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'conv2d_10/kernel:0' shape=(5, 5, 1, 32) dtype=float32_ref>,\n",
       " <tf.Variable 'conv2d_10/bias:0' shape=(32,) dtype=float32_ref>,\n",
       " <tf.Variable 'conv2d_11/kernel:0' shape=(4, 4, 32, 32) dtype=float32_ref>,\n",
       " <tf.Variable 'conv2d_11/bias:0' shape=(32,) dtype=float32_ref>,\n",
       " <tf.Variable 'conv2d_12/kernel:0' shape=(5, 5, 32, 64) dtype=float32_ref>,\n",
       " <tf.Variable 'conv2d_12/bias:0' shape=(64,) dtype=float32_ref>,\n",
       " <tf.Variable 'dense_7/kernel:0' shape=(1600, 3072) dtype=float32_ref>,\n",
       " <tf.Variable 'dense_7/bias:0' shape=(3072,) dtype=float32_ref>,\n",
       " <tf.Variable 'dense_8/kernel:0' shape=(3072, 7) dtype=float32_ref>,\n",
       " <tf.Variable 'dense_8/bias:0' shape=(7,) dtype=float32_ref>]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.get_updates_for()\n",
    "model.weights"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
